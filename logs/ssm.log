[INFO] [2017-11-07 17:53:20][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-07 17:53:21][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@7dc7cbad: startup date [Tue Nov 07 17:53:21 CST 2017]; root of context hierarchy
  [WARN] [2017-11-07 17:53:21][org.mybatis.spring.mapper.ClassPathMapperScanner]No MyBatis mapper was found in '[com.cn.uuu.IDao]' package. Please check your configuration.
  [INFO] [2017-11-07 17:53:21][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [ERROR] [2017-11-07 17:53:21][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@54c562f7] to prepare test instance [org.zyp.testmybatis.TestMybBatis@318ba8c8]
  java.lang.IllegalStateException: Failed to load ApplicationContext
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:99)
	at org.springframework.test.context.DefaultTestContext.getApplicationContext(DefaultTestContext.java:101)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'dataSource' defined in class path resource [spring-mybatis.xml]: Could not resolve placeholder 'jdbc.driver' in string value "${jdbc.driver}"; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'jdbc.driver' in string value "${jdbc.driver}"
	at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:211)
	at org.springframework.beans.factory.config.PropertyPlaceholderConfigurer.processProperties(PropertyPlaceholderConfigurer.java:223)
	at org.springframework.beans.factory.config.PropertyResourceConfigurer.postProcessBeanFactory(PropertyResourceConfigurer.java:86)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:265)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:162)
	at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:609)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:464)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:121)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:60)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.delegateLoading(AbstractDelegatingSmartContextLoader.java:100)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.loadContext(AbstractDelegatingSmartContextLoader.java:250)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContextInternal(CacheAwareContextLoaderDelegate.java:64)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)
	... 25 more
Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'jdbc.driver' in string value "${jdbc.driver}"
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:174)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.beans.factory.config.PropertyPlaceholderConfigurer$PlaceholderResolvingStringValueResolver.resolveStringValue(PropertyPlaceholderConfigurer.java:259)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveStringValue(BeanDefinitionVisitor.java:282)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:204)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitPropertyValues(BeanDefinitionVisitor.java:141)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitBeanDefinition(BeanDefinitionVisitor.java:82)
	at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:208)
	... 37 more
[INFO] [2017-11-07 17:58:14][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-07 17:58:15][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@7dc7cbad: startup date [Tue Nov 07 17:58:15 CST 2017]; root of context hierarchy
  [WARN] [2017-11-07 17:58:15][org.mybatis.spring.mapper.ClassPathMapperScanner]No MyBatis mapper was found in '[com.cn.uuu.IDao]' package. Please check your configuration.
  [INFO] [2017-11-07 17:58:15][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [ERROR] [2017-11-07 17:58:15][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@54c562f7] to prepare test instance [org.zyp.testmybatis.TestMybBatis@318ba8c8]
  java.lang.IllegalStateException: Failed to load ApplicationContext
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:99)
	at org.springframework.test.context.DefaultTestContext.getApplicationContext(DefaultTestContext.java:101)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'dataSource' defined in class path resource [spring-mybatis.xml]: Could not resolve placeholder 'database.driverClassName' in string value "${database.driverClassName}"; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'database.driverClassName' in string value "${database.driverClassName}"
	at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:211)
	at org.springframework.beans.factory.config.PropertyPlaceholderConfigurer.processProperties(PropertyPlaceholderConfigurer.java:223)
	at org.springframework.beans.factory.config.PropertyResourceConfigurer.postProcessBeanFactory(PropertyResourceConfigurer.java:86)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:265)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:162)
	at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:609)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:464)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:121)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:60)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.delegateLoading(AbstractDelegatingSmartContextLoader.java:100)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.loadContext(AbstractDelegatingSmartContextLoader.java:250)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContextInternal(CacheAwareContextLoaderDelegate.java:64)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)
	... 25 more
Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'database.driverClassName' in string value "${database.driverClassName}"
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:174)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.beans.factory.config.PropertyPlaceholderConfigurer$PlaceholderResolvingStringValueResolver.resolveStringValue(PropertyPlaceholderConfigurer.java:259)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveStringValue(BeanDefinitionVisitor.java:282)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:204)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitPropertyValues(BeanDefinitionVisitor.java:141)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitBeanDefinition(BeanDefinitionVisitor.java:82)
	at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:208)
	... 37 more
[INFO] [2017-11-07 17:58:36][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-07 17:58:37][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@7dc7cbad: startup date [Tue Nov 07 17:58:37 CST 2017]; root of context hierarchy
  [WARN] [2017-11-07 17:58:37][org.mybatis.spring.mapper.ClassPathMapperScanner]No MyBatis mapper was found in '[com.cn.uuu.IDao]' package. Please check your configuration.
  [INFO] [2017-11-07 17:58:37][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [ERROR] [2017-11-07 17:58:37][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@54c562f7] to prepare test instance [org.zyp.testmybatis.TestMybBatis@318ba8c8]
  java.lang.IllegalStateException: Failed to load ApplicationContext
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:99)
	at org.springframework.test.context.DefaultTestContext.getApplicationContext(DefaultTestContext.java:101)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'dataSource' defined in class path resource [spring-mybatis.xml]: Could not resolve placeholder 'jdbc.driverClassName' in string value "${jdbc.driverClassName}"; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'jdbc.driverClassName' in string value "${jdbc.driverClassName}"
	at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:211)
	at org.springframework.beans.factory.config.PropertyPlaceholderConfigurer.processProperties(PropertyPlaceholderConfigurer.java:223)
	at org.springframework.beans.factory.config.PropertyResourceConfigurer.postProcessBeanFactory(PropertyResourceConfigurer.java:86)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:265)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:162)
	at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:609)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:464)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:121)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:60)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.delegateLoading(AbstractDelegatingSmartContextLoader.java:100)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.loadContext(AbstractDelegatingSmartContextLoader.java:250)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContextInternal(CacheAwareContextLoaderDelegate.java:64)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)
	... 25 more
Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'jdbc.driverClassName' in string value "${jdbc.driverClassName}"
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:174)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.beans.factory.config.PropertyPlaceholderConfigurer$PlaceholderResolvingStringValueResolver.resolveStringValue(PropertyPlaceholderConfigurer.java:259)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveStringValue(BeanDefinitionVisitor.java:282)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:204)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitPropertyValues(BeanDefinitionVisitor.java:141)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitBeanDefinition(BeanDefinitionVisitor.java:82)
	at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:208)
	... 37 more
[INFO] [2017-11-07 19:51:40][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-07 19:51:40][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@7dc7cbad: startup date [Tue Nov 07 19:51:40 CST 2017]; root of context hierarchy
  [WARN] [2017-11-07 19:51:41][org.mybatis.spring.mapper.ClassPathMapperScanner]No MyBatis mapper was found in '[com.cn.uuu.IDao]' package. Please check your configuration.
  [INFO] [2017-11-07 19:51:41][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [ERROR] [2017-11-07 19:51:41][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@54c562f7] to prepare test instance [org.zyp.testmybatis.TestMybBatis@318ba8c8]
  java.lang.IllegalStateException: Failed to load ApplicationContext
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:99)
	at org.springframework.test.context.DefaultTestContext.getApplicationContext(DefaultTestContext.java:101)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'dataSource' defined in class path resource [spring-mybatis.xml]: Could not resolve placeholder 'jdbc.initialSize' in string value "${jdbc.initialSize}"; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'jdbc.initialSize' in string value "${jdbc.initialSize}"
	at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:211)
	at org.springframework.beans.factory.config.PropertyPlaceholderConfigurer.processProperties(PropertyPlaceholderConfigurer.java:223)
	at org.springframework.beans.factory.config.PropertyResourceConfigurer.postProcessBeanFactory(PropertyResourceConfigurer.java:86)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:265)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:162)
	at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:609)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:464)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:121)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:60)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.delegateLoading(AbstractDelegatingSmartContextLoader.java:100)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.loadContext(AbstractDelegatingSmartContextLoader.java:250)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContextInternal(CacheAwareContextLoaderDelegate.java:64)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)
	... 25 more
Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'jdbc.initialSize' in string value "${jdbc.initialSize}"
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:174)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.beans.factory.config.PropertyPlaceholderConfigurer$PlaceholderResolvingStringValueResolver.resolveStringValue(PropertyPlaceholderConfigurer.java:259)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveStringValue(BeanDefinitionVisitor.java:282)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:204)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitPropertyValues(BeanDefinitionVisitor.java:141)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitBeanDefinition(BeanDefinitionVisitor.java:82)
	at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:208)
	... 37 more
[INFO] [2017-11-07 20:27:44][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-07 20:27:44][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@7dc7cbad: startup date [Tue Nov 07 20:27:44 CST 2017]; root of context hierarchy
  [WARN] [2017-11-07 20:27:44][org.mybatis.spring.mapper.ClassPathMapperScanner]No MyBatis mapper was found in '[com.cn.uuu.IDao]' package. Please check your configuration.
  [INFO] [2017-11-07 20:27:44][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-07 20:27:44][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [ERROR] [2017-11-07 20:27:44][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@59fd97a8] to prepare test instance [org.zyp.testmybatis.TestMybBatis@f5ac9e4]
  java.lang.IllegalStateException: Failed to load ApplicationContext
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:99)
	at org.springframework.test.context.DefaultTestContext.getApplicationContext(DefaultTestContext.java:101)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Initialization of bean failed; nested exception is org.springframework.beans.TypeMismatchException: Failed to convert property value of type 'java.lang.String' to required type 'org.springframework.core.io.Resource[]' for property 'mapperLocations'; nested exception is java.lang.IllegalArgumentException: Could not resolve resource location pattern [classpath:com/cn/uuu/mapping/*.xml]: class path resource [com/cn/uuu/mapping/] cannot be resolved to URL because it does not exist
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:547)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:681)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:121)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:60)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.delegateLoading(AbstractDelegatingSmartContextLoader.java:100)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.loadContext(AbstractDelegatingSmartContextLoader.java:250)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContextInternal(CacheAwareContextLoaderDelegate.java:64)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)
	... 25 more
Caused by: org.springframework.beans.TypeMismatchException: Failed to convert property value of type 'java.lang.String' to required type 'org.springframework.core.io.Resource[]' for property 'mapperLocations'; nested exception is java.lang.IllegalArgumentException: Could not resolve resource location pattern [classpath:com/cn/uuu/mapping/*.xml]: class path resource [com/cn/uuu/mapping/] cannot be resolved to URL because it does not exist
	at org.springframework.beans.BeanWrapperImpl.convertIfNecessary(BeanWrapperImpl.java:479)
	at org.springframework.beans.BeanWrapperImpl.convertForProperty(BeanWrapperImpl.java:511)
	at org.springframework.beans.BeanWrapperImpl.convertForProperty(BeanWrapperImpl.java:505)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.convertForProperty(AbstractAutowireCapableBeanFactory.java:1502)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1461)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1197)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	... 39 more
Caused by: java.lang.IllegalArgumentException: Could not resolve resource location pattern [classpath:com/cn/uuu/mapping/*.xml]: class path resource [com/cn/uuu/mapping/] cannot be resolved to URL because it does not exist
	at org.springframework.core.io.support.ResourceArrayPropertyEditor.setAsText(ResourceArrayPropertyEditor.java:140)
	at org.springframework.beans.TypeConverterDelegate.doConvertTextValue(TypeConverterDelegate.java:430)
	at org.springframework.beans.TypeConverterDelegate.doConvertValue(TypeConverterDelegate.java:403)
	at org.springframework.beans.TypeConverterDelegate.convertIfNecessary(TypeConverterDelegate.java:181)
	at org.springframework.beans.BeanWrapperImpl.convertIfNecessary(BeanWrapperImpl.java:459)
	... 45 more
[INFO] [2017-11-07 20:35:21][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-07 20:35:21][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@7dc7cbad: startup date [Tue Nov 07 20:35:21 CST 2017]; root of context hierarchy
  [WARN] [2017-11-07 20:35:21][org.mybatis.spring.mapper.ClassPathMapperScanner]No MyBatis mapper was found in '[com.cn.uuu.IDao]' package. Please check your configuration.
  [INFO] [2017-11-07 20:35:21][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-07 20:35:21][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [ERROR] [2017-11-07 20:35:21][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@31190526] to prepare test instance [org.zyp.testmybatis.TestMybBatis@662ac478]
  java.lang.IllegalStateException: Failed to load ApplicationContext
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:99)
	at org.springframework.test.context.DefaultTestContext.getApplicationContext(DefaultTestContext.java:101)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/yinkgh/provider/dao/UserMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.yinkgh.provider.dao.UserMapper.BaseResultMap
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1553)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:681)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:121)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:60)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.delegateLoading(AbstractDelegatingSmartContextLoader.java:100)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.loadContext(AbstractDelegatingSmartContextLoader.java:250)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContextInternal(CacheAwareContextLoaderDelegate.java:64)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)
	... 25 more
Caused by: org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/yinkgh/provider/dao/UserMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.yinkgh.provider.dao.UserMapper.BaseResultMap
	at org.mybatis.spring.SqlSessionFactoryBean.buildSqlSessionFactory(SqlSessionFactoryBean.java:466)
	at org.mybatis.spring.SqlSessionFactoryBean.afterPropertiesSet(SqlSessionFactoryBean.java:340)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1612)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1549)
	... 40 more
Caused by: org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.yinkgh.provider.dao.UserMapper.BaseResultMap
	at org.apache.ibatis.builder.xml.XMLMapperBuilder.configurationElement(XMLMapperBuilder.java:120)
	at org.apache.ibatis.builder.xml.XMLMapperBuilder.parse(XMLMapperBuilder.java:92)
	at org.mybatis.spring.SqlSessionFactoryBean.buildSqlSessionFactory(SqlSessionFactoryBean.java:464)
	... 43 more
Caused by: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.yinkgh.provider.dao.UserMapper.BaseResultMap
	at org.apache.ibatis.session.Configuration$StrictMap.put(Configuration.java:782)
	at org.apache.ibatis.session.Configuration$StrictMap.put(Configuration.java:754)
	at org.apache.ibatis.session.Configuration.addResultMap(Configuration.java:536)
	at org.apache.ibatis.builder.MapperBuilderAssistant.addResultMap(MapperBuilderAssistant.java:207)
	at org.apache.ibatis.builder.ResultMapResolver.resolve(ResultMapResolver.java:47)
	at org.apache.ibatis.builder.xml.XMLMapperBuilder.resultMapElement(XMLMapperBuilder.java:284)
	at org.apache.ibatis.builder.xml.XMLMapperBuilder.resultMapElement(XMLMapperBuilder.java:251)
	at org.apache.ibatis.builder.xml.XMLMapperBuilder.resultMapElements(XMLMapperBuilder.java:243)
	at org.apache.ibatis.builder.xml.XMLMapperBuilder.configurationElement(XMLMapperBuilder.java:116)
	... 45 more
[INFO] [2017-11-07 20:40:46][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-07 20:40:47][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@7dc7cbad: startup date [Tue Nov 07 20:40:47 CST 2017]; root of context hierarchy
  [INFO] [2017-11-07 20:40:47][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-07 20:40:47][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [ERROR] [2017-11-07 20:40:47][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@3bf7ca37] to prepare test instance [org.zyp.testmybatis.TestMybBatis@79079097]
  java.lang.IllegalStateException: Failed to load ApplicationContext
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:99)
	at org.springframework.test.context.DefaultTestContext.getApplicationContext(DefaultTestContext.java:101)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/yinkgh/provider/dao/UserMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.yinkgh.provider.dao.UserMapper.BaseResultMap
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1553)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:681)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:121)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:60)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.delegateLoading(AbstractDelegatingSmartContextLoader.java:100)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.loadContext(AbstractDelegatingSmartContextLoader.java:250)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContextInternal(CacheAwareContextLoaderDelegate.java:64)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)
	... 25 more
Caused by: org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/yinkgh/provider/dao/UserMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.yinkgh.provider.dao.UserMapper.BaseResultMap
	at org.mybatis.spring.SqlSessionFactoryBean.buildSqlSessionFactory(SqlSessionFactoryBean.java:466)
	at org.mybatis.spring.SqlSessionFactoryBean.afterPropertiesSet(SqlSessionFactoryBean.java:340)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1612)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1549)
	... 40 more
Caused by: org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.yinkgh.provider.dao.UserMapper.BaseResultMap
	at org.apache.ibatis.builder.xml.XMLMapperBuilder.configurationElement(XMLMapperBuilder.java:120)
	at org.apache.ibatis.builder.xml.XMLMapperBuilder.parse(XMLMapperBuilder.java:92)
	at org.mybatis.spring.SqlSessionFactoryBean.buildSqlSessionFactory(SqlSessionFactoryBean.java:464)
	... 43 more
Caused by: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.yinkgh.provider.dao.UserMapper.BaseResultMap
	at org.apache.ibatis.session.Configuration$StrictMap.put(Configuration.java:782)
	at org.apache.ibatis.session.Configuration$StrictMap.put(Configuration.java:754)
	at org.apache.ibatis.session.Configuration.addResultMap(Configuration.java:536)
	at org.apache.ibatis.builder.MapperBuilderAssistant.addResultMap(MapperBuilderAssistant.java:207)
	at org.apache.ibatis.builder.ResultMapResolver.resolve(ResultMapResolver.java:47)
	at org.apache.ibatis.builder.xml.XMLMapperBuilder.resultMapElement(XMLMapperBuilder.java:284)
	at org.apache.ibatis.builder.xml.XMLMapperBuilder.resultMapElement(XMLMapperBuilder.java:251)
	at org.apache.ibatis.builder.xml.XMLMapperBuilder.resultMapElements(XMLMapperBuilder.java:243)
	at org.apache.ibatis.builder.xml.XMLMapperBuilder.configurationElement(XMLMapperBuilder.java:116)
	... 45 more
[INFO] [2017-11-07 21:01:59][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-07 21:01:59][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@7dc7cbad: startup date [Tue Nov 07 21:01:59 CST 2017]; root of context hierarchy
  [INFO] [2017-11-07 21:02:00][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-07 21:02:00][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [ERROR] [2017-11-07 21:02:00][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@1f97cf0d] to prepare test instance [org.zyp.testmybatis.TestMybBatis@140c9f39]
  org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.zyp.testmybatis.TestMybBatis': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.test.service.UserService] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireBeanProperties(AbstractAutowireCapableBeanFactory.java:384)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:110)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.test.service.UserService] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.raiseNoSuchBeanDefinitionException(DefaultListableBeanFactory.java:1100)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:960)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 26 more
[INFO] [2017-11-07 21:02:00][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@7dc7cbad: startup date [Tue Nov 07 21:01:59 CST 2017]; root of context hierarchy
  [INFO] [2017-11-07 21:08:42][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-07 21:08:42][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@7dc7cbad: startup date [Tue Nov 07 21:08:42 CST 2017]; root of context hierarchy
  [INFO] [2017-11-07 21:08:42][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-07 21:08:42][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [ERROR] [2017-11-07 21:08:43][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@1f97cf0d] to prepare test instance [org.zyp.testmybatis.TestMybBatis@140c9f39]
  org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.zyp.testmybatis.TestMybBatis': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.test.service.UserService] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireBeanProperties(AbstractAutowireCapableBeanFactory.java:384)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:110)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.test.service.UserService] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.raiseNoSuchBeanDefinitionException(DefaultListableBeanFactory.java:1100)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:960)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 26 more
[INFO] [2017-11-07 21:08:43][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@7dc7cbad: startup date [Tue Nov 07 21:08:42 CST 2017]; root of context hierarchy
  [INFO] [2017-11-07 21:17:27][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-07 21:17:27][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@d2cc05a: startup date [Tue Nov 07 21:17:27 CST 2017]; root of context hierarchy
  [INFO] [2017-11-07 21:17:27][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-07 21:17:27][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [ERROR] [2017-11-07 21:17:28][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@19d481b] to prepare test instance [org.zyp.testmybatis.TestMybBatis@1f97cf0d]
  org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.zyp.testmybatis.TestMybBatis': Injection of autowired dependencies failed; nested exception is org.springframework.beans.factory.BeanCreationException: Could not autowire field: private com.test.service.UserService org.zyp.testmybatis.TestMybBatis.userService; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.test.service.UserService] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)}
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:292)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireBeanProperties(AbstractAutowireCapableBeanFactory.java:384)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:110)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanCreationException: Could not autowire field: private com.test.service.UserService org.zyp.testmybatis.TestMybBatis.userService; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.test.service.UserService] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)}
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:508)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:289)
	... 26 more
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.test.service.UserService] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)}
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.raiseNoSuchBeanDefinitionException(DefaultListableBeanFactory.java:1100)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:960)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:480)
	... 28 more
[INFO] [2017-11-07 21:17:28][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@d2cc05a: startup date [Tue Nov 07 21:17:27 CST 2017]; root of context hierarchy
  [INFO] [2017-11-07 21:18:16][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-07 21:18:16][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@7dc7cbad: startup date [Tue Nov 07 21:18:16 CST 2017]; root of context hierarchy
  [INFO] [2017-11-07 21:18:17][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-07 21:18:17][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [ERROR] [2017-11-07 21:18:17][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@1f97cf0d] to prepare test instance [org.zyp.testmybatis.TestMybBatis@140c9f39]
  org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.zyp.testmybatis.TestMybBatis': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.test.service.UserService] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireBeanProperties(AbstractAutowireCapableBeanFactory.java:384)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:110)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.test.service.UserService] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.raiseNoSuchBeanDefinitionException(DefaultListableBeanFactory.java:1100)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:960)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 26 more
[INFO] [2017-11-07 21:18:17][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@7dc7cbad: startup date [Tue Nov 07 21:18:16 CST 2017]; root of context hierarchy
  [INFO] [2017-11-07 21:20:21][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-07 21:20:21][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@d2cc05a: startup date [Tue Nov 07 21:20:21 CST 2017]; root of context hierarchy
  [INFO] [2017-11-07 21:20:21][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-07 21:20:21][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [ERROR] [2017-11-07 21:20:22][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@19d481b] to prepare test instance [org.zyp.testmybatis.TestMybBatis@1f97cf0d]
  org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.zyp.testmybatis.TestMybBatis': Injection of autowired dependencies failed; nested exception is org.springframework.beans.factory.BeanCreationException: Could not autowire field: private com.test.service.UserService org.zyp.testmybatis.TestMybBatis.userService; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.test.service.UserService] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)}
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:292)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireBeanProperties(AbstractAutowireCapableBeanFactory.java:384)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:110)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanCreationException: Could not autowire field: private com.test.service.UserService org.zyp.testmybatis.TestMybBatis.userService; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.test.service.UserService] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)}
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:508)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:289)
	... 26 more
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.test.service.UserService] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)}
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.raiseNoSuchBeanDefinitionException(DefaultListableBeanFactory.java:1100)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:960)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:480)
	... 28 more
[INFO] [2017-11-07 21:20:22][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@d2cc05a: startup date [Tue Nov 07 21:20:21 CST 2017]; root of context hierarchy
  [INFO] [2017-11-07 21:33:05][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-07 21:33:05][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@7dc7cbad: startup date [Tue Nov 07 21:33:05 CST 2017]; root of context hierarchy
  [INFO] [2017-11-07 21:33:05][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-07 21:33:05][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-11-07 21:33:06][org.zyp.testmybatis.TestMybBatis]{"age":20,"id":1,"password":"wasd","user_name":"zyp"}
  [INFO] [2017-11-07 21:33:07][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@7dc7cbad: startup date [Tue Nov 07 21:33:05 CST 2017]; root of context hierarchy
  [INFO] [2017-11-23 11:19:55][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-23 11:19:56][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 23 11:19:56 CST 2017]; root of context hierarchy
  [INFO] [2017-11-23 11:19:57][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-23 11:19:57][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-11-23 11:19:58][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [org/springframework/jdbc/support/sql-error-codes.xml]
  [INFO] [2017-11-23 11:19:58][org.springframework.jdbc.support.SQLErrorCodesFactory]SQLErrorCodes loaded: [DB2, Derby, H2, HSQL, Informix, MS-SQL, MySQL, Oracle, PostgreSQL, Sybase]
  [INFO] [2017-11-23 11:19:58][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 23 11:19:56 CST 2017]; root of context hierarchy
  [INFO] [2017-11-23 11:28:37][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-23 11:28:37][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 23 11:28:37 CST 2017]; root of context hierarchy
  [INFO] [2017-11-23 11:28:37][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-23 11:28:37][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-11-23 11:28:38][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [org/springframework/jdbc/support/sql-error-codes.xml]
  [INFO] [2017-11-23 11:28:39][org.springframework.jdbc.support.SQLErrorCodesFactory]SQLErrorCodes loaded: [DB2, Derby, H2, HSQL, Informix, MS-SQL, MySQL, Oracle, PostgreSQL, Sybase]
  [INFO] [2017-11-23 11:28:39][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 23 11:28:37 CST 2017]; root of context hierarchy
  [INFO] [2017-11-23 12:01:09][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-23 12:01:09][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 23 12:01:09 CST 2017]; root of context hierarchy
  [INFO] [2017-11-23 12:01:10][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-23 12:01:10][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-11-23 12:01:11][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [org/springframework/jdbc/support/sql-error-codes.xml]
  [INFO] [2017-11-23 12:01:11][org.springframework.jdbc.support.SQLErrorCodesFactory]SQLErrorCodes loaded: [DB2, Derby, H2, HSQL, Informix, MS-SQL, MySQL, Oracle, PostgreSQL, Sybase]
  [INFO] [2017-11-23 12:01:11][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 23 12:01:09 CST 2017]; root of context hierarchy
  [INFO] [2017-11-23 12:01:56][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-23 12:01:57][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@1d8d30f7: startup date [Thu Nov 23 12:01:57 CST 2017]; root of context hierarchy
  [INFO] [2017-11-23 12:01:57][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-23 12:01:57][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-11-23 12:01:59][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [org/springframework/jdbc/support/sql-error-codes.xml]
  [INFO] [2017-11-23 12:01:59][org.springframework.jdbc.support.SQLErrorCodesFactory]SQLErrorCodes loaded: [DB2, Derby, H2, HSQL, Informix, MS-SQL, MySQL, Oracle, PostgreSQL, Sybase]
  [INFO] [2017-11-23 12:01:59][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@1d8d30f7: startup date [Thu Nov 23 12:01:57 CST 2017]; root of context hierarchy
  [INFO] [2017-11-23 12:17:53][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-23 12:17:53][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 23 12:17:53 CST 2017]; root of context hierarchy
  [INFO] [2017-11-23 12:17:54][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-23 12:17:54][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-11-23 12:17:55][org.zyp.testmybatis.TestMybBatis][{"book_id":"'45ff80ee9e84495a8a01880e125cc0b3'","book_num":191535,"book_rank":1,"log_date":"20171116"},{"book_id":"'d717e0fa935e45cd87ca3d5a4f482de1'","book_num":153851,"book_rank":2,"log_date":"20171116"},{"book_id":"'c8112e68ef0446cea43cb0fd3fabdd8f'","book_num":87329,"book_rank":3,"log_date":"20171116"},{"book_id":"'fdd4517182db476baed076e8ea0c73a2'","book_num":66118,"book_rank":4,"log_date":"20171116"},{"book_id":"'7d2cdcf9a5e54f3d835de6a063b95ad1'","book_num":62868,"book_rank":5,"log_date":"20171116"},{"book_id":"'99210b3f60db48aca913cb1973fec3c9'","book_num":61493,"book_rank":6,"log_date":"20171116"},{"book_id":"'dc70f449c7a24b80970ba279b85c05b0'","book_num":56719,"book_rank":7,"log_date":"20171116"},{"book_id":"'c4149360a00047babaed60b564bd9c94'","book_num":50514,"book_rank":8,"log_date":"20171116"},{"book_id":"'c41dcb4d3d054cad82760e081437ba55'","book_num":46068,"book_rank":9,"log_date":"20171116"},{"book_id":"'376890106c5748529a7cd428cb749db4'","book_num":44696,"book_rank":10,"log_date":"20171116"}]
  [INFO] [2017-11-23 12:17:55][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 23 12:17:53 CST 2017]; root of context hierarchy
  [INFO] [2017-11-23 13:25:40][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-23 13:25:42][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 23 13:25:42 CST 2017]; root of context hierarchy
  [INFO] [2017-11-23 13:25:42][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-23 13:25:42][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-11-23 13:25:44][org.zyp.testmybatis.TestMybBatis][{"book_id":"'45ff80ee9e84495a8a01880e125cc0b3'","book_num":191535,"book_rank":1,"log_date":"20171116"},{"book_id":"'d717e0fa935e45cd87ca3d5a4f482de1'","book_num":153851,"book_rank":2,"log_date":"20171116"},{"book_id":"'c8112e68ef0446cea43cb0fd3fabdd8f'","book_num":87329,"book_rank":3,"log_date":"20171116"},{"book_id":"'fdd4517182db476baed076e8ea0c73a2'","book_num":66118,"book_rank":4,"log_date":"20171116"},{"book_id":"'7d2cdcf9a5e54f3d835de6a063b95ad1'","book_num":62868,"book_rank":5,"log_date":"20171116"},{"book_id":"'99210b3f60db48aca913cb1973fec3c9'","book_num":61493,"book_rank":6,"log_date":"20171116"},{"book_id":"'dc70f449c7a24b80970ba279b85c05b0'","book_num":56719,"book_rank":7,"log_date":"20171116"},{"book_id":"'c4149360a00047babaed60b564bd9c94'","book_num":50514,"book_rank":8,"log_date":"20171116"},{"book_id":"'c41dcb4d3d054cad82760e081437ba55'","book_num":46068,"book_rank":9,"log_date":"20171116"},{"book_id":"'376890106c5748529a7cd428cb749db4'","book_num":44696,"book_rank":10,"log_date":"20171116"}]
  [INFO] [2017-11-23 13:25:44][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 23 13:25:42 CST 2017]; root of context hierarchy
  [INFO] [2017-11-23 13:26:38][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-23 13:26:40][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 23 13:26:40 CST 2017]; root of context hierarchy
  [INFO] [2017-11-23 13:26:40][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-23 13:26:40][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-11-23 13:26:41][org.zyp.testmybatis.TestMybBatis][{"book_id":"'45ff80ee9e84495a8a01880e125cc0b3'","book_num":191535,"book_rank":1,"log_date":"20171116"},{"book_id":"'d717e0fa935e45cd87ca3d5a4f482de1'","book_num":153851,"book_rank":2,"log_date":"20171116"},{"book_id":"'c8112e68ef0446cea43cb0fd3fabdd8f'","book_num":87329,"book_rank":3,"log_date":"20171116"},{"book_id":"'fdd4517182db476baed076e8ea0c73a2'","book_num":66118,"book_rank":4,"log_date":"20171116"},{"book_id":"'7d2cdcf9a5e54f3d835de6a063b95ad1'","book_num":62868,"book_rank":5,"log_date":"20171116"},{"book_id":"'99210b3f60db48aca913cb1973fec3c9'","book_num":61493,"book_rank":6,"log_date":"20171116"},{"book_id":"'dc70f449c7a24b80970ba279b85c05b0'","book_num":56719,"book_rank":7,"log_date":"20171116"},{"book_id":"'c4149360a00047babaed60b564bd9c94'","book_num":50514,"book_rank":8,"log_date":"20171116"},{"book_id":"'c41dcb4d3d054cad82760e081437ba55'","book_num":46068,"book_rank":9,"log_date":"20171116"},{"book_id":"'376890106c5748529a7cd428cb749db4'","book_num":44696,"book_rank":10,"log_date":"20171116"}]
  [INFO] [2017-11-23 13:26:41][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 23 13:26:40 CST 2017]; root of context hierarchy
  [INFO] [2017-11-23 16:30:43][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-23 16:30:44][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2c039ac6: startup date [Thu Nov 23 16:30:44 CST 2017]; root of context hierarchy
  [INFO] [2017-11-23 16:30:45][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-23 16:30:45][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-11-23 16:30:47][org.zyp.testmybatis.TestMybBatis][{"book_id":"'45ff80ee9e84495a8a01880e125cc0b3'","book_num":191535,"book_rank":1,"log_date":"20171116"},{"book_id":"'d717e0fa935e45cd87ca3d5a4f482de1'","book_num":153851,"book_rank":2,"log_date":"20171116"},{"book_id":"'c8112e68ef0446cea43cb0fd3fabdd8f'","book_num":87329,"book_rank":3,"log_date":"20171116"},{"book_id":"'fdd4517182db476baed076e8ea0c73a2'","book_num":66118,"book_rank":4,"log_date":"20171116"},{"book_id":"'7d2cdcf9a5e54f3d835de6a063b95ad1'","book_num":62868,"book_rank":5,"log_date":"20171116"},{"book_id":"'99210b3f60db48aca913cb1973fec3c9'","book_num":61493,"book_rank":6,"log_date":"20171116"},{"book_id":"'dc70f449c7a24b80970ba279b85c05b0'","book_num":56719,"book_rank":7,"log_date":"20171116"},{"book_id":"'c4149360a00047babaed60b564bd9c94'","book_num":50514,"book_rank":8,"log_date":"20171116"},{"book_id":"'c41dcb4d3d054cad82760e081437ba55'","book_num":46068,"book_rank":9,"log_date":"20171116"},{"book_id":"'376890106c5748529a7cd428cb749db4'","book_num":44696,"book_rank":10,"log_date":"20171116"}]
  [INFO] [2017-11-23 16:30:47][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2c039ac6: startup date [Thu Nov 23 16:30:44 CST 2017]; root of context hierarchy
  [WARN] [2017-11-24 17:40:08][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-11-24 17:41:51][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-11-24 17:42:27][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-11-24 17:42:37][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-11-24 17:42:46][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-11-24 17:42:57][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-11-24 17:44:26][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-11-24 17:56:39][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-11-24 17:56:49][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-11-24 18:00:28][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-11-25 09:27:06][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-11-25 15:14:23][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [INFO] [2017-11-30 17:07:31][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-30 17:07:32][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 30 17:07:32 CST 2017]; root of context hierarchy
  [INFO] [2017-11-30 17:07:32][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-30 17:07:32][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-11-30 17:07:35][org.zyp.testmybatis.TestMybBatis][{"book_id":"'45ff80ee9e84495a8a01880e125cc0b3'","book_num":26738,"book_rank":1,"log_date":"20171116"},{"book_id":"'d717e0fa935e45cd87ca3d5a4f482de1'","book_num":24177,"book_rank":2,"log_date":"20171116"},{"book_id":"'c8112e68ef0446cea43cb0fd3fabdd8f'","book_num":13821,"book_rank":3,"log_date":"20171116"},{"book_id":"'fdd4517182db476baed076e8ea0c73a2'","book_num":13602,"book_rank":4,"log_date":"20171116"},{"book_id":"'c4149360a00047babaed60b564bd9c94'","book_num":9917,"book_rank":5,"log_date":"20171116"},{"book_id":"'99210b3f60db48aca913cb1973fec3c9'","book_num":8683,"book_rank":6,"log_date":"20171116"},{"book_id":"'376890106c5748529a7cd428cb749db4'","book_num":8365,"book_rank":7,"log_date":"20171116"},{"book_id":"'7d2cdcf9a5e54f3d835de6a063b95ad1'","book_num":7856,"book_rank":8,"log_date":"20171116"},{"book_id":"'dc70f449c7a24b80970ba279b85c05b0'","book_num":7326,"book_rank":9,"log_date":"20171116"},{"book_id":"'db4232cf381447b39317c4411b47d9d7'","book_num":6797,"book_rank":10,"log_date":"20171116"},{"book_id":"'03124c0da03f4ce1937a2a4564d52309'","book_num":6532,"book_rank":11,"log_date":"20171116"},{"book_id":"'5ba8ea74ef724ae28f9aca2e6a81f216'","book_num":5881,"book_rank":12,"log_date":"20171116"},{"book_id":"'5f03d165c079413a8311a70bf91813f1'","book_num":5826,"book_rank":13,"log_date":"20171116"},{"book_id":"'396a65b821774507b5d0d2a92a1a287f'","book_num":5531,"book_rank":14,"log_date":"20171116"},{"book_id":"'c41dcb4d3d054cad82760e081437ba55'","book_num":5524,"book_rank":15,"log_date":"20171116"},{"book_id":"'4a62e15e685f4f4f96826db8da23dbf7'","book_num":4512,"book_rank":16,"log_date":"20171116"},{"book_id":"'851bb631ccc54564b0591cd073f779ff'","book_num":4466,"book_rank":17,"log_date":"20171116"},{"book_id":"'440eef2f51dc46a68f58249de19228c6'","book_num":4369,"book_rank":18,"log_date":"20171116"},{"book_id":"'fe6d1e9774674343b0c76af951ad36e1'","book_num":4306,"book_rank":19,"log_date":"20171116"},{"book_id":"'1aa2dc061d864f85b640551a0010f400'","book_num":4280,"book_rank":20,"log_date":"20171116"},{"book_id":"'e9dffa16a8b741aab2b612c22fc754a7'","book_num":4055,"book_rank":21,"log_date":"20171116"},{"book_id":"'1e2e7c0d94ee46d380607df8ad73db17'","book_num":3841,"book_rank":22,"log_date":"20171116"},{"book_id":"'8bd296afde7d4726b6cfa758773de030'","book_num":3705,"book_rank":23,"log_date":"20171116"},{"book_id":"'566b9a78ec0449c0822650af7e8be391'","book_num":3616,"book_rank":24,"log_date":"20171116"},{"book_id":"'a847790d1e57438a94934b615299d1a5'","book_num":3463,"book_rank":25,"log_date":"20171116"},{"book_id":"'ec1cde7557d640aa995ed4fdef63425b'","book_num":3032,"book_rank":26,"log_date":"20171116"},{"book_id":"'66ddfafed082458089e64c5524568859'","book_num":2743,"book_rank":27,"log_date":"20171116"},{"book_id":"'30ebb3e4cdd244118356e947ec50c490'","book_num":2615,"book_rank":28,"log_date":"20171116"},{"book_id":"'253521df08d149c7bcc7c89eb897250f'","book_num":2598,"book_rank":29,"log_date":"20171116"},{"book_id":"'123365f1d8804e39bfba8a68e9f0f97f'","book_num":2423,"book_rank":30,"log_date":"20171116"},{"book_id":"'47e9e0d1bbc249f69c59c60b564e4d28'","book_num":2365,"book_rank":31,"log_date":"20171116"},{"book_id":"'7f822c3dc5814cb991443083a863b78c'","book_num":2130,"book_rank":32,"log_date":"20171116"},{"book_id":"'9fa69b855999478daef0ed50c35da00e'","book_num":1892,"book_rank":33,"log_date":"20171116"},{"book_id":"'e8007f9dd2754d97afce055c13b2b867'","book_num":1670,"book_rank":34,"log_date":"20171116"},{"book_id":"'b62f2f28d02d4f888c4523cb0d87eae0'","book_num":1605,"book_rank":35,"log_date":"20171116"},{"book_id":"'9f7dffde590649f4a77479f6c400aa2d'","book_num":1573,"book_rank":36,"log_date":"20171116"},{"book_id":"'2972abd4b3934b9b96bc404652fafbea'","book_num":1573,"book_rank":36,"log_date":"20171116"},{"book_id":"'da1df87e65f84bcb8634d16cb808994e'","book_num":1565,"book_rank":38,"log_date":"20171116"},{"book_id":"'1aa028da91c84f9292ef64caa2d43a12'","book_num":1502,"book_rank":39,"log_date":"20171116"},{"book_id":"'c0d2b8d17f1f4de7a786947cd44f31dd'","book_num":1438,"book_rank":40,"log_date":"20171116"},{"book_id":"'b0a746867ecf4c52850077cc900746ac'","book_num":1402,"book_rank":41,"log_date":"20171116"},{"book_id":"'77233e7c55a546c0af8474e44f5891c7'","book_num":1390,"book_rank":42,"log_date":"20171116"},{"book_id":"'2e4cb5649a554fa5a5dc2022045e4cf7'","book_num":1344,"book_rank":43,"log_date":"20171116"},{"book_id":"'3c7714df30104b52b0738f066229d701'","book_num":1333,"book_rank":44,"log_date":"20171116"},{"book_id":"'2c2685eafa6f481ba890c136832c07a7'","book_num":1330,"book_rank":45,"log_date":"20171116"},{"book_id":"'ce1e9ab7e13543e6bb5cf13864f73060'","book_num":1309,"book_rank":46,"log_date":"20171116"},{"book_id":"'01c993a297214944ad7c9bc8767beec2'","book_num":1296,"book_rank":47,"log_date":"20171116"},{"book_id":"'3e62c2a294ad4350a286fc71fc711472'","book_num":1196,"book_rank":48,"log_date":"20171116"},{"book_id":"'13ae5b4fc36e45f4857fd0ddd0d09be0'","book_num":1186,"book_rank":49,"log_date":"20171116"},{"book_id":"'90842a84d245445e82c081ec5e837037'","book_num":1159,"book_rank":50,"log_date":"20171116"}]
  [INFO] [2017-11-30 17:07:35][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 30 17:07:32 CST 2017]; root of context hierarchy
  [WARN] [2017-11-30 17:33:02][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-11-30 17:33:39][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-11-30 17:34:00][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-11-30 17:34:45][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-11-30 17:35:04][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [INFO] [2017-11-30 18:19:55][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-30 18:19:56][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 30 18:19:56 CST 2017]; root of context hierarchy
  [INFO] [2017-11-30 18:19:56][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-30 18:19:56][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-11-30 18:19:58][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 30 18:19:56 CST 2017]; root of context hierarchy
  [INFO] [2017-11-30 18:23:36][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-30 18:23:37][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 30 18:23:37 CST 2017]; root of context hierarchy
  [INFO] [2017-11-30 18:23:37][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-30 18:23:37][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-11-30 18:23:39][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 30 18:23:37 CST 2017]; root of context hierarchy
  [INFO] [2017-11-30 18:29:43][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-30 18:29:44][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 30 18:29:44 CST 2017]; root of context hierarchy
  [INFO] [2017-11-30 18:29:44][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-30 18:29:44][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-11-30 18:29:45][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 30 18:29:44 CST 2017]; root of context hierarchy
  [INFO] [2017-11-30 18:30:09][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-30 18:30:10][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 30 18:30:10 CST 2017]; root of context hierarchy
  [INFO] [2017-11-30 18:30:11][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-30 18:30:11][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-11-30 18:30:12][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 30 18:30:10 CST 2017]; root of context hierarchy
  [INFO] [2017-11-30 18:30:37][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-11-30 18:30:38][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 30 18:30:38 CST 2017]; root of context hierarchy
  [INFO] [2017-11-30 18:30:38][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-11-30 18:30:38][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-11-30 18:30:39][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Nov 30 18:30:38 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 10:28:10][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-01 10:28:11][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 10:28:11 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 10:28:12][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-01 10:28:12][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-01 10:28:13][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 10:28:11 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 10:45:02][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-01 10:45:03][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 10:45:03 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 10:45:03][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-01 10:45:03][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [ERROR] [2017-12-01 10:45:04][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@5b7a8434] to prepare test instance [org.zyp.testmybatis.TestMyBatis@5c45d770]
  java.lang.IllegalStateException: Failed to load ApplicationContext
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:99)
	at org.springframework.test.context.DefaultTestContext.getApplicationContext(DefaultTestContext.java:101)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonService': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.topLessonMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'clickRateMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/clickRateMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/topLessonMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'linkedhashmap'.  Cause: java.lang.ClassNotFoundException: Cannot find class: linkedhashmap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/topLessonMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/topLessonMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'linkedhashmap'.  Cause: java.lang.ClassNotFoundException: Cannot find class: linkedhashmap
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:700)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:121)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:60)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.delegateLoading(AbstractDelegatingSmartContextLoader.java:100)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.loadContext(AbstractDelegatingSmartContextLoader.java:250)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContextInternal(CacheAwareContextLoaderDelegate.java:64)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)
	... 25 more
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.topLessonMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.raiseNoSuchBeanDefinitionException(DefaultListableBeanFactory.java:1100)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:960)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 41 more
[INFO] [2017-12-01 11:00:34][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-01 11:00:35][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 11:00:35 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 11:00:35][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-01 11:00:35][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-01 11:00:36][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 11:00:35 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 11:13:37][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-01 11:13:38][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 11:13:38 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 11:13:38][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-01 11:13:38][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-01 11:13:40][org.zyp.testmybatis.TestMyBatis][{"book_id":"45ff80ee9e84495a8a01880e125cc0b3","sum(book_num)":27535},{"book_id":"d717e0fa935e45cd87ca3d5a4f482de1","sum(book_num)":24817},{"book_id":"c8112e68ef0446cea43cb0fd3fabdd8f","sum(book_num)":14250},{"book_id":"fdd4517182db476baed076e8ea0c73a2","sum(book_num)":14073},{"book_id":"c4149360a00047babaed60b564bd9c94","sum(book_num)":10074},{"book_id":"99210b3f60db48aca913cb1973fec3c9","sum(book_num)":8885},{"book_id":"376890106c5748529a7cd428cb749db4","sum(book_num)":8700},{"book_id":"7d2cdcf9a5e54f3d835de6a063b95ad1","sum(book_num)":8104},{"book_id":"dc70f449c7a24b80970ba279b85c05b0","sum(book_num)":7514},{"book_id":"db4232cf381447b39317c4411b47d9d7","sum(book_num)":7008},{"book_id":"03124c0da03f4ce1937a2a4564d52309","sum(book_num)":6674},{"book_id":"5f03d165c079413a8311a70bf91813f1","sum(book_num)":6053},{"book_id":"5ba8ea74ef724ae28f9aca2e6a81f216","sum(book_num)":6044},{"book_id":"c41dcb4d3d054cad82760e081437ba55","sum(book_num)":5661},{"book_id":"396a65b821774507b5d0d2a92a1a287f","sum(book_num)":5643},{"book_id":"4a62e15e685f4f4f96826db8da23dbf7","sum(book_num)":4589},{"book_id":"851bb631ccc54564b0591cd073f779ff","sum(book_num)":4545},{"book_id":"440eef2f51dc46a68f58249de19228c6","sum(book_num)":4475},{"book_id":"fe6d1e9774674343b0c76af951ad36e1","sum(book_num)":4425},{"book_id":"1aa2dc061d864f85b640551a0010f400","sum(book_num)":4390},{"book_id":"e9dffa16a8b741aab2b612c22fc754a7","sum(book_num)":4174},{"book_id":"1e2e7c0d94ee46d380607df8ad73db17","sum(book_num)":3921},{"book_id":"8bd296afde7d4726b6cfa758773de030","sum(book_num)":3802},{"book_id":"566b9a78ec0449c0822650af7e8be391","sum(book_num)":3697},{"book_id":"a847790d1e57438a94934b615299d1a5","sum(book_num)":3546},{"book_id":"ec1cde7557d640aa995ed4fdef63425b","sum(book_num)":3102},{"book_id":"66ddfafed082458089e64c5524568859","sum(book_num)":2784},{"book_id":"30ebb3e4cdd244118356e947ec50c490","sum(book_num)":2690},{"book_id":"253521df08d149c7bcc7c89eb897250f","sum(book_num)":2666},{"book_id":"123365f1d8804e39bfba8a68e9f0f97f","sum(book_num)":2460},{"book_id":"47e9e0d1bbc249f69c59c60b564e4d28","sum(book_num)":2440},{"book_id":"7f822c3dc5814cb991443083a863b78c","sum(book_num)":2197},{"book_id":"9fa69b855999478daef0ed50c35da00e","sum(book_num)":1940},{"book_id":"e8007f9dd2754d97afce055c13b2b867","sum(book_num)":1696},{"book_id":"b62f2f28d02d4f888c4523cb0d87eae0","sum(book_num)":1663},{"book_id":"2972abd4b3934b9b96bc404652fafbea","sum(book_num)":1617},{"book_id":"9f7dffde590649f4a77479f6c400aa2d","sum(book_num)":1610},{"book_id":"da1df87e65f84bcb8634d16cb808994e","sum(book_num)":1581},{"book_id":"1aa028da91c84f9292ef64caa2d43a12","sum(book_num)":1529},{"book_id":"c0d2b8d17f1f4de7a786947cd44f31dd","sum(book_num)":1467},{"book_id":"b0a746867ecf4c52850077cc900746ac","sum(book_num)":1449},{"book_id":"77233e7c55a546c0af8474e44f5891c7","sum(book_num)":1421},{"book_id":"3c7714df30104b52b0738f066229d701","sum(book_num)":1390},{"book_id":"2c2685eafa6f481ba890c136832c07a7","sum(book_num)":1377},{"book_id":"2e4cb5649a554fa5a5dc2022045e4cf7","sum(book_num)":1375},{"book_id":"ce1e9ab7e13543e6bb5cf13864f73060","sum(book_num)":1347},{"book_id":"01c993a297214944ad7c9bc8767beec2","sum(book_num)":1327},{"book_id":"3e62c2a294ad4350a286fc71fc711472","sum(book_num)":1244},{"book_id":"13ae5b4fc36e45f4857fd0ddd0d09be0","sum(book_num)":1209},{"book_id":"90842a84d245445e82c081ec5e837037","sum(book_num)":1190},{"book_id":"5040f2f4b856441bbdaaed2b2df59530","sum(book_num)":1177},{"book_id":"351bb43c60314784ad3a7e7d553cd5f8","sum(book_num)":1152},{"book_id":"9b6ced512a8b47c4a3c7955955c1f9a0","sum(book_num)":1135},{"book_id":"df3d97d34d8f4d7ab3428c274099b205","sum(book_num)":1110},{"book_id":"e867b2ab654b4255bb4ddc30d480f5e6","sum(book_num)":1077},{"book_id":"4b6570cbe45040e78e73949e85cd1dfd","sum(book_num)":1072},{"book_id":"01eebcb08c2b4763b924d599c100fd69","sum(book_num)":1070},{"book_id":"4f881074c5b54d01bd5cf9428db1739d","sum(book_num)":999},{"book_id":"40b4e061c08540a7990bff60f2ae59e1","sum(book_num)":879},{"book_id":"846488dd9e064aa79c722bce258db2fb","sum(book_num)":844},{"book_id":"233096320fa246a6bc3fa8889bd7b7cc","sum(book_num)":840},{"book_id":"b30aa883c5904cb59f65456509354a71","sum(book_num)":828},{"book_id":"bb406373ac984673a14a2570e531a3ef","sum(book_num)":729},{"book_id":"38df5bd30f6b4ef48d95d89376cbd9cb","sum(book_num)":717},{"book_id":"37b364f9c00548d890de11e0b837262b","sum(book_num)":688},{"book_id":"9cf787d7967146f6a51c878b91e3d626","sum(book_num)":663},{"book_id":"3a8d6b655cdf4100b3554fbdca335347","sum(book_num)":631},{"book_id":"d8da835c7e8c47418e47fdab67500c9f","sum(book_num)":618},{"book_id":"9883ce7cf86d44d8bae44f6ef29928bb","sum(book_num)":615},{"book_id":"06dfc16fd96940508a7e8833f1ee0055","sum(book_num)":574},{"book_id":"a4c5a068fad049cca30ad9c1cef568a1","sum(book_num)":526},{"book_id":"de83bb0264124936bcc20ccc327b6296","sum(book_num)":524},{"book_id":"b944637b334848bb8aa85008610a7f49","sum(book_num)":520},{"book_id":"810e9c7186154342bd915ac60257ee1a","sum(book_num)":520},{"book_id":"00cab91f0a5a48cc86cfa4435c9c87b1","sum(book_num)":505},{"book_id":"9009b169df3048c8912b9b552c4e13b2","sum(book_num)":504},{"book_id":"e7d51f0778834837801615eca0102033","sum(book_num)":440},{"book_id":"15e23671df614b6ab20c71b9d84aa44f","sum(book_num)":433},{"book_id":"498befb008844ff28aa27eab31a669f1","sum(book_num)":433},{"book_id":"ed64eb9649454e54a26ad2a387110fff","sum(book_num)":429},{"book_id":"6302b2c2592541bbaf1d91dcac257418","sum(book_num)":423},{"book_id":"70539e880139461c846033ae15dba25b","sum(book_num)":400},{"book_id":"1ead233280064a7886a2488728002905","sum(book_num)":390},{"book_id":"e2e9399608ef40a39fa48a85aaabc8fe","sum(book_num)":376},{"book_id":"a40e61a708534e5490f0a9004f2c093c","sum(book_num)":366},{"book_id":"919e7a706a3646dfb8f6f819094658e5","sum(book_num)":357},{"book_id":"f348eaa44ef0421a827dd74002596e31","sum(book_num)":355},{"book_id":"ed79ab65b6d54622b83bc8895996265b","sum(book_num)":342},{"book_id":"1d35bc345c6142a992b802ab8963dd13","sum(book_num)":323},{"book_id":"980f3a4e9108438b9e12b798bd970807","sum(book_num)":319},{"book_id":"11f7670dd49847f4a61172d06c7f950b","sum(book_num)":309},{"book_id":"be5e1e7003764e848f0a4f60a1abd6b8","sum(book_num)":307},{"book_id":"1f9c72dbdaa743329eba873a27355fb6","sum(book_num)":305},{"book_id":"b55871617fb643dfaa2462d8f1aae201","sum(book_num)":302},{"book_id":"1fce544e9230409e980aac04a9e1c487","sum(book_num)":301},{"book_id":"50b0e3c100484641be3c095c4033fa85","sum(book_num)":275},{"book_id":"6ef5cf8b71004b70b18a10d190b170df","sum(book_num)":269},{"book_id":"c622b55386b54764972a0ddca4ec864d","sum(book_num)":263},{"book_id":"09216fb35aa64656a1838dfa7a0cb454","sum(book_num)":240},{"book_id":"a1188f68745843759bef47c634fb1810","sum(book_num)":237},{"book_id":"f36eaa9ab4fe4e65a0e6c1cb6cff115f","sum(book_num)":231},{"book_id":"9b623cd2ccf04ccbbbb7dc64f61f401a","sum(book_num)":216},{"book_id":"0cda21bc11f3486b87ee21e82417098b","sum(book_num)":208},{"book_id":"16eceb96568a48e99fe50322a2506f3f","sum(book_num)":200},{"book_id":"a60366445dee46078432bb0f8b1ac0f9","sum(book_num)":200},{"book_id":"5c44edfbdbad407ab8901f69209b7641","sum(book_num)":185},{"book_id":"00e6272b2f6947da8486b046e172eec8","sum(book_num)":179},{"book_id":"2d7c2bb3036c4f1683cd5db6d26dc8b7","sum(book_num)":169},{"book_id":"5ec6db156f694e798d6eaf08181e8ade","sum(book_num)":160},{"book_id":"c1bcfe3708e84948adde9b8c40dd149d","sum(book_num)":151},{"book_id":"13d37aa1b58b4406a5085b70c753f439","sum(book_num)":150},{"book_id":"9d1544e21a624ba3bc185b6e13c20deb","sum(book_num)":145},{"book_id":"781c2b25313c4e7b9ef897ce1f6bda5d","sum(book_num)":144},{"book_id":"c04ab8fac9af4ab097ecb1b28da9fe6f","sum(book_num)":140},{"book_id":"dee3298fe8b444b68113078a135882e3","sum(book_num)":137},{"book_id":"7cd7311cb0f041b9ac9dbdddfdc58d91","sum(book_num)":136},{"book_id":"22136a68dfe145c5ba9502b82a9c7775","sum(book_num)":128},{"book_id":"9f34633961014dbb9f02b3e6ee43a88e","sum(book_num)":126},{"book_id":"8481b5f2a3934aef8bd025f0f2c28d6d","sum(book_num)":124},{"book_id":"f9c0b9d084b548fa872166b6a7222255","sum(book_num)":117},{"book_id":"57a4cd0b33704102828b51bc4cceecda","sum(book_num)":115},{"book_id":"496d6503cbe7429f8551bcc9a3457410","sum(book_num)":115},{"book_id":"bffd4416937e4a06af644e913ec77a4e","sum(book_num)":113},{"book_id":"b658732a6c43491bb69912d1ed9ee1ef","sum(book_num)":112},{"book_id":"5eb2a70a2453447998a63a543b23f87f","sum(book_num)":111},{"book_id":"b86feffb48a44169b02ef885c5a1b94a","sum(book_num)":108},{"book_id":"cdddadad83124d59949822f8e6764f9e","sum(book_num)":104},{"book_id":"0e930eb2a2b54159b5f6300c09cb70e6","sum(book_num)":102},{"book_id":"4b380d3d47564ffc960e72835f893c51","sum(book_num)":101},{"book_id":"d8d9292061d34946abef24fabb606c8e","sum(book_num)":100},{"book_id":"16c075eabc59434cbf976e876507834d","sum(book_num)":99},{"book_id":"0f6a18d73d9b45628fcb71b4aa70a652","sum(book_num)":98},{"book_id":"d4a41e181431409cb482b0bf5c52d319","sum(book_num)":96},{"book_id":"350ab1e6a0f44e27bf06585187bdfd03","sum(book_num)":95},{"book_id":"3dc204a22edc4e5ea29c573cae065edf","sum(book_num)":95},{"book_id":"bc42e619783840daba951acb09153368","sum(book_num)":89},{"book_id":"6db9cbca359a4824a76ba1f8706ba58a","sum(book_num)":88},{"book_id":"24d967bc9ece48b8b73f04f7a2fffaa6","sum(book_num)":86},{"book_id":"bbadb7da35e546e28eb4a4c816f55d3a","sum(book_num)":86},{"book_id":"68b20d144d294343bf964739c0d99b7f","sum(book_num)":83},{"book_id":"dc5ed12f9d984e2fa0f824d141cddd94","sum(book_num)":80},{"book_id":"826c0aa85f6144e9bc28c0a9be3eace4","sum(book_num)":78},{"book_id":"5165c30e1ad7491eb6dd3e1536d7e614","sum(book_num)":77},{"book_id":"014f05d45e5c437d843e9ef0d302d0a9","sum(book_num)":76},{"book_id":"8ae2c5465a714e279a335434fb29bf83","sum(book_num)":75},{"book_id":"a2544e33b7c44c2480cc7f4c59e62be9","sum(book_num)":75},{"book_id":"a586975c0477462c8feb52bb1303bb1e","sum(book_num)":74},{"book_id":"4508ca48c4fd4e6abc6f0fca92a2de34","sum(book_num)":73},{"book_id":"d2272b95f3464318965793b9c69057d7","sum(book_num)":72},{"book_id":"a9dc8c634f3c4936b923ef60f8891627","sum(book_num)":72},{"book_id":"c3cabdf0a78c40b9b6844c601130b07c","sum(book_num)":72},{"book_id":"f453f69bb18f409ea0a66039a53d1f08","sum(book_num)":68},{"book_id":"7c13c27945ae43d18926db0e30fde02d","sum(book_num)":68},{"book_id":"cf077d41d82d454d9217536e8a7228bb","sum(book_num)":67},{"book_id":"5073b7c7cf7042359372827053385708","sum(book_num)":66},{"book_id":"93110b8616ce4c2baa46bf0eef70725c","sum(book_num)":64},{"book_id":"9beed71289af4105a2be7c4ab7dcfa2d","sum(book_num)":64},{"book_id":"fd79d80afe4b458fa472646c9a53e9b8","sum(book_num)":62},{"book_id":"7154b8fa2f974710a2fe6f33d6095ca9","sum(book_num)":61},{"book_id":"079340e8ac4a4ca9afa5de5558f396a4","sum(book_num)":60},{"book_id":"0b8b8e00b9a647c1832c7537adc82714","sum(book_num)":60},{"book_id":"2c29e880626845c28449db5fe5f38507","sum(book_num)":59},{"book_id":"9b7a537923c44373aa294499b54dec90","sum(book_num)":58},{"book_id":"67bffdba9e8741e7903d6fb990882864","sum(book_num)":58},{"book_id":"58e74468d0e24e9baa894c39df405d3e","sum(book_num)":58},{"book_id":"49a12fd9f42b470e98cfc4af08a06eb0","sum(book_num)":56},{"book_id":"f01474cf152b4f868c45cbb9d43e6c77","sum(book_num)":54},{"book_id":"37260ee6327044f49e231f9d6338c620","sum(book_num)":53},{"book_id":"1ad83507275c4294a739784887371b6f","sum(book_num)":51},{"book_id":"138f8de35b974cfda45903bc92e45d3d","sum(book_num)":48},{"book_id":"286e14032a994ae5bee8c1aed67abb1c","sum(book_num)":46},{"book_id":"2f470abec766424090aa2810017b059a","sum(book_num)":45},{"book_id":"14d1f24652614dd996ca445553bb05f4","sum(book_num)":42},{"book_id":"b06e3edfbeaf4e0398bf9f3c660a538b","sum(book_num)":42},{"book_id":"62aed01b29a04c20a38af051dab36c4b","sum(book_num)":42},{"book_id":"c33221b2932a4dfc8f8e6816096a56b0","sum(book_num)":41},{"book_id":"0c150aeabdf446aeba4ad23e2f427654","sum(book_num)":40},{"book_id":"60799f81b7584c5fb835056f8e9b5e69","sum(book_num)":40},{"book_id":"597c591fb939412ab7addf4db4e85122","sum(book_num)":39},{"book_id":"83d299240bd24d58a0985abe76cd6918","sum(book_num)":39},{"book_id":"e228b207d651403992bac2a665e1d170","sum(book_num)":38},{"book_id":"1f60db198f4444e697f6282de746b095","sum(book_num)":35},{"book_id":"7793e2dd786b47b7a97eb7f04d469c58","sum(book_num)":34},{"book_id":"7a43632dfa3543c9b14eb3d8fb424570","sum(book_num)":34},{"book_id":"097dbb8ebdd344379a0e0adfb84b27b9","sum(book_num)":34},{"book_id":"18228fdcc8a443c390f150412f38dbff","sum(book_num)":34},{"book_id":"27de13406734427e91483495b5d53925","sum(book_num)":33},{"book_id":"e978a13953214c19876954a232b7dec1","sum(book_num)":33},{"book_id":"c1f3773ee21e490aaf6040c860e4869d","sum(book_num)":32},{"book_id":"08ad04c0955a44d3b342a06f583854ed","sum(book_num)":32},{"book_id":"76dbd31a5efd4f8190509c40d7cf8702","sum(book_num)":31},{"book_id":"63d9d1ff36ea414787ac7bd1ec2effc4","sum(book_num)":31},{"book_id":"e262a7a3f62645ebba377a8b33cfc6e2","sum(book_num)":31},{"book_id":"93e7bc71bab04e2196f97a10b7e19bdb","sum(book_num)":31},{"book_id":"73dd1a537ca74c58ab86d4f682e88f54","sum(book_num)":31},{"book_id":"9c183a1696874f68bc0eebefa03c459b","sum(book_num)":30},{"book_id":"d496ec4fb2f9448ca192e625e83e6d99","sum(book_num)":30},{"book_id":"1070a2c5adb240c692ec00e2dd83b456","sum(book_num)":29},{"book_id":"62bdbfe562294798b55df25622793e6f","sum(book_num)":29},{"book_id":"e12f0f73bf54425fbdafce85e3f5d80a","sum(book_num)":29},{"book_id":"7cc767458b4c49a3a6344887624e9634","sum(book_num)":29},{"book_id":"52542974e7494b6cab1a3382d7e9a339","sum(book_num)":28},{"book_id":"76f128dfc180424389cdd99c7fc39834","sum(book_num)":28},{"book_id":"e7713fced13f46cc9342a7b57ef04985","sum(book_num)":28},{"book_id":"34df0011ba05436f8f02fe389fa20f00","sum(book_num)":27},{"book_id":"7c07016cec834dc2a7d58246b7475fa5","sum(book_num)":26},{"book_id":"ce49214bfc4d4e20896cccf06424bf74","sum(book_num)":25},{"book_id":"6c7ad672f2bc46568eee2ae481b2a3a5","sum(book_num)":25},{"book_id":"114a1c3c005440d2917f8d69b5ca316d","sum(book_num)":24},{"book_id":"8d2dd26d507d41f58c990832eea70e4b","sum(book_num)":24},{"book_id":"0567d6f508df418aa156df90cf05811a","sum(book_num)":24},{"book_id":"9bc6c8044325464da199d81b21c1d970","sum(book_num)":23},{"book_id":"dc581668c8104fee9af1bd8ffca72407","sum(book_num)":22},{"book_id":"cae308e334594f1187a76345cd6b8efb","sum(book_num)":22},{"book_id":"8505a959bc2245ce8f6f012e31e57f5b","sum(book_num)":21},{"book_id":"8862f49192e8426cb83ac0c4eabac4ef","sum(book_num)":21},{"book_id":"59e2c90e4ce04891aa108cb52da9b6fa","sum(book_num)":20},{"book_id":"c6278e24f1b44f4691cb67c729960bb5","sum(book_num)":20},{"book_id":"d61be36d95794fe0a1881c5ad4393957","sum(book_num)":20},{"book_id":"81bd8e77de544de6a4c5b636a0bbafd0","sum(book_num)":19},{"book_id":"53e3996e10604c748952b7898a6a729e","sum(book_num)":19},{"book_id":"73bd55f442114694b3558eee57aeadcf","sum(book_num)":19},{"book_id":"fda0b692457945bb968585877035cec6","sum(book_num)":18},{"book_id":"729540a00cc84fbb874c735f5d853b65","sum(book_num)":18},{"book_id":"221947d326234115bbdfa5fd04a9a09d","sum(book_num)":18},{"book_id":"9c90fb39c87448049a867e8d1848ab34","sum(book_num)":18},{"book_id":"8318664c753d414d83eabedbc8a122e2","sum(book_num)":18},{"book_id":"fe617146f40f4ef4a936d7917497f833","sum(book_num)":18},{"book_id":"166a6067ebea4bfa87ff8fdcf2adf063","sum(book_num)":18},{"book_id":"c8333d0341304bf5b17dcce06c9f8225","sum(book_num)":17},{"book_id":"8a20ae9b41078847014129922bc80d7c","sum(book_num)":16},{"book_id":"16d5d1c809314d0fb611a1e56f677012","sum(book_num)":16},{"book_id":"ebd9df9b55044032b1e1d3859197a2cd","sum(book_num)":16},{"book_id":"61b5aba8df20496e8243bdc2d2701719","sum(book_num)":15},{"book_id":"d1fe9868a6624fca9e05baac29602ac1","sum(book_num)":15},{"book_id":"c5b028056e364dc8b8c8018d37c29a22","sum(book_num)":15},{"book_id":"e4748716566d4426b1d87a3581f1880a","sum(book_num)":15},{"book_id":"f41e01f289af47d4ad071ed9e2ca5d34","sum(book_num)":15},{"book_id":"5325433d83214977a5274f6fee054666","sum(book_num)":14},{"book_id":"843c3ce0f2e447278b7e05baa85ecae0","sum(book_num)":13},{"book_id":"92244198d9f2476cb57ef289511e1589","sum(book_num)":13},{"book_id":"279ce6c19da64b3db419014722dd3978","sum(book_num)":13},{"book_id":"11d80bb736db47c9a3dda8fd438ce1a4","sum(book_num)":12},{"book_id":"39d79c38d1184f8981a631fdf5b5ed1c","sum(book_num)":12},{"book_id":"9950e2bd7b2944d189c83fb077a186eb","sum(book_num)":12},{"book_id":"bd6591b98fbb4147bfce9d051ca81d52","sum(book_num)":11},{"book_id":"8d5221f113af4fa99f05d0728e41a8ee","sum(book_num)":11},{"book_id":"21b4a6cb5fcd4c6a91afebb8b79b235a","sum(book_num)":11},{"book_id":"85edef8203d94417aa017028430edf14","sum(book_num)":11},{"book_id":"db19bd910b75463691a4b3028b649cf8","sum(book_num)":11},{"book_id":"85a9cc7dec6f4bff82d642c004a5dbc4","sum(book_num)":11},{"book_id":"f3d0353a5ba24cf8825f6d11f2ac02b6","sum(book_num)":10},{"book_id":"913b2f0f161a48a4b7d56f144535b1f5","sum(book_num)":10},{"book_id":"e71b1876ae034f53a9944a2b47a2f2f6","sum(book_num)":9},{"book_id":"7da4486829164c258fe92dcb041510c8","sum(book_num)":9},{"book_id":"bf51ed6eaf194b51965877bf577aff5a","sum(book_num)":9},{"book_id":"b6b28bc180be4718bee1cf87c0998598","sum(book_num)":9},{"book_id":"ecaabf24f73b43cbb9caa6cf3457c374","sum(book_num)":9},{"book_id":"51116b7ad8d146289a8ba43ef69a3221","sum(book_num)":9},{"book_id":"31f84fdb5c0347768e95a2f9ee373be4","sum(book_num)":8},{"book_id":"2d8ee6d8042141819762c18bf8ec2d4b","sum(book_num)":8},{"book_id":"32e479a9a09c4af7bd4482a708942af1","sum(book_num)":8},{"book_id":"22ac386c62e64e93af229b59c1570a82","sum(book_num)":8},{"book_id":"eda3662f66074db19751f809730d1804","sum(book_num)":8},{"book_id":"2c8756a5bf454a94a6d5220d346ffbbf","sum(book_num)":8},{"book_id":"93c9e0a1653543628fc516ef2fffd0cc","sum(book_num)":8},{"book_id":"228c4fd30b934921aa1068eaaec63675","sum(book_num)":8},{"book_id":"4e1a31577b9b4907a43fee5371fbf186","sum(book_num)":8},{"book_id":"bff035fb46aa4f808c4e2a6fbb5be619","sum(book_num)":8},{"book_id":"d62332d2157d49e887130337b890954f","sum(book_num)":8},{"book_id":"9b1e1469724f487ca0b7491d8ccdc282","sum(book_num)":7},{"book_id":"b7e56610f05944c49c229bcc13645f91","sum(book_num)":7},{"book_id":"d71d86aade844aa7811a8b9a002f9625","sum(book_num)":7},{"book_id":"310830153fbc4125a8b7bc594c0abb7b","sum(book_num)":7},{"book_id":"b5fe959e81544e38a779e832c7d04953","sum(book_num)":7},{"book_id":"642bbc668b66416da03d40d98a5c16f7","sum(book_num)":7},{"book_id":"5f7dc383e8374f5f9fe4953b2482a4dd","sum(book_num)":6},{"book_id":"d75926ac587d489ea07cf36adf36c967","sum(book_num)":6},{"book_id":"fa6bd7f742654f29b4acf9678252323b","sum(book_num)":6},{"book_id":"22e84a6eab784df29d8b4e3bf2d2ce8e","sum(book_num)":6},{"book_id":"e47c44c7574a4c1aac6de1d322c96168","sum(book_num)":6},{"book_id":"ac2d0af9f5fe491eb741ea9d3caebf49","sum(book_num)":6},{"book_id":"56cc98aa112c4189adb7f5529b1f7ab2","sum(book_num)":6},{"book_id":"a44096b8bba84849812e8aab45606c44","sum(book_num)":5},{"book_id":"4b9eddfc789345a2abfab2e36c8d7377","sum(book_num)":5},{"book_id":"7cb3679d86064a26b53251dfa9938efe","sum(book_num)":5},{"book_id":"b31528c73dfd478caa7304f56a0a601b","sum(book_num)":5},{"book_id":"460dd3fdd314446ea919b3da943451b8","sum(book_num)":5},{"book_id":"64b90a8d3cda4cb596ce4e6d1b6c91e0","sum(book_num)":5},{"book_id":"ed27311353fc425c870f87b1c776378e","sum(book_num)":5},{"book_id":"33f6bafcb5d643cfbcbf5d6829613b9d","sum(book_num)":5},{"book_id":"8f422f365f634af59173f084b15fd12b","sum(book_num)":5},{"book_id":"2b2ff9d9e3fd4973a871a4c14a3ee493","sum(book_num)":5},{"book_id":"a3e32a42d9db418190ae7130d1932263","sum(book_num)":5},{"book_id":"0e515ef90e2148d99dc6225ba109a57c","sum(book_num)":5},{"book_id":"505bd60904ca486a951bad9f0e0444d8","sum(book_num)":5},{"book_id":"f00448fa976444f0b57abb1c057b2e8e","sum(book_num)":5},{"book_id":"2a167edfccfe4fcca18d80deb868c648","sum(book_num)":5},{"book_id":"fa73a70b6ff742a5bc6a9760d6b18298","sum(book_num)":5},{"book_id":"03344839ced3406e933eefeddadfe7a5","sum(book_num)":4},{"book_id":"ab5c7c7c8592480595fc6c44c77197df","sum(book_num)":4},{"book_id":"1a16731f75404fec95af6c407d5a648e","sum(book_num)":4},{"book_id":"95b8139aa34b49c3afe0bf34d459773b","sum(book_num)":4},{"book_id":"75a3e0979c5d4c1b9f24f380ef0d21ed","sum(book_num)":4},{"book_id":"aead24a0df1a481ba8a2caa1b3477bfe","sum(book_num)":4},{"book_id":"4fbc317cb59047ccbb56062b127832a9","sum(book_num)":4},{"book_id":"8985724bc788426b8d9e253de12a58e9","sum(book_num)":4},{"book_id":"9e93e11332144029b238be9c104105eb","sum(book_num)":4},{"book_id":"c339ad7e10144f29804c4ae8c5a0a8c9","sum(book_num)":4},{"book_id":"3263824ab58e4b4a850c4191212282bf","sum(book_num)":4},{"book_id":"2386a9b109d5404db7fdd5c803ed1d59","sum(book_num)":4},{"book_id":"1bac4575175b4a02bd7f36bf15350cf1","sum(book_num)":4},{"book_id":"42001a7869db4cb38350d4e31b1ae8a0","sum(book_num)":4},{"book_id":"5871216a9de3488ca1c77d62134ae250","sum(book_num)":4},{"book_id":"8a8feccad34f480a9bb64f1172aa2a9f","sum(book_num)":4},{"book_id":"b1422214573941d3882b83fc7937f3f9","sum(book_num)":4},{"book_id":"9e197c5436cb45f0baad606ada626b43","sum(book_num)":4},{"book_id":"da43da0d5e6c48b9a456372fe0a91e70","sum(book_num)":3},{"book_id":"90019b70496247d48448f01255b282fb","sum(book_num)":3},{"book_id":"f6c04897f9f440428265a553ccd6c2c3","sum(book_num)":3},{"book_id":"30631a675c5a4e8eb7fdbfa94ccb70b5","sum(book_num)":3},{"book_id":"b0aca910ea2f46f6997ec9c18f363a13","sum(book_num)":3},{"book_id":"ba8dc4064ae94c46b8030bfc53bef2a0","sum(book_num)":3},{"book_id":"21b59a9615c4473aaa0dc8c62151fd59","sum(book_num)":3},{"book_id":"870f6071a87a41d4829f929b076f51cf","sum(book_num)":3},{"book_id":"8b0d5163c9784e1488ec30434e8c5ae0","sum(book_num)":3},{"book_id":"7d070e2f5ae9402bbad5f41f26ba2824","sum(book_num)":3},{"book_id":"e8e93e676cde4357bb1db6b1aff94eff","sum(book_num)":3},{"book_id":"23a44bf982d34398b6b14f539a679fc1","sum(book_num)":3},{"book_id":"28c6d1e51fb54107ba22662de01c9c53","sum(book_num)":3},{"book_id":"7840b4ca4dbf4873929b82ef153019be","sum(book_num)":3},{"book_id":"575603ac138d4acb9a33741ebe0ec140","sum(book_num)":3},{"book_id":"1420eb5d336d4be99d033d86bff9e3b4","sum(book_num)":3},{"book_id":"7d93d86053e04f04a69566264352a24d","sum(book_num)":3},{"book_id":"b67d42750ad440958c46250dc45f32cb","sum(book_num)":3},{"book_id":"80ced25f6fe242edb4591875e9549275","sum(book_num)":3},{"book_id":"96a96d4f435d4fc68c9a0ce0fd95108b","sum(book_num)":3},{"book_id":"c332abe2a20e40fba8d676cc34711116","sum(book_num)":2},{"book_id":"ea52e3790a8f4a6a99dce60e5b66d0fb","sum(book_num)":2},{"book_id":"afd996f9788b4b5d8633c6cd15429666","sum(book_num)":2},{"book_id":"e10b3d8cfb284ba0a23f0b0e9dc7aff6","sum(book_num)":2},{"book_id":"0ae7c61858054bfdbcff0829d95efbc3","sum(book_num)":2},{"book_id":"40b7638b0c9e4d5693b1108f5067e189","sum(book_num)":2},{"book_id":"12faed3dee784a7695d33128fd0d618f","sum(book_num)":2},{"book_id":"222f31c8df224e3c846ec9d276a9c69b","sum(book_num)":2},{"book_id":"32ecb2bc54644f53911c14ceecfd7596","sum(book_num)":2},{"book_id":"56aeb24f224c4821aa6129d7f76a1e37","sum(book_num)":2},{"book_id":"e9f5924a89114608a5c98e926ed2546d","sum(book_num)":2},{"book_id":"cc617f82c261485c90a1591180a18ef7","sum(book_num)":2},{"book_id":"e0b3aca7afa34b4db727ac02dd143014","sum(book_num)":2},{"book_id":"f64ecacbdfaa4e39b89667aa47e9aac7","sum(book_num)":2},{"book_id":"38084312e0e94dac814a3e218fa460d0","sum(book_num)":2},{"book_id":"0c0ed804855e49008563b459163de2d7","sum(book_num)":2},{"book_id":"cc5064df050c4d019fd3765ed329489b","sum(book_num)":2},{"book_id":"e096b806058c4f1f82613addaf720f93","sum(book_num)":2},{"book_id":"2f33a0f1945b442dad6ccb02822dcec7","sum(book_num)":2},{"book_id":"4600f81b14f04595a035423eaf806206","sum(book_num)":2},{"book_id":"5fb3002312c34a68ba795c1a6fe20459","sum(book_num)":2},{"book_id":"8d18ec7b2fbb4cc6b993c604d3b2abdf","sum(book_num)":2},{"book_id":"ceccc2dacfe84786907d0f210f48c333","sum(book_num)":2},{"book_id":"8827c133125c447bb55ade66b60f69e9","sum(book_num)":1},{"book_id":"ccd54c62b1344f288a1c2c08bc8296ad","sum(book_num)":1},{"book_id":"f20e456024ec4b139fe51d1509a3b98c","sum(book_num)":1},{"book_id":"2806b7efcd4b4549aeb09ae487e69073","sum(book_num)":1},{"book_id":"1b054162e9f744fd98a3e70ef0ff280d","sum(book_num)":1},{"book_id":"e92545f63deb4a9d92e5d3122d0cf24c","sum(book_num)":1},{"book_id":"0f18dbb526e6477e8d6cf3ea061cca3e","sum(book_num)":1},{"book_id":"584c562ba3264ef3b07dd952abdd1a31","sum(book_num)":1},{"book_id":"72e8cf655cdb4a1b9de3d69eac27fa7e","sum(book_num)":1},{"book_id":"dfc608db53ef4839b96762d698543249","sum(book_num)":1},{"book_id":"eeb1a05eeb434d9489e25daa2ddd5ce1","sum(book_num)":1},{"book_id":"359b313431e24e56a3a8f28a1ae1a61f","sum(book_num)":1},{"book_id":"66563ba79d464812840259841aa0a0fd","sum(book_num)":1},{"book_id":"bfcf488265f3445a9c55288b917adeca","sum(book_num)":1},{"book_id":"3d3fd0bfc9124d268d66c81c9cba154b","sum(book_num)":1},{"book_id":"31871473b9114852b7fa374c3d21827d","sum(book_num)":1},{"book_id":"9a8a842d77cc4a498992fbf28b0d9490","sum(book_num)":1},{"book_id":"dcbced10965447ecbfe75fe60a2f9214","sum(book_num)":1},{"book_id":"f070200ea42845b5a380601215f30111","sum(book_num)":1},{"book_id":"8f5e569fe799488bbf6b0b16f257b2f6","sum(book_num)":1},{"book_id":"1f9bfe24b47f409ca51f1612ad394359","sum(book_num)":1},{"book_id":"222","sum(book_num)":1},{"book_id":"5bd00399da624d73a430e40b0d0dcf61","sum(book_num)":1},{"book_id":"c828363caf8e4bf9bdb14768ecd94519","sum(book_num)":1},{"book_id":"edd5754937034948ba04613f35ba7689","sum(book_num)":1},{"book_id":"352de4cacb70499587ab0b01254703ee","sum(book_num)":1},{"book_id":"7c65614757e046328590c3b06af24419","sum(book_num)":1},{"book_id":"1a15557288504493b24c5cd2e1cb1e7f","sum(book_num)":1},{"book_id":"a078cf789a0644f0b0b8ee625b93d717","sum(book_num)":1},{"book_id":"e796877b93664e1ead623e57c039921a","sum(book_num)":1},{"book_id":"f9f0ca9ab3734188ba88c3cf752839b4","sum(book_num)":1},{"book_id":"996ba02c1fdf4e90aad491e8b12ca74d","sum(book_num)":1},{"book_id":"ae4a860e9dac45ac92289faf77d5741b","sum(book_num)":1},{"book_id":"13945e6cfa9b490d944150c1dda054f2","sum(book_num)":1},{"book_id":"6711f32af9eb4a1c97b2a5676c2bf232","sum(book_num)":1},{"book_id":"91fe296a9bfc4ae8a8dcf380c4489c6c","sum(book_num)":1},{"book_id":"a86cb40e1e1d442ca19dfb3b3fe11daf","sum(book_num)":1},{"book_id":"eb92c173d0ba4332a9775ff50a5d623e","sum(book_num)":1},{"book_id":"37fabda094ae4aa2b20515b39e0c2bf8","sum(book_num)":1},{"book_id":"b8f8d614cf8344bfb2be9de0aa6f17c7","sum(book_num)":1},{"book_id":"3aa9cb39fc474cd68a7196ad9d06a4ec","sum(book_num)":1},{"book_id":"338a2822f05343199516fdb3349ad6f6","sum(book_num)":1},{"book_id":"9bab6e40bb6f4b268fd11e727462d5f6","sum(book_num)":1},{"book_id":"e071f1f8fcdf4a0eaaa4cf6bf56f7ca9","sum(book_num)":1},{"book_id":"37095d0c6d224c23be26d309a7f451a3","sum(book_num)":1},{"book_id":"8dae4d7006ab4fe1b346429bf14d02a9","sum(book_num)":1},{"book_id":"39416576cb0b419c89a93ef52677093a","sum(book_num)":1}]
  [INFO] [2017-12-01 11:13:40][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 11:13:38 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 11:23:39][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-01 11:23:40][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 11:23:40 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 11:23:40][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-01 11:23:40][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-01 11:23:41][org.zyp.testmybatis.TestMyBatis][{"book_id":"45ff80ee9e84495a8a01880e125cc0b3","num":27535},{"book_id":"d717e0fa935e45cd87ca3d5a4f482de1","num":24817},{"book_id":"c8112e68ef0446cea43cb0fd3fabdd8f","num":14250},{"book_id":"fdd4517182db476baed076e8ea0c73a2","num":14073},{"book_id":"c4149360a00047babaed60b564bd9c94","num":10074},{"book_id":"99210b3f60db48aca913cb1973fec3c9","num":8885},{"book_id":"376890106c5748529a7cd428cb749db4","num":8700},{"book_id":"7d2cdcf9a5e54f3d835de6a063b95ad1","num":8104},{"book_id":"dc70f449c7a24b80970ba279b85c05b0","num":7514},{"book_id":"db4232cf381447b39317c4411b47d9d7","num":7008},{"book_id":"03124c0da03f4ce1937a2a4564d52309","num":6674},{"book_id":"5f03d165c079413a8311a70bf91813f1","num":6053},{"book_id":"5ba8ea74ef724ae28f9aca2e6a81f216","num":6044},{"book_id":"c41dcb4d3d054cad82760e081437ba55","num":5661},{"book_id":"396a65b821774507b5d0d2a92a1a287f","num":5643},{"book_id":"4a62e15e685f4f4f96826db8da23dbf7","num":4589},{"book_id":"851bb631ccc54564b0591cd073f779ff","num":4545},{"book_id":"440eef2f51dc46a68f58249de19228c6","num":4475},{"book_id":"fe6d1e9774674343b0c76af951ad36e1","num":4425},{"book_id":"1aa2dc061d864f85b640551a0010f400","num":4390},{"book_id":"e9dffa16a8b741aab2b612c22fc754a7","num":4174},{"book_id":"1e2e7c0d94ee46d380607df8ad73db17","num":3921},{"book_id":"8bd296afde7d4726b6cfa758773de030","num":3802},{"book_id":"566b9a78ec0449c0822650af7e8be391","num":3697},{"book_id":"a847790d1e57438a94934b615299d1a5","num":3546},{"book_id":"ec1cde7557d640aa995ed4fdef63425b","num":3102},{"book_id":"66ddfafed082458089e64c5524568859","num":2784},{"book_id":"30ebb3e4cdd244118356e947ec50c490","num":2690},{"book_id":"253521df08d149c7bcc7c89eb897250f","num":2666},{"book_id":"123365f1d8804e39bfba8a68e9f0f97f","num":2460},{"book_id":"47e9e0d1bbc249f69c59c60b564e4d28","num":2440},{"book_id":"7f822c3dc5814cb991443083a863b78c","num":2197},{"book_id":"9fa69b855999478daef0ed50c35da00e","num":1940},{"book_id":"e8007f9dd2754d97afce055c13b2b867","num":1696},{"book_id":"b62f2f28d02d4f888c4523cb0d87eae0","num":1663},{"book_id":"2972abd4b3934b9b96bc404652fafbea","num":1617},{"book_id":"9f7dffde590649f4a77479f6c400aa2d","num":1610},{"book_id":"da1df87e65f84bcb8634d16cb808994e","num":1581},{"book_id":"1aa028da91c84f9292ef64caa2d43a12","num":1529},{"book_id":"c0d2b8d17f1f4de7a786947cd44f31dd","num":1467},{"book_id":"b0a746867ecf4c52850077cc900746ac","num":1449},{"book_id":"77233e7c55a546c0af8474e44f5891c7","num":1421},{"book_id":"3c7714df30104b52b0738f066229d701","num":1390},{"book_id":"2c2685eafa6f481ba890c136832c07a7","num":1377},{"book_id":"2e4cb5649a554fa5a5dc2022045e4cf7","num":1375},{"book_id":"ce1e9ab7e13543e6bb5cf13864f73060","num":1347},{"book_id":"01c993a297214944ad7c9bc8767beec2","num":1327},{"book_id":"3e62c2a294ad4350a286fc71fc711472","num":1244},{"book_id":"13ae5b4fc36e45f4857fd0ddd0d09be0","num":1209},{"book_id":"90842a84d245445e82c081ec5e837037","num":1190},{"book_id":"5040f2f4b856441bbdaaed2b2df59530","num":1177},{"book_id":"351bb43c60314784ad3a7e7d553cd5f8","num":1152},{"book_id":"9b6ced512a8b47c4a3c7955955c1f9a0","num":1135},{"book_id":"df3d97d34d8f4d7ab3428c274099b205","num":1110},{"book_id":"e867b2ab654b4255bb4ddc30d480f5e6","num":1077},{"book_id":"4b6570cbe45040e78e73949e85cd1dfd","num":1072},{"book_id":"01eebcb08c2b4763b924d599c100fd69","num":1070},{"book_id":"4f881074c5b54d01bd5cf9428db1739d","num":999},{"book_id":"40b4e061c08540a7990bff60f2ae59e1","num":879},{"book_id":"846488dd9e064aa79c722bce258db2fb","num":844},{"book_id":"233096320fa246a6bc3fa8889bd7b7cc","num":840},{"book_id":"b30aa883c5904cb59f65456509354a71","num":828},{"book_id":"bb406373ac984673a14a2570e531a3ef","num":729},{"book_id":"38df5bd30f6b4ef48d95d89376cbd9cb","num":717},{"book_id":"37b364f9c00548d890de11e0b837262b","num":688},{"book_id":"9cf787d7967146f6a51c878b91e3d626","num":663},{"book_id":"3a8d6b655cdf4100b3554fbdca335347","num":631},{"book_id":"d8da835c7e8c47418e47fdab67500c9f","num":618},{"book_id":"9883ce7cf86d44d8bae44f6ef29928bb","num":615},{"book_id":"06dfc16fd96940508a7e8833f1ee0055","num":574},{"book_id":"a4c5a068fad049cca30ad9c1cef568a1","num":526},{"book_id":"de83bb0264124936bcc20ccc327b6296","num":524},{"book_id":"810e9c7186154342bd915ac60257ee1a","num":520},{"book_id":"b944637b334848bb8aa85008610a7f49","num":520},{"book_id":"00cab91f0a5a48cc86cfa4435c9c87b1","num":505},{"book_id":"9009b169df3048c8912b9b552c4e13b2","num":504},{"book_id":"e7d51f0778834837801615eca0102033","num":440},{"book_id":"15e23671df614b6ab20c71b9d84aa44f","num":433},{"book_id":"498befb008844ff28aa27eab31a669f1","num":433},{"book_id":"ed64eb9649454e54a26ad2a387110fff","num":429},{"book_id":"6302b2c2592541bbaf1d91dcac257418","num":423},{"book_id":"70539e880139461c846033ae15dba25b","num":400},{"book_id":"1ead233280064a7886a2488728002905","num":390},{"book_id":"e2e9399608ef40a39fa48a85aaabc8fe","num":376},{"book_id":"a40e61a708534e5490f0a9004f2c093c","num":366},{"book_id":"919e7a706a3646dfb8f6f819094658e5","num":357},{"book_id":"f348eaa44ef0421a827dd74002596e31","num":355},{"book_id":"ed79ab65b6d54622b83bc8895996265b","num":342},{"book_id":"1d35bc345c6142a992b802ab8963dd13","num":323},{"book_id":"980f3a4e9108438b9e12b798bd970807","num":319},{"book_id":"11f7670dd49847f4a61172d06c7f950b","num":309},{"book_id":"be5e1e7003764e848f0a4f60a1abd6b8","num":307},{"book_id":"1f9c72dbdaa743329eba873a27355fb6","num":305},{"book_id":"b55871617fb643dfaa2462d8f1aae201","num":302},{"book_id":"1fce544e9230409e980aac04a9e1c487","num":301},{"book_id":"50b0e3c100484641be3c095c4033fa85","num":275},{"book_id":"6ef5cf8b71004b70b18a10d190b170df","num":269},{"book_id":"c622b55386b54764972a0ddca4ec864d","num":263},{"book_id":"09216fb35aa64656a1838dfa7a0cb454","num":240},{"book_id":"a1188f68745843759bef47c634fb1810","num":237},{"book_id":"f36eaa9ab4fe4e65a0e6c1cb6cff115f","num":231},{"book_id":"9b623cd2ccf04ccbbbb7dc64f61f401a","num":216},{"book_id":"0cda21bc11f3486b87ee21e82417098b","num":208},{"book_id":"a60366445dee46078432bb0f8b1ac0f9","num":200},{"book_id":"16eceb96568a48e99fe50322a2506f3f","num":200},{"book_id":"5c44edfbdbad407ab8901f69209b7641","num":185},{"book_id":"00e6272b2f6947da8486b046e172eec8","num":179},{"book_id":"2d7c2bb3036c4f1683cd5db6d26dc8b7","num":169},{"book_id":"5ec6db156f694e798d6eaf08181e8ade","num":160},{"book_id":"c1bcfe3708e84948adde9b8c40dd149d","num":151},{"book_id":"13d37aa1b58b4406a5085b70c753f439","num":150},{"book_id":"9d1544e21a624ba3bc185b6e13c20deb","num":145},{"book_id":"781c2b25313c4e7b9ef897ce1f6bda5d","num":144},{"book_id":"c04ab8fac9af4ab097ecb1b28da9fe6f","num":140},{"book_id":"dee3298fe8b444b68113078a135882e3","num":137},{"book_id":"7cd7311cb0f041b9ac9dbdddfdc58d91","num":136},{"book_id":"22136a68dfe145c5ba9502b82a9c7775","num":128},{"book_id":"9f34633961014dbb9f02b3e6ee43a88e","num":126},{"book_id":"8481b5f2a3934aef8bd025f0f2c28d6d","num":124},{"book_id":"f9c0b9d084b548fa872166b6a7222255","num":117},{"book_id":"496d6503cbe7429f8551bcc9a3457410","num":115},{"book_id":"57a4cd0b33704102828b51bc4cceecda","num":115},{"book_id":"bffd4416937e4a06af644e913ec77a4e","num":113},{"book_id":"b658732a6c43491bb69912d1ed9ee1ef","num":112},{"book_id":"5eb2a70a2453447998a63a543b23f87f","num":111},{"book_id":"b86feffb48a44169b02ef885c5a1b94a","num":108},{"book_id":"cdddadad83124d59949822f8e6764f9e","num":104},{"book_id":"0e930eb2a2b54159b5f6300c09cb70e6","num":102},{"book_id":"4b380d3d47564ffc960e72835f893c51","num":101},{"book_id":"d8d9292061d34946abef24fabb606c8e","num":100},{"book_id":"16c075eabc59434cbf976e876507834d","num":99},{"book_id":"0f6a18d73d9b45628fcb71b4aa70a652","num":98},{"book_id":"d4a41e181431409cb482b0bf5c52d319","num":96},{"book_id":"3dc204a22edc4e5ea29c573cae065edf","num":95},{"book_id":"350ab1e6a0f44e27bf06585187bdfd03","num":95},{"book_id":"bc42e619783840daba951acb09153368","num":89},{"book_id":"6db9cbca359a4824a76ba1f8706ba58a","num":88},{"book_id":"bbadb7da35e546e28eb4a4c816f55d3a","num":86},{"book_id":"24d967bc9ece48b8b73f04f7a2fffaa6","num":86},{"book_id":"68b20d144d294343bf964739c0d99b7f","num":83},{"book_id":"dc5ed12f9d984e2fa0f824d141cddd94","num":80},{"book_id":"826c0aa85f6144e9bc28c0a9be3eace4","num":78},{"book_id":"5165c30e1ad7491eb6dd3e1536d7e614","num":77},{"book_id":"014f05d45e5c437d843e9ef0d302d0a9","num":76},{"book_id":"a2544e33b7c44c2480cc7f4c59e62be9","num":75},{"book_id":"8ae2c5465a714e279a335434fb29bf83","num":75},{"book_id":"a586975c0477462c8feb52bb1303bb1e","num":74},{"book_id":"4508ca48c4fd4e6abc6f0fca92a2de34","num":73},{"book_id":"c3cabdf0a78c40b9b6844c601130b07c","num":72},{"book_id":"a9dc8c634f3c4936b923ef60f8891627","num":72},{"book_id":"d2272b95f3464318965793b9c69057d7","num":72},{"book_id":"f453f69bb18f409ea0a66039a53d1f08","num":68},{"book_id":"7c13c27945ae43d18926db0e30fde02d","num":68},{"book_id":"cf077d41d82d454d9217536e8a7228bb","num":67},{"book_id":"5073b7c7cf7042359372827053385708","num":66},{"book_id":"93110b8616ce4c2baa46bf0eef70725c","num":64},{"book_id":"9beed71289af4105a2be7c4ab7dcfa2d","num":64},{"book_id":"fd79d80afe4b458fa472646c9a53e9b8","num":62},{"book_id":"7154b8fa2f974710a2fe6f33d6095ca9","num":61},{"book_id":"0b8b8e00b9a647c1832c7537adc82714","num":60},{"book_id":"079340e8ac4a4ca9afa5de5558f396a4","num":60},{"book_id":"2c29e880626845c28449db5fe5f38507","num":59},{"book_id":"58e74468d0e24e9baa894c39df405d3e","num":58},{"book_id":"9b7a537923c44373aa294499b54dec90","num":58},{"book_id":"67bffdba9e8741e7903d6fb990882864","num":58},{"book_id":"49a12fd9f42b470e98cfc4af08a06eb0","num":56},{"book_id":"f01474cf152b4f868c45cbb9d43e6c77","num":54},{"book_id":"37260ee6327044f49e231f9d6338c620","num":53},{"book_id":"1ad83507275c4294a739784887371b6f","num":51},{"book_id":"138f8de35b974cfda45903bc92e45d3d","num":48},{"book_id":"286e14032a994ae5bee8c1aed67abb1c","num":46},{"book_id":"2f470abec766424090aa2810017b059a","num":45},{"book_id":"b06e3edfbeaf4e0398bf9f3c660a538b","num":42},{"book_id":"14d1f24652614dd996ca445553bb05f4","num":42},{"book_id":"62aed01b29a04c20a38af051dab36c4b","num":42},{"book_id":"c33221b2932a4dfc8f8e6816096a56b0","num":41},{"book_id":"0c150aeabdf446aeba4ad23e2f427654","num":40},{"book_id":"60799f81b7584c5fb835056f8e9b5e69","num":40},{"book_id":"597c591fb939412ab7addf4db4e85122","num":39},{"book_id":"83d299240bd24d58a0985abe76cd6918","num":39},{"book_id":"e228b207d651403992bac2a665e1d170","num":38},{"book_id":"1f60db198f4444e697f6282de746b095","num":35},{"book_id":"18228fdcc8a443c390f150412f38dbff","num":34},{"book_id":"7a43632dfa3543c9b14eb3d8fb424570","num":34},{"book_id":"7793e2dd786b47b7a97eb7f04d469c58","num":34},{"book_id":"097dbb8ebdd344379a0e0adfb84b27b9","num":34},{"book_id":"27de13406734427e91483495b5d53925","num":33},{"book_id":"e978a13953214c19876954a232b7dec1","num":33},{"book_id":"c1f3773ee21e490aaf6040c860e4869d","num":32},{"book_id":"08ad04c0955a44d3b342a06f583854ed","num":32},{"book_id":"73dd1a537ca74c58ab86d4f682e88f54","num":31},{"book_id":"76dbd31a5efd4f8190509c40d7cf8702","num":31},{"book_id":"93e7bc71bab04e2196f97a10b7e19bdb","num":31},{"book_id":"e262a7a3f62645ebba377a8b33cfc6e2","num":31},{"book_id":"63d9d1ff36ea414787ac7bd1ec2effc4","num":31},{"book_id":"d496ec4fb2f9448ca192e625e83e6d99","num":30},{"book_id":"9c183a1696874f68bc0eebefa03c459b","num":30},{"book_id":"1070a2c5adb240c692ec00e2dd83b456","num":29},{"book_id":"e12f0f73bf54425fbdafce85e3f5d80a","num":29},{"book_id":"62bdbfe562294798b55df25622793e6f","num":29},{"book_id":"7cc767458b4c49a3a6344887624e9634","num":29},{"book_id":"e7713fced13f46cc9342a7b57ef04985","num":28},{"book_id":"76f128dfc180424389cdd99c7fc39834","num":28},{"book_id":"52542974e7494b6cab1a3382d7e9a339","num":28},{"book_id":"34df0011ba05436f8f02fe389fa20f00","num":27},{"book_id":"7c07016cec834dc2a7d58246b7475fa5","num":26},{"book_id":"6c7ad672f2bc46568eee2ae481b2a3a5","num":25},{"book_id":"ce49214bfc4d4e20896cccf06424bf74","num":25},{"book_id":"114a1c3c005440d2917f8d69b5ca316d","num":24},{"book_id":"0567d6f508df418aa156df90cf05811a","num":24},{"book_id":"8d2dd26d507d41f58c990832eea70e4b","num":24},{"book_id":"9bc6c8044325464da199d81b21c1d970","num":23},{"book_id":"dc581668c8104fee9af1bd8ffca72407","num":22},{"book_id":"cae308e334594f1187a76345cd6b8efb","num":22},{"book_id":"8862f49192e8426cb83ac0c4eabac4ef","num":21},{"book_id":"8505a959bc2245ce8f6f012e31e57f5b","num":21},{"book_id":"c6278e24f1b44f4691cb67c729960bb5","num":20},{"book_id":"d61be36d95794fe0a1881c5ad4393957","num":20},{"book_id":"59e2c90e4ce04891aa108cb52da9b6fa","num":20},{"book_id":"53e3996e10604c748952b7898a6a729e","num":19},{"book_id":"73bd55f442114694b3558eee57aeadcf","num":19},{"book_id":"81bd8e77de544de6a4c5b636a0bbafd0","num":19},{"book_id":"8318664c753d414d83eabedbc8a122e2","num":18},{"book_id":"221947d326234115bbdfa5fd04a9a09d","num":18},{"book_id":"729540a00cc84fbb874c735f5d853b65","num":18},{"book_id":"fda0b692457945bb968585877035cec6","num":18},{"book_id":"fe617146f40f4ef4a936d7917497f833","num":18},{"book_id":"166a6067ebea4bfa87ff8fdcf2adf063","num":18},{"book_id":"9c90fb39c87448049a867e8d1848ab34","num":18},{"book_id":"c8333d0341304bf5b17dcce06c9f8225","num":17},{"book_id":"ebd9df9b55044032b1e1d3859197a2cd","num":16},{"book_id":"16d5d1c809314d0fb611a1e56f677012","num":16},{"book_id":"8a20ae9b41078847014129922bc80d7c","num":16},{"book_id":"e4748716566d4426b1d87a3581f1880a","num":15},{"book_id":"61b5aba8df20496e8243bdc2d2701719","num":15},{"book_id":"f41e01f289af47d4ad071ed9e2ca5d34","num":15},{"book_id":"d1fe9868a6624fca9e05baac29602ac1","num":15},{"book_id":"c5b028056e364dc8b8c8018d37c29a22","num":15},{"book_id":"5325433d83214977a5274f6fee054666","num":14},{"book_id":"843c3ce0f2e447278b7e05baa85ecae0","num":13},{"book_id":"279ce6c19da64b3db419014722dd3978","num":13},{"book_id":"92244198d9f2476cb57ef289511e1589","num":13},{"book_id":"11d80bb736db47c9a3dda8fd438ce1a4","num":12},{"book_id":"9950e2bd7b2944d189c83fb077a186eb","num":12},{"book_id":"39d79c38d1184f8981a631fdf5b5ed1c","num":12},{"book_id":"8d5221f113af4fa99f05d0728e41a8ee","num":11},{"book_id":"db19bd910b75463691a4b3028b649cf8","num":11},{"book_id":"85edef8203d94417aa017028430edf14","num":11},{"book_id":"21b4a6cb5fcd4c6a91afebb8b79b235a","num":11},{"book_id":"bd6591b98fbb4147bfce9d051ca81d52","num":11},{"book_id":"85a9cc7dec6f4bff82d642c004a5dbc4","num":11},{"book_id":"913b2f0f161a48a4b7d56f144535b1f5","num":10},{"book_id":"f3d0353a5ba24cf8825f6d11f2ac02b6","num":10},{"book_id":"7da4486829164c258fe92dcb041510c8","num":9},{"book_id":"ecaabf24f73b43cbb9caa6cf3457c374","num":9},{"book_id":"e71b1876ae034f53a9944a2b47a2f2f6","num":9},{"book_id":"51116b7ad8d146289a8ba43ef69a3221","num":9},{"book_id":"b6b28bc180be4718bee1cf87c0998598","num":9},{"book_id":"bf51ed6eaf194b51965877bf577aff5a","num":9},{"book_id":"32e479a9a09c4af7bd4482a708942af1","num":8},{"book_id":"bff035fb46aa4f808c4e2a6fbb5be619","num":8},{"book_id":"d62332d2157d49e887130337b890954f","num":8},{"book_id":"93c9e0a1653543628fc516ef2fffd0cc","num":8},{"book_id":"2c8756a5bf454a94a6d5220d346ffbbf","num":8},{"book_id":"228c4fd30b934921aa1068eaaec63675","num":8},{"book_id":"2d8ee6d8042141819762c18bf8ec2d4b","num":8},{"book_id":"31f84fdb5c0347768e95a2f9ee373be4","num":8},{"book_id":"4e1a31577b9b4907a43fee5371fbf186","num":8},{"book_id":"eda3662f66074db19751f809730d1804","num":8},{"book_id":"22ac386c62e64e93af229b59c1570a82","num":8},{"book_id":"b7e56610f05944c49c229bcc13645f91","num":7},{"book_id":"310830153fbc4125a8b7bc594c0abb7b","num":7},{"book_id":"d71d86aade844aa7811a8b9a002f9625","num":7},{"book_id":"642bbc668b66416da03d40d98a5c16f7","num":7},{"book_id":"9b1e1469724f487ca0b7491d8ccdc282","num":7},{"book_id":"b5fe959e81544e38a779e832c7d04953","num":7},{"book_id":"d75926ac587d489ea07cf36adf36c967","num":6},{"book_id":"56cc98aa112c4189adb7f5529b1f7ab2","num":6},{"book_id":"5f7dc383e8374f5f9fe4953b2482a4dd","num":6},{"book_id":"fa6bd7f742654f29b4acf9678252323b","num":6},{"book_id":"22e84a6eab784df29d8b4e3bf2d2ce8e","num":6},{"book_id":"ac2d0af9f5fe491eb741ea9d3caebf49","num":6},{"book_id":"e47c44c7574a4c1aac6de1d322c96168","num":6},{"book_id":"460dd3fdd314446ea919b3da943451b8","num":5},{"book_id":"fa73a70b6ff742a5bc6a9760d6b18298","num":5},{"book_id":"a3e32a42d9db418190ae7130d1932263","num":5},{"book_id":"2b2ff9d9e3fd4973a871a4c14a3ee493","num":5},{"book_id":"64b90a8d3cda4cb596ce4e6d1b6c91e0","num":5},{"book_id":"4b9eddfc789345a2abfab2e36c8d7377","num":5},{"book_id":"7cb3679d86064a26b53251dfa9938efe","num":5},{"book_id":"a44096b8bba84849812e8aab45606c44","num":5},{"book_id":"2a167edfccfe4fcca18d80deb868c648","num":5},{"book_id":"505bd60904ca486a951bad9f0e0444d8","num":5},{"book_id":"8f422f365f634af59173f084b15fd12b","num":5},{"book_id":"b31528c73dfd478caa7304f56a0a601b","num":5},{"book_id":"0e515ef90e2148d99dc6225ba109a57c","num":5},{"book_id":"f00448fa976444f0b57abb1c057b2e8e","num":5},{"book_id":"33f6bafcb5d643cfbcbf5d6829613b9d","num":5},{"book_id":"ed27311353fc425c870f87b1c776378e","num":5},{"book_id":"8985724bc788426b8d9e253de12a58e9","num":4},{"book_id":"75a3e0979c5d4c1b9f24f380ef0d21ed","num":4},{"book_id":"aead24a0df1a481ba8a2caa1b3477bfe","num":4},{"book_id":"ab5c7c7c8592480595fc6c44c77197df","num":4},{"book_id":"1bac4575175b4a02bd7f36bf15350cf1","num":4},{"book_id":"4fbc317cb59047ccbb56062b127832a9","num":4},{"book_id":"8a8feccad34f480a9bb64f1172aa2a9f","num":4},{"book_id":"b1422214573941d3882b83fc7937f3f9","num":4},{"book_id":"c339ad7e10144f29804c4ae8c5a0a8c9","num":4},{"book_id":"3263824ab58e4b4a850c4191212282bf","num":4},{"book_id":"95b8139aa34b49c3afe0bf34d459773b","num":4},{"book_id":"9e93e11332144029b238be9c104105eb","num":4},{"book_id":"42001a7869db4cb38350d4e31b1ae8a0","num":4},{"book_id":"5871216a9de3488ca1c77d62134ae250","num":4},{"book_id":"1a16731f75404fec95af6c407d5a648e","num":4},{"book_id":"03344839ced3406e933eefeddadfe7a5","num":4},{"book_id":"9e197c5436cb45f0baad606ada626b43","num":4},{"book_id":"2386a9b109d5404db7fdd5c803ed1d59","num":4},{"book_id":"870f6071a87a41d4829f929b076f51cf","num":3},{"book_id":"21b59a9615c4473aaa0dc8c62151fd59","num":3},{"book_id":"30631a675c5a4e8eb7fdbfa94ccb70b5","num":3},{"book_id":"96a96d4f435d4fc68c9a0ce0fd95108b","num":3},{"book_id":"28c6d1e51fb54107ba22662de01c9c53","num":3},{"book_id":"7d070e2f5ae9402bbad5f41f26ba2824","num":3},{"book_id":"e8e93e676cde4357bb1db6b1aff94eff","num":3},{"book_id":"f6c04897f9f440428265a553ccd6c2c3","num":3},{"book_id":"80ced25f6fe242edb4591875e9549275","num":3},{"book_id":"7d93d86053e04f04a69566264352a24d","num":3},{"book_id":"23a44bf982d34398b6b14f539a679fc1","num":3},{"book_id":"8b0d5163c9784e1488ec30434e8c5ae0","num":3},{"book_id":"b0aca910ea2f46f6997ec9c18f363a13","num":3},{"book_id":"da43da0d5e6c48b9a456372fe0a91e70","num":3},{"book_id":"575603ac138d4acb9a33741ebe0ec140","num":3},{"book_id":"ba8dc4064ae94c46b8030bfc53bef2a0","num":3},{"book_id":"90019b70496247d48448f01255b282fb","num":3},{"book_id":"b67d42750ad440958c46250dc45f32cb","num":3},{"book_id":"1420eb5d336d4be99d033d86bff9e3b4","num":3},{"book_id":"7840b4ca4dbf4873929b82ef153019be","num":3},{"book_id":"e9f5924a89114608a5c98e926ed2546d","num":2},{"book_id":"40b7638b0c9e4d5693b1108f5067e189","num":2},{"book_id":"38084312e0e94dac814a3e218fa460d0","num":2},{"book_id":"cc617f82c261485c90a1591180a18ef7","num":2},{"book_id":"e0b3aca7afa34b4db727ac02dd143014","num":2},{"book_id":"4600f81b14f04595a035423eaf806206","num":2},{"book_id":"56aeb24f224c4821aa6129d7f76a1e37","num":2},{"book_id":"0ae7c61858054bfdbcff0829d95efbc3","num":2},{"book_id":"ceccc2dacfe84786907d0f210f48c333","num":2},{"book_id":"cc5064df050c4d019fd3765ed329489b","num":2},{"book_id":"e096b806058c4f1f82613addaf720f93","num":2},{"book_id":"12faed3dee784a7695d33128fd0d618f","num":2},{"book_id":"222f31c8df224e3c846ec9d276a9c69b","num":2},{"book_id":"32ecb2bc54644f53911c14ceecfd7596","num":2},{"book_id":"afd996f9788b4b5d8633c6cd15429666","num":2},{"book_id":"c332abe2a20e40fba8d676cc34711116","num":2},{"book_id":"ea52e3790a8f4a6a99dce60e5b66d0fb","num":2},{"book_id":"2f33a0f1945b442dad6ccb02822dcec7","num":2},{"book_id":"0c0ed804855e49008563b459163de2d7","num":2},{"book_id":"f64ecacbdfaa4e39b89667aa47e9aac7","num":2},{"book_id":"e10b3d8cfb284ba0a23f0b0e9dc7aff6","num":2},{"book_id":"5fb3002312c34a68ba795c1a6fe20459","num":2},{"book_id":"8d18ec7b2fbb4cc6b993c604d3b2abdf","num":2},{"book_id":"5bd00399da624d73a430e40b0d0dcf61","num":1},{"book_id":"222","num":1},{"book_id":"9a8a842d77cc4a498992fbf28b0d9490","num":1},{"book_id":"31871473b9114852b7fa374c3d21827d","num":1},{"book_id":"584c562ba3264ef3b07dd952abdd1a31","num":1},{"book_id":"72e8cf655cdb4a1b9de3d69eac27fa7e","num":1},{"book_id":"e92545f63deb4a9d92e5d3122d0cf24c","num":1},{"book_id":"0f18dbb526e6477e8d6cf3ea061cca3e","num":1},{"book_id":"3aa9cb39fc474cd68a7196ad9d06a4ec","num":1},{"book_id":"6711f32af9eb4a1c97b2a5676c2bf232","num":1},{"book_id":"91fe296a9bfc4ae8a8dcf380c4489c6c","num":1},{"book_id":"a078cf789a0644f0b0b8ee625b93d717","num":1},{"book_id":"1a15557288504493b24c5cd2e1cb1e7f","num":1},{"book_id":"8f5e569fe799488bbf6b0b16f257b2f6","num":1},{"book_id":"f070200ea42845b5a380601215f30111","num":1},{"book_id":"359b313431e24e56a3a8f28a1ae1a61f","num":1},{"book_id":"dfc608db53ef4839b96762d698543249","num":1},{"book_id":"eeb1a05eeb434d9489e25daa2ddd5ce1","num":1},{"book_id":"338a2822f05343199516fdb3349ad6f6","num":1},{"book_id":"9bab6e40bb6f4b268fd11e727462d5f6","num":1},{"book_id":"eb92c173d0ba4332a9775ff50a5d623e","num":1},{"book_id":"996ba02c1fdf4e90aad491e8b12ca74d","num":1},{"book_id":"ae4a860e9dac45ac92289faf77d5741b","num":1},{"book_id":"1f9bfe24b47f409ca51f1612ad394359","num":1},{"book_id":"3d3fd0bfc9124d268d66c81c9cba154b","num":1},{"book_id":"bfcf488265f3445a9c55288b917adeca","num":1},{"book_id":"1b054162e9f744fd98a3e70ef0ff280d","num":1},{"book_id":"39416576cb0b419c89a93ef52677093a","num":1},{"book_id":"37fabda094ae4aa2b20515b39e0c2bf8","num":1},{"book_id":"b8f8d614cf8344bfb2be9de0aa6f17c7","num":1},{"book_id":"352de4cacb70499587ab0b01254703ee","num":1},{"book_id":"7c65614757e046328590c3b06af24419","num":1},{"book_id":"c828363caf8e4bf9bdb14768ecd94519","num":1},{"book_id":"edd5754937034948ba04613f35ba7689","num":1},{"book_id":"dcbced10965447ecbfe75fe60a2f9214","num":1},{"book_id":"8827c133125c447bb55ade66b60f69e9","num":1},{"book_id":"a86cb40e1e1d442ca19dfb3b3fe11daf","num":1},{"book_id":"e796877b93664e1ead623e57c039921a","num":1},{"book_id":"f9f0ca9ab3734188ba88c3cf752839b4","num":1},{"book_id":"66563ba79d464812840259841aa0a0fd","num":1},{"book_id":"ccd54c62b1344f288a1c2c08bc8296ad","num":1},{"book_id":"f20e456024ec4b139fe51d1509a3b98c","num":1},{"book_id":"2806b7efcd4b4549aeb09ae487e69073","num":1},{"book_id":"37095d0c6d224c23be26d309a7f451a3","num":1},{"book_id":"8dae4d7006ab4fe1b346429bf14d02a9","num":1},{"book_id":"e071f1f8fcdf4a0eaaa4cf6bf56f7ca9","num":1},{"book_id":"13945e6cfa9b490d944150c1dda054f2","num":1}]
  [INFO] [2017-12-01 11:23:41][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 11:23:40 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 11:36:23][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-01 11:36:23][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 11:36:23 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 11:36:24][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-01 11:36:24][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-01 11:36:26][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 11:36:23 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 11:44:02][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-01 11:44:03][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 11:44:03 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 11:44:03][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-01 11:44:03][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-01 11:44:04][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 11:44:03 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 11:47:37][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-01 11:47:38][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 11:47:38 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 11:47:38][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-01 11:47:38][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-01 11:47:39][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 11:47:38 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 11:59:42][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-01 11:59:43][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 11:59:43 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 11:59:43][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-01 11:59:43][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-01 11:59:45][org.zyp.testmybatis.TestMyBatis]{"45ff80ee9e84495a8a01880e125cc0b3":"27535","d717e0fa935e45cd87ca3d5a4f482de1":"24817","c8112e68ef0446cea43cb0fd3fabdd8f":"14250","fdd4517182db476baed076e8ea0c73a2":"14073","c4149360a00047babaed60b564bd9c94":"10074","99210b3f60db48aca913cb1973fec3c9":"8885","376890106c5748529a7cd428cb749db4":"8700","7d2cdcf9a5e54f3d835de6a063b95ad1":"8104","dc70f449c7a24b80970ba279b85c05b0":"7514","db4232cf381447b39317c4411b47d9d7":"7008","03124c0da03f4ce1937a2a4564d52309":"6674","5f03d165c079413a8311a70bf91813f1":"6053","5ba8ea74ef724ae28f9aca2e6a81f216":"6044","c41dcb4d3d054cad82760e081437ba55":"5661","396a65b821774507b5d0d2a92a1a287f":"5643","4a62e15e685f4f4f96826db8da23dbf7":"4589","851bb631ccc54564b0591cd073f779ff":"4545","440eef2f51dc46a68f58249de19228c6":"4475","fe6d1e9774674343b0c76af951ad36e1":"4425","1aa2dc061d864f85b640551a0010f400":"4390","e9dffa16a8b741aab2b612c22fc754a7":"4174","1e2e7c0d94ee46d380607df8ad73db17":"3921","8bd296afde7d4726b6cfa758773de030":"3802","566b9a78ec0449c0822650af7e8be391":"3697","a847790d1e57438a94934b615299d1a5":"3546","ec1cde7557d640aa995ed4fdef63425b":"3102","66ddfafed082458089e64c5524568859":"2784","30ebb3e4cdd244118356e947ec50c490":"2690","253521df08d149c7bcc7c89eb897250f":"2666","123365f1d8804e39bfba8a68e9f0f97f":"2460","47e9e0d1bbc249f69c59c60b564e4d28":"2440","7f822c3dc5814cb991443083a863b78c":"2197","9fa69b855999478daef0ed50c35da00e":"1940","e8007f9dd2754d97afce055c13b2b867":"1696","b62f2f28d02d4f888c4523cb0d87eae0":"1663","2972abd4b3934b9b96bc404652fafbea":"1617","9f7dffde590649f4a77479f6c400aa2d":"1610","da1df87e65f84bcb8634d16cb808994e":"1581","1aa028da91c84f9292ef64caa2d43a12":"1529","c0d2b8d17f1f4de7a786947cd44f31dd":"1467","b0a746867ecf4c52850077cc900746ac":"1449","77233e7c55a546c0af8474e44f5891c7":"1421","3c7714df30104b52b0738f066229d701":"1390","2c2685eafa6f481ba890c136832c07a7":"1377","2e4cb5649a554fa5a5dc2022045e4cf7":"1375","ce1e9ab7e13543e6bb5cf13864f73060":"1347","01c993a297214944ad7c9bc8767beec2":"1327","3e62c2a294ad4350a286fc71fc711472":"1244","13ae5b4fc36e45f4857fd0ddd0d09be0":"1209","90842a84d245445e82c081ec5e837037":"1190","5040f2f4b856441bbdaaed2b2df59530":"1177","351bb43c60314784ad3a7e7d553cd5f8":"1152","9b6ced512a8b47c4a3c7955955c1f9a0":"1135","df3d97d34d8f4d7ab3428c274099b205":"1110","e867b2ab654b4255bb4ddc30d480f5e6":"1077","4b6570cbe45040e78e73949e85cd1dfd":"1072","01eebcb08c2b4763b924d599c100fd69":"1070","4f881074c5b54d01bd5cf9428db1739d":"999","40b4e061c08540a7990bff60f2ae59e1":"879","846488dd9e064aa79c722bce258db2fb":"844","233096320fa246a6bc3fa8889bd7b7cc":"840","b30aa883c5904cb59f65456509354a71":"828","bb406373ac984673a14a2570e531a3ef":"729","38df5bd30f6b4ef48d95d89376cbd9cb":"717","37b364f9c00548d890de11e0b837262b":"688","9cf787d7967146f6a51c878b91e3d626":"663","3a8d6b655cdf4100b3554fbdca335347":"631","d8da835c7e8c47418e47fdab67500c9f":"618","9883ce7cf86d44d8bae44f6ef29928bb":"615","06dfc16fd96940508a7e8833f1ee0055":"574","a4c5a068fad049cca30ad9c1cef568a1":"526","de83bb0264124936bcc20ccc327b6296":"524","810e9c7186154342bd915ac60257ee1a":"520","b944637b334848bb8aa85008610a7f49":"520","00cab91f0a5a48cc86cfa4435c9c87b1":"505","9009b169df3048c8912b9b552c4e13b2":"504","e7d51f0778834837801615eca0102033":"440","15e23671df614b6ab20c71b9d84aa44f":"433","498befb008844ff28aa27eab31a669f1":"433","ed64eb9649454e54a26ad2a387110fff":"429","6302b2c2592541bbaf1d91dcac257418":"423","70539e880139461c846033ae15dba25b":"400","1ead233280064a7886a2488728002905":"390","e2e9399608ef40a39fa48a85aaabc8fe":"376","a40e61a708534e5490f0a9004f2c093c":"366","919e7a706a3646dfb8f6f819094658e5":"357","f348eaa44ef0421a827dd74002596e31":"355","ed79ab65b6d54622b83bc8895996265b":"342","1d35bc345c6142a992b802ab8963dd13":"323","980f3a4e9108438b9e12b798bd970807":"319","11f7670dd49847f4a61172d06c7f950b":"309","be5e1e7003764e848f0a4f60a1abd6b8":"307","1f9c72dbdaa743329eba873a27355fb6":"305","b55871617fb643dfaa2462d8f1aae201":"302","1fce544e9230409e980aac04a9e1c487":"301","50b0e3c100484641be3c095c4033fa85":"275","6ef5cf8b71004b70b18a10d190b170df":"269","c622b55386b54764972a0ddca4ec864d":"263","09216fb35aa64656a1838dfa7a0cb454":"240","a1188f68745843759bef47c634fb1810":"237","f36eaa9ab4fe4e65a0e6c1cb6cff115f":"231","9b623cd2ccf04ccbbbb7dc64f61f401a":"216","0cda21bc11f3486b87ee21e82417098b":"208","a60366445dee46078432bb0f8b1ac0f9":"200","16eceb96568a48e99fe50322a2506f3f":"200","5c44edfbdbad407ab8901f69209b7641":"185","00e6272b2f6947da8486b046e172eec8":"179","2d7c2bb3036c4f1683cd5db6d26dc8b7":"169","5ec6db156f694e798d6eaf08181e8ade":"160","c1bcfe3708e84948adde9b8c40dd149d":"151","13d37aa1b58b4406a5085b70c753f439":"150","9d1544e21a624ba3bc185b6e13c20deb":"145","781c2b25313c4e7b9ef897ce1f6bda5d":"144","c04ab8fac9af4ab097ecb1b28da9fe6f":"140","dee3298fe8b444b68113078a135882e3":"137","7cd7311cb0f041b9ac9dbdddfdc58d91":"136","22136a68dfe145c5ba9502b82a9c7775":"128","9f34633961014dbb9f02b3e6ee43a88e":"126","8481b5f2a3934aef8bd025f0f2c28d6d":"124","f9c0b9d084b548fa872166b6a7222255":"117","496d6503cbe7429f8551bcc9a3457410":"115","57a4cd0b33704102828b51bc4cceecda":"115","bffd4416937e4a06af644e913ec77a4e":"113","b658732a6c43491bb69912d1ed9ee1ef":"112","5eb2a70a2453447998a63a543b23f87f":"111","b86feffb48a44169b02ef885c5a1b94a":"108","cdddadad83124d59949822f8e6764f9e":"104","0e930eb2a2b54159b5f6300c09cb70e6":"102","4b380d3d47564ffc960e72835f893c51":"101","d8d9292061d34946abef24fabb606c8e":"100","16c075eabc59434cbf976e876507834d":"99","0f6a18d73d9b45628fcb71b4aa70a652":"98","d4a41e181431409cb482b0bf5c52d319":"96","3dc204a22edc4e5ea29c573cae065edf":"95","350ab1e6a0f44e27bf06585187bdfd03":"95","bc42e619783840daba951acb09153368":"89","6db9cbca359a4824a76ba1f8706ba58a":"88","bbadb7da35e546e28eb4a4c816f55d3a":"86","24d967bc9ece48b8b73f04f7a2fffaa6":"86","68b20d144d294343bf964739c0d99b7f":"83","dc5ed12f9d984e2fa0f824d141cddd94":"80","826c0aa85f6144e9bc28c0a9be3eace4":"78","5165c30e1ad7491eb6dd3e1536d7e614":"77","014f05d45e5c437d843e9ef0d302d0a9":"76","a2544e33b7c44c2480cc7f4c59e62be9":"75","8ae2c5465a714e279a335434fb29bf83":"75","a586975c0477462c8feb52bb1303bb1e":"74","4508ca48c4fd4e6abc6f0fca92a2de34":"73","c3cabdf0a78c40b9b6844c601130b07c":"72","a9dc8c634f3c4936b923ef60f8891627":"72","d2272b95f3464318965793b9c69057d7":"72","f453f69bb18f409ea0a66039a53d1f08":"68","7c13c27945ae43d18926db0e30fde02d":"68","cf077d41d82d454d9217536e8a7228bb":"67","5073b7c7cf7042359372827053385708":"66","93110b8616ce4c2baa46bf0eef70725c":"64","9beed71289af4105a2be7c4ab7dcfa2d":"64","fd79d80afe4b458fa472646c9a53e9b8":"62","7154b8fa2f974710a2fe6f33d6095ca9":"61","0b8b8e00b9a647c1832c7537adc82714":"60","079340e8ac4a4ca9afa5de5558f396a4":"60","2c29e880626845c28449db5fe5f38507":"59","58e74468d0e24e9baa894c39df405d3e":"58","9b7a537923c44373aa294499b54dec90":"58","67bffdba9e8741e7903d6fb990882864":"58","49a12fd9f42b470e98cfc4af08a06eb0":"56","f01474cf152b4f868c45cbb9d43e6c77":"54","37260ee6327044f49e231f9d6338c620":"53","1ad83507275c4294a739784887371b6f":"51","138f8de35b974cfda45903bc92e45d3d":"48","286e14032a994ae5bee8c1aed67abb1c":"46","2f470abec766424090aa2810017b059a":"45","b06e3edfbeaf4e0398bf9f3c660a538b":"42","14d1f24652614dd996ca445553bb05f4":"42","62aed01b29a04c20a38af051dab36c4b":"42","c33221b2932a4dfc8f8e6816096a56b0":"41","0c150aeabdf446aeba4ad23e2f427654":"40","60799f81b7584c5fb835056f8e9b5e69":"40","597c591fb939412ab7addf4db4e85122":"39","83d299240bd24d58a0985abe76cd6918":"39","e228b207d651403992bac2a665e1d170":"38","1f60db198f4444e697f6282de746b095":"35","18228fdcc8a443c390f150412f38dbff":"34","7a43632dfa3543c9b14eb3d8fb424570":"34","7793e2dd786b47b7a97eb7f04d469c58":"34","097dbb8ebdd344379a0e0adfb84b27b9":"34","27de13406734427e91483495b5d53925":"33","e978a13953214c19876954a232b7dec1":"33","c1f3773ee21e490aaf6040c860e4869d":"32","08ad04c0955a44d3b342a06f583854ed":"32","73dd1a537ca74c58ab86d4f682e88f54":"31","76dbd31a5efd4f8190509c40d7cf8702":"31","93e7bc71bab04e2196f97a10b7e19bdb":"31","e262a7a3f62645ebba377a8b33cfc6e2":"31","63d9d1ff36ea414787ac7bd1ec2effc4":"31","d496ec4fb2f9448ca192e625e83e6d99":"30","9c183a1696874f68bc0eebefa03c459b":"30","1070a2c5adb240c692ec00e2dd83b456":"29","e12f0f73bf54425fbdafce85e3f5d80a":"29","62bdbfe562294798b55df25622793e6f":"29","7cc767458b4c49a3a6344887624e9634":"29","e7713fced13f46cc9342a7b57ef04985":"28","76f128dfc180424389cdd99c7fc39834":"28","52542974e7494b6cab1a3382d7e9a339":"28","34df0011ba05436f8f02fe389fa20f00":"27","7c07016cec834dc2a7d58246b7475fa5":"26","6c7ad672f2bc46568eee2ae481b2a3a5":"25","ce49214bfc4d4e20896cccf06424bf74":"25","114a1c3c005440d2917f8d69b5ca316d":"24","0567d6f508df418aa156df90cf05811a":"24","8d2dd26d507d41f58c990832eea70e4b":"24","9bc6c8044325464da199d81b21c1d970":"23","dc581668c8104fee9af1bd8ffca72407":"22","cae308e334594f1187a76345cd6b8efb":"22","8862f49192e8426cb83ac0c4eabac4ef":"21","8505a959bc2245ce8f6f012e31e57f5b":"21","c6278e24f1b44f4691cb67c729960bb5":"20","d61be36d95794fe0a1881c5ad4393957":"20","59e2c90e4ce04891aa108cb52da9b6fa":"20","53e3996e10604c748952b7898a6a729e":"19","73bd55f442114694b3558eee57aeadcf":"19","81bd8e77de544de6a4c5b636a0bbafd0":"19","8318664c753d414d83eabedbc8a122e2":"18","221947d326234115bbdfa5fd04a9a09d":"18","729540a00cc84fbb874c735f5d853b65":"18","fda0b692457945bb968585877035cec6":"18","fe617146f40f4ef4a936d7917497f833":"18","166a6067ebea4bfa87ff8fdcf2adf063":"18","9c90fb39c87448049a867e8d1848ab34":"18","c8333d0341304bf5b17dcce06c9f8225":"17","ebd9df9b55044032b1e1d3859197a2cd":"16","16d5d1c809314d0fb611a1e56f677012":"16","8a20ae9b41078847014129922bc80d7c":"16","e4748716566d4426b1d87a3581f1880a":"15","61b5aba8df20496e8243bdc2d2701719":"15","f41e01f289af47d4ad071ed9e2ca5d34":"15","d1fe9868a6624fca9e05baac29602ac1":"15","c5b028056e364dc8b8c8018d37c29a22":"15","5325433d83214977a5274f6fee054666":"14","843c3ce0f2e447278b7e05baa85ecae0":"13","279ce6c19da64b3db419014722dd3978":"13","92244198d9f2476cb57ef289511e1589":"13","11d80bb736db47c9a3dda8fd438ce1a4":"12","9950e2bd7b2944d189c83fb077a186eb":"12","39d79c38d1184f8981a631fdf5b5ed1c":"12","8d5221f113af4fa99f05d0728e41a8ee":"11","db19bd910b75463691a4b3028b649cf8":"11","85edef8203d94417aa017028430edf14":"11","21b4a6cb5fcd4c6a91afebb8b79b235a":"11","bd6591b98fbb4147bfce9d051ca81d52":"11","85a9cc7dec6f4bff82d642c004a5dbc4":"11","913b2f0f161a48a4b7d56f144535b1f5":"10","f3d0353a5ba24cf8825f6d11f2ac02b6":"10","7da4486829164c258fe92dcb041510c8":"9","ecaabf24f73b43cbb9caa6cf3457c374":"9","e71b1876ae034f53a9944a2b47a2f2f6":"9","51116b7ad8d146289a8ba43ef69a3221":"9","b6b28bc180be4718bee1cf87c0998598":"9","bf51ed6eaf194b51965877bf577aff5a":"9","32e479a9a09c4af7bd4482a708942af1":"8","bff035fb46aa4f808c4e2a6fbb5be619":"8","d62332d2157d49e887130337b890954f":"8","93c9e0a1653543628fc516ef2fffd0cc":"8","2c8756a5bf454a94a6d5220d346ffbbf":"8","228c4fd30b934921aa1068eaaec63675":"8","2d8ee6d8042141819762c18bf8ec2d4b":"8","31f84fdb5c0347768e95a2f9ee373be4":"8","4e1a31577b9b4907a43fee5371fbf186":"8","eda3662f66074db19751f809730d1804":"8","22ac386c62e64e93af229b59c1570a82":"8","b7e56610f05944c49c229bcc13645f91":"7","310830153fbc4125a8b7bc594c0abb7b":"7","d71d86aade844aa7811a8b9a002f9625":"7","642bbc668b66416da03d40d98a5c16f7":"7","9b1e1469724f487ca0b7491d8ccdc282":"7","b5fe959e81544e38a779e832c7d04953":"7","d75926ac587d489ea07cf36adf36c967":"6","56cc98aa112c4189adb7f5529b1f7ab2":"6","5f7dc383e8374f5f9fe4953b2482a4dd":"6","fa6bd7f742654f29b4acf9678252323b":"6","22e84a6eab784df29d8b4e3bf2d2ce8e":"6","ac2d0af9f5fe491eb741ea9d3caebf49":"6","e47c44c7574a4c1aac6de1d322c96168":"6","460dd3fdd314446ea919b3da943451b8":"5","fa73a70b6ff742a5bc6a9760d6b18298":"5","a3e32a42d9db418190ae7130d1932263":"5","2b2ff9d9e3fd4973a871a4c14a3ee493":"5","64b90a8d3cda4cb596ce4e6d1b6c91e0":"5","4b9eddfc789345a2abfab2e36c8d7377":"5","7cb3679d86064a26b53251dfa9938efe":"5","a44096b8bba84849812e8aab45606c44":"5","2a167edfccfe4fcca18d80deb868c648":"5","505bd60904ca486a951bad9f0e0444d8":"5","8f422f365f634af59173f084b15fd12b":"5","b31528c73dfd478caa7304f56a0a601b":"5","0e515ef90e2148d99dc6225ba109a57c":"5","f00448fa976444f0b57abb1c057b2e8e":"5","33f6bafcb5d643cfbcbf5d6829613b9d":"5","ed27311353fc425c870f87b1c776378e":"5","8985724bc788426b8d9e253de12a58e9":"4","75a3e0979c5d4c1b9f24f380ef0d21ed":"4","aead24a0df1a481ba8a2caa1b3477bfe":"4","ab5c7c7c8592480595fc6c44c77197df":"4","1bac4575175b4a02bd7f36bf15350cf1":"4","4fbc317cb59047ccbb56062b127832a9":"4","8a8feccad34f480a9bb64f1172aa2a9f":"4","b1422214573941d3882b83fc7937f3f9":"4","c339ad7e10144f29804c4ae8c5a0a8c9":"4","3263824ab58e4b4a850c4191212282bf":"4","95b8139aa34b49c3afe0bf34d459773b":"4","9e93e11332144029b238be9c104105eb":"4","42001a7869db4cb38350d4e31b1ae8a0":"4","5871216a9de3488ca1c77d62134ae250":"4","1a16731f75404fec95af6c407d5a648e":"4","03344839ced3406e933eefeddadfe7a5":"4","9e197c5436cb45f0baad606ada626b43":"4","2386a9b109d5404db7fdd5c803ed1d59":"4","870f6071a87a41d4829f929b076f51cf":"3","21b59a9615c4473aaa0dc8c62151fd59":"3","30631a675c5a4e8eb7fdbfa94ccb70b5":"3","96a96d4f435d4fc68c9a0ce0fd95108b":"3","28c6d1e51fb54107ba22662de01c9c53":"3","7d070e2f5ae9402bbad5f41f26ba2824":"3","e8e93e676cde4357bb1db6b1aff94eff":"3","f6c04897f9f440428265a553ccd6c2c3":"3","80ced25f6fe242edb4591875e9549275":"3","7d93d86053e04f04a69566264352a24d":"3","23a44bf982d34398b6b14f539a679fc1":"3","8b0d5163c9784e1488ec30434e8c5ae0":"3","b0aca910ea2f46f6997ec9c18f363a13":"3","da43da0d5e6c48b9a456372fe0a91e70":"3","575603ac138d4acb9a33741ebe0ec140":"3","ba8dc4064ae94c46b8030bfc53bef2a0":"3","90019b70496247d48448f01255b282fb":"3","b67d42750ad440958c46250dc45f32cb":"3","1420eb5d336d4be99d033d86bff9e3b4":"3","7840b4ca4dbf4873929b82ef153019be":"3","e9f5924a89114608a5c98e926ed2546d":"2","40b7638b0c9e4d5693b1108f5067e189":"2","38084312e0e94dac814a3e218fa460d0":"2","cc617f82c261485c90a1591180a18ef7":"2","e0b3aca7afa34b4db727ac02dd143014":"2","4600f81b14f04595a035423eaf806206":"2","56aeb24f224c4821aa6129d7f76a1e37":"2","0ae7c61858054bfdbcff0829d95efbc3":"2","ceccc2dacfe84786907d0f210f48c333":"2","cc5064df050c4d019fd3765ed329489b":"2","e096b806058c4f1f82613addaf720f93":"2","12faed3dee784a7695d33128fd0d618f":"2","222f31c8df224e3c846ec9d276a9c69b":"2","32ecb2bc54644f53911c14ceecfd7596":"2","afd996f9788b4b5d8633c6cd15429666":"2","c332abe2a20e40fba8d676cc34711116":"2","ea52e3790a8f4a6a99dce60e5b66d0fb":"2","2f33a0f1945b442dad6ccb02822dcec7":"2","0c0ed804855e49008563b459163de2d7":"2","f64ecacbdfaa4e39b89667aa47e9aac7":"2","e10b3d8cfb284ba0a23f0b0e9dc7aff6":"2","5fb3002312c34a68ba795c1a6fe20459":"2","8d18ec7b2fbb4cc6b993c604d3b2abdf":"2","5bd00399da624d73a430e40b0d0dcf61":"1","222":"1","9a8a842d77cc4a498992fbf28b0d9490":"1","31871473b9114852b7fa374c3d21827d":"1","584c562ba3264ef3b07dd952abdd1a31":"1","72e8cf655cdb4a1b9de3d69eac27fa7e":"1","e92545f63deb4a9d92e5d3122d0cf24c":"1","0f18dbb526e6477e8d6cf3ea061cca3e":"1","3aa9cb39fc474cd68a7196ad9d06a4ec":"1","6711f32af9eb4a1c97b2a5676c2bf232":"1","91fe296a9bfc4ae8a8dcf380c4489c6c":"1","a078cf789a0644f0b0b8ee625b93d717":"1","1a15557288504493b24c5cd2e1cb1e7f":"1","8f5e569fe799488bbf6b0b16f257b2f6":"1","f070200ea42845b5a380601215f30111":"1","359b313431e24e56a3a8f28a1ae1a61f":"1","dfc608db53ef4839b96762d698543249":"1","eeb1a05eeb434d9489e25daa2ddd5ce1":"1","338a2822f05343199516fdb3349ad6f6":"1","9bab6e40bb6f4b268fd11e727462d5f6":"1","eb92c173d0ba4332a9775ff50a5d623e":"1","996ba02c1fdf4e90aad491e8b12ca74d":"1","ae4a860e9dac45ac92289faf77d5741b":"1","1f9bfe24b47f409ca51f1612ad394359":"1","3d3fd0bfc9124d268d66c81c9cba154b":"1","bfcf488265f3445a9c55288b917adeca":"1","1b054162e9f744fd98a3e70ef0ff280d":"1","39416576cb0b419c89a93ef52677093a":"1","37fabda094ae4aa2b20515b39e0c2bf8":"1","b8f8d614cf8344bfb2be9de0aa6f17c7":"1","352de4cacb70499587ab0b01254703ee":"1","7c65614757e046328590c3b06af24419":"1","c828363caf8e4bf9bdb14768ecd94519":"1","edd5754937034948ba04613f35ba7689":"1","dcbced10965447ecbfe75fe60a2f9214":"1","8827c133125c447bb55ade66b60f69e9":"1","a86cb40e1e1d442ca19dfb3b3fe11daf":"1","e796877b93664e1ead623e57c039921a":"1","f9f0ca9ab3734188ba88c3cf752839b4":"1","66563ba79d464812840259841aa0a0fd":"1","ccd54c62b1344f288a1c2c08bc8296ad":"1","f20e456024ec4b139fe51d1509a3b98c":"1","2806b7efcd4b4549aeb09ae487e69073":"1","37095d0c6d224c23be26d309a7f451a3":"1","8dae4d7006ab4fe1b346429bf14d02a9":"1","e071f1f8fcdf4a0eaaa4cf6bf56f7ca9":"1","13945e6cfa9b490d944150c1dda054f2":"1"}
  [INFO] [2017-12-01 11:59:45][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 11:59:43 CST 2017]; root of context hierarchy
  [WARN] [2017-12-01 14:18:45][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-01 14:19:03][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-01 14:19:17][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-01 14:19:29][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-01 14:19:41][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [INFO] [2017-12-01 15:15:10][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-01 15:15:10][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 15:15:10 CST 2017]; root of context hierarchy
  [INFO] [2017-12-01 15:15:10][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-01 15:15:10][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-01 15:15:12][org.zyp.testmybatis.TestMyBatis]{"45ff80ee9e84495a8a01880e125cc0b3":"27000","d717e0fa935e45cd87ca3d5a4f482de1":"24412","c8112e68ef0446cea43cb0fd3fabdd8f":"14029","fdd4517182db476baed076e8ea0c73a2":"13669","c4149360a00047babaed60b564bd9c94":"9734","99210b3f60db48aca913cb1973fec3c9":"8837","376890106c5748529a7cd428cb749db4":"8600","7d2cdcf9a5e54f3d835de6a063b95ad1":"8016","dc70f449c7a24b80970ba279b85c05b0":"7454","db4232cf381447b39317c4411b47d9d7":"6891","03124c0da03f4ce1937a2a4564d52309":"6618","5f03d165c079413a8311a70bf91813f1":"5970","5ba8ea74ef724ae28f9aca2e6a81f216":"5918","c41dcb4d3d054cad82760e081437ba55":"5619","396a65b821774507b5d0d2a92a1a287f":"5593","4a62e15e685f4f4f96826db8da23dbf7":"4553","851bb631ccc54564b0591cd073f779ff":"4523","440eef2f51dc46a68f58249de19228c6":"4455","fe6d1e9774674343b0c76af951ad36e1":"4377","1aa2dc061d864f85b640551a0010f400":"4341","e9dffa16a8b741aab2b612c22fc754a7":"4089","1e2e7c0d94ee46d380607df8ad73db17":"3873","8bd296afde7d4726b6cfa758773de030":"3746","566b9a78ec0449c0822650af7e8be391":"3671","a847790d1e57438a94934b615299d1a5":"3504","ec1cde7557d640aa995ed4fdef63425b":"3069","66ddfafed082458089e64c5524568859":"2698","30ebb3e4cdd244118356e947ec50c490":"2681","253521df08d149c7bcc7c89eb897250f":"2638","123365f1d8804e39bfba8a68e9f0f97f":"2440","47e9e0d1bbc249f69c59c60b564e4d28":"2419","7f822c3dc5814cb991443083a863b78c":"2183","9fa69b855999478daef0ed50c35da00e":"1927","e8007f9dd2754d97afce055c13b2b867":"1687","b62f2f28d02d4f888c4523cb0d87eae0":"1637","9f7dffde590649f4a77479f6c400aa2d":"1601","2972abd4b3934b9b96bc404652fafbea":"1598","da1df87e65f84bcb8634d16cb808994e":"1578","1aa028da91c84f9292ef64caa2d43a12":"1527","c0d2b8d17f1f4de7a786947cd44f31dd":"1459","b0a746867ecf4c52850077cc900746ac":"1424","77233e7c55a546c0af8474e44f5891c7":"1416","2e4cb5649a554fa5a5dc2022045e4cf7":"1372","3c7714df30104b52b0738f066229d701":"1370","2c2685eafa6f481ba890c136832c07a7":"1369","ce1e9ab7e13543e6bb5cf13864f73060":"1332","01c993a297214944ad7c9bc8767beec2":"1323","3e62c2a294ad4350a286fc71fc711472":"1239","13ae5b4fc36e45f4857fd0ddd0d09be0":"1191","90842a84d245445e82c081ec5e837037":"1180","5040f2f4b856441bbdaaed2b2df59530":"1175","351bb43c60314784ad3a7e7d553cd5f8":"1151","9b6ced512a8b47c4a3c7955955c1f9a0":"1115","df3d97d34d8f4d7ab3428c274099b205":"1108","e867b2ab654b4255bb4ddc30d480f5e6":"1069","4b6570cbe45040e78e73949e85cd1dfd":"1060","01eebcb08c2b4763b924d599c100fd69":"1020","4f881074c5b54d01bd5cf9428db1739d":"989","40b4e061c08540a7990bff60f2ae59e1":"877","846488dd9e064aa79c722bce258db2fb":"840","233096320fa246a6bc3fa8889bd7b7cc":"840","b30aa883c5904cb59f65456509354a71":"806","bb406373ac984673a14a2570e531a3ef":"722","38df5bd30f6b4ef48d95d89376cbd9cb":"716","37b364f9c00548d890de11e0b837262b":"684","9cf787d7967146f6a51c878b91e3d626":"661","3a8d6b655cdf4100b3554fbdca335347":"630","d8da835c7e8c47418e47fdab67500c9f":"615","9883ce7cf86d44d8bae44f6ef29928bb":"610","06dfc16fd96940508a7e8833f1ee0055":"570","a4c5a068fad049cca30ad9c1cef568a1":"521","b944637b334848bb8aa85008610a7f49":"515","de83bb0264124936bcc20ccc327b6296":"515","810e9c7186154342bd915ac60257ee1a":"513","00cab91f0a5a48cc86cfa4435c9c87b1":"505","9009b169df3048c8912b9b552c4e13b2":"501","e7d51f0778834837801615eca0102033":"439","498befb008844ff28aa27eab31a669f1":"432","15e23671df614b6ab20c71b9d84aa44f":"431","ed64eb9649454e54a26ad2a387110fff":"424","6302b2c2592541bbaf1d91dcac257418":"423","70539e880139461c846033ae15dba25b":"393","1ead233280064a7886a2488728002905":"386","e2e9399608ef40a39fa48a85aaabc8fe":"371","919e7a706a3646dfb8f6f819094658e5":"356","a40e61a708534e5490f0a9004f2c093c":"354","f348eaa44ef0421a827dd74002596e31":"354","ed79ab65b6d54622b83bc8895996265b":"340","1d35bc345c6142a992b802ab8963dd13":"323","980f3a4e9108438b9e12b798bd970807":"319","be5e1e7003764e848f0a4f60a1abd6b8":"307","1f9c72dbdaa743329eba873a27355fb6":"304","1fce544e9230409e980aac04a9e1c487":"301","b55871617fb643dfaa2462d8f1aae201":"300","11f7670dd49847f4a61172d06c7f950b":"300","50b0e3c100484641be3c095c4033fa85":"270","6ef5cf8b71004b70b18a10d190b170df":"266","c622b55386b54764972a0ddca4ec864d":"263","09216fb35aa64656a1838dfa7a0cb454":"240","a1188f68745843759bef47c634fb1810":"235","f36eaa9ab4fe4e65a0e6c1cb6cff115f":"228","9b623cd2ccf04ccbbbb7dc64f61f401a":"211","0cda21bc11f3486b87ee21e82417098b":"207","a60366445dee46078432bb0f8b1ac0f9":"200","16eceb96568a48e99fe50322a2506f3f":"199","5c44edfbdbad407ab8901f69209b7641":"183","00e6272b2f6947da8486b046e172eec8":"177","2d7c2bb3036c4f1683cd5db6d26dc8b7":"168","5ec6db156f694e798d6eaf08181e8ade":"160","c1bcfe3708e84948adde9b8c40dd149d":"150","13d37aa1b58b4406a5085b70c753f439":"150","781c2b25313c4e7b9ef897ce1f6bda5d":"144","9d1544e21a624ba3bc185b6e13c20deb":"142","c04ab8fac9af4ab097ecb1b28da9fe6f":"138","dee3298fe8b444b68113078a135882e3":"137","7cd7311cb0f041b9ac9dbdddfdc58d91":"136","22136a68dfe145c5ba9502b82a9c7775":"128","8481b5f2a3934aef8bd025f0f2c28d6d":"124","9f34633961014dbb9f02b3e6ee43a88e":"122","f9c0b9d084b548fa872166b6a7222255":"117","496d6503cbe7429f8551bcc9a3457410":"115","57a4cd0b33704102828b51bc4cceecda":"115","b658732a6c43491bb69912d1ed9ee1ef":"111","5eb2a70a2453447998a63a543b23f87f":"111","b86feffb48a44169b02ef885c5a1b94a":"108","bffd4416937e4a06af644e913ec77a4e":"106","cdddadad83124d59949822f8e6764f9e":"104","0e930eb2a2b54159b5f6300c09cb70e6":"101","4b380d3d47564ffc960e72835f893c51":"99","16c075eabc59434cbf976e876507834d":"99","0f6a18d73d9b45628fcb71b4aa70a652":"98","d4a41e181431409cb482b0bf5c52d319":"96","3dc204a22edc4e5ea29c573cae065edf":"95","350ab1e6a0f44e27bf06585187bdfd03":"95","d8d9292061d34946abef24fabb606c8e":"94","6db9cbca359a4824a76ba1f8706ba58a":"87","bc42e619783840daba951acb09153368":"87","bbadb7da35e546e28eb4a4c816f55d3a":"86","24d967bc9ece48b8b73f04f7a2fffaa6":"85","68b20d144d294343bf964739c0d99b7f":"83","dc5ed12f9d984e2fa0f824d141cddd94":"80","826c0aa85f6144e9bc28c0a9be3eace4":"78","5165c30e1ad7491eb6dd3e1536d7e614":"77","014f05d45e5c437d843e9ef0d302d0a9":"76","8ae2c5465a714e279a335434fb29bf83":"75","a586975c0477462c8feb52bb1303bb1e":"74","a2544e33b7c44c2480cc7f4c59e62be9":"74","4508ca48c4fd4e6abc6f0fca92a2de34":"73","c3cabdf0a78c40b9b6844c601130b07c":"72","d2272b95f3464318965793b9c69057d7":"72","a9dc8c634f3c4936b923ef60f8891627":"71","f453f69bb18f409ea0a66039a53d1f08":"68","cf077d41d82d454d9217536e8a7228bb":"67","7c13c27945ae43d18926db0e30fde02d":"66","5073b7c7cf7042359372827053385708":"64","93110b8616ce4c2baa46bf0eef70725c":"64","9beed71289af4105a2be7c4ab7dcfa2d":"62","fd79d80afe4b458fa472646c9a53e9b8":"62","7154b8fa2f974710a2fe6f33d6095ca9":"61","079340e8ac4a4ca9afa5de5558f396a4":"60","0b8b8e00b9a647c1832c7537adc82714":"60","2c29e880626845c28449db5fe5f38507":"59","9b7a537923c44373aa294499b54dec90":"58","67bffdba9e8741e7903d6fb990882864":"58","58e74468d0e24e9baa894c39df405d3e":"58","49a12fd9f42b470e98cfc4af08a06eb0":"56","f01474cf152b4f868c45cbb9d43e6c77":"53","37260ee6327044f49e231f9d6338c620":"52","1ad83507275c4294a739784887371b6f":"51","138f8de35b974cfda45903bc92e45d3d":"48","2f470abec766424090aa2810017b059a":"45","286e14032a994ae5bee8c1aed67abb1c":"45","62aed01b29a04c20a38af051dab36c4b":"42","c33221b2932a4dfc8f8e6816096a56b0":"41","14d1f24652614dd996ca445553bb05f4":"41","b06e3edfbeaf4e0398bf9f3c660a538b":"40","0c150aeabdf446aeba4ad23e2f427654":"40","83d299240bd24d58a0985abe76cd6918":"39","60799f81b7584c5fb835056f8e9b5e69":"39","597c591fb939412ab7addf4db4e85122":"38","e228b207d651403992bac2a665e1d170":"38","7a43632dfa3543c9b14eb3d8fb424570":"34","7793e2dd786b47b7a97eb7f04d469c58":"34","1f60db198f4444e697f6282de746b095":"34","097dbb8ebdd344379a0e0adfb84b27b9":"34","e978a13953214c19876954a232b7dec1":"33","27de13406734427e91483495b5d53925":"33","18228fdcc8a443c390f150412f38dbff":"33","08ad04c0955a44d3b342a06f583854ed":"32","76dbd31a5efd4f8190509c40d7cf8702":"31","e262a7a3f62645ebba377a8b33cfc6e2":"31","63d9d1ff36ea414787ac7bd1ec2effc4":"31","73dd1a537ca74c58ab86d4f682e88f54":"31","c1f3773ee21e490aaf6040c860e4869d":"31","93e7bc71bab04e2196f97a10b7e19bdb":"30","d496ec4fb2f9448ca192e625e83e6d99":"30","9c183a1696874f68bc0eebefa03c459b":"30","7cc767458b4c49a3a6344887624e9634":"29","1070a2c5adb240c692ec00e2dd83b456":"29","e12f0f73bf54425fbdafce85e3f5d80a":"29","62bdbfe562294798b55df25622793e6f":"29","52542974e7494b6cab1a3382d7e9a339":"28","e7713fced13f46cc9342a7b57ef04985":"28","76f128dfc180424389cdd99c7fc39834":"28","34df0011ba05436f8f02fe389fa20f00":"27","7c07016cec834dc2a7d58246b7475fa5":"26","ce49214bfc4d4e20896cccf06424bf74":"25","6c7ad672f2bc46568eee2ae481b2a3a5":"25","114a1c3c005440d2917f8d69b5ca316d":"24","0567d6f508df418aa156df90cf05811a":"24","8d2dd26d507d41f58c990832eea70e4b":"24","9bc6c8044325464da199d81b21c1d970":"23","dc581668c8104fee9af1bd8ffca72407":"22","cae308e334594f1187a76345cd6b8efb":"22","59e2c90e4ce04891aa108cb52da9b6fa":"20","c6278e24f1b44f4691cb67c729960bb5":"20","8862f49192e8426cb83ac0c4eabac4ef":"20","8505a959bc2245ce8f6f012e31e57f5b":"20","d61be36d95794fe0a1881c5ad4393957":"20","81bd8e77de544de6a4c5b636a0bbafd0":"19","53e3996e10604c748952b7898a6a729e":"19","73bd55f442114694b3558eee57aeadcf":"18","fe617146f40f4ef4a936d7917497f833":"18","166a6067ebea4bfa87ff8fdcf2adf063":"18","9c90fb39c87448049a867e8d1848ab34":"18","8318664c753d414d83eabedbc8a122e2":"18","221947d326234115bbdfa5fd04a9a09d":"18","729540a00cc84fbb874c735f5d853b65":"18","fda0b692457945bb968585877035cec6":"18","c8333d0341304bf5b17dcce06c9f8225":"17","8a20ae9b41078847014129922bc80d7c":"16","ebd9df9b55044032b1e1d3859197a2cd":"16","16d5d1c809314d0fb611a1e56f677012":"16","d1fe9868a6624fca9e05baac29602ac1":"15","c5b028056e364dc8b8c8018d37c29a22":"15","e4748716566d4426b1d87a3581f1880a":"15","61b5aba8df20496e8243bdc2d2701719":"15","f41e01f289af47d4ad071ed9e2ca5d34":"15","5325433d83214977a5274f6fee054666":"14","92244198d9f2476cb57ef289511e1589":"13","843c3ce0f2e447278b7e05baa85ecae0":"13","279ce6c19da64b3db419014722dd3978":"13","11d80bb736db47c9a3dda8fd438ce1a4":"12","9950e2bd7b2944d189c83fb077a186eb":"12","39d79c38d1184f8981a631fdf5b5ed1c":"12","85a9cc7dec6f4bff82d642c004a5dbc4":"11","8d5221f113af4fa99f05d0728e41a8ee":"11","db19bd910b75463691a4b3028b649cf8":"11","85edef8203d94417aa017028430edf14":"11","21b4a6cb5fcd4c6a91afebb8b79b235a":"11","bd6591b98fbb4147bfce9d051ca81d52":"11","913b2f0f161a48a4b7d56f144535b1f5":"10","f3d0353a5ba24cf8825f6d11f2ac02b6":"10","bf51ed6eaf194b51965877bf577aff5a":"9","7da4486829164c258fe92dcb041510c8":"9","ecaabf24f73b43cbb9caa6cf3457c374":"9","e71b1876ae034f53a9944a2b47a2f2f6":"9","51116b7ad8d146289a8ba43ef69a3221":"9","b6b28bc180be4718bee1cf87c0998598":"9","31f84fdb5c0347768e95a2f9ee373be4":"8","4e1a31577b9b4907a43fee5371fbf186":"8","eda3662f66074db19751f809730d1804":"8","22ac386c62e64e93af229b59c1570a82":"8","bff035fb46aa4f808c4e2a6fbb5be619":"8","d62332d2157d49e887130337b890954f":"8","2c8756a5bf454a94a6d5220d346ffbbf":"8","93c9e0a1653543628fc516ef2fffd0cc":"8","228c4fd30b934921aa1068eaaec63675":"8","2d8ee6d8042141819762c18bf8ec2d4b":"8","9b1e1469724f487ca0b7491d8ccdc282":"7","b5fe959e81544e38a779e832c7d04953":"7","32e479a9a09c4af7bd4482a708942af1":"7","b7e56610f05944c49c229bcc13645f91":"7","310830153fbc4125a8b7bc594c0abb7b":"7","d71d86aade844aa7811a8b9a002f9625":"7","642bbc668b66416da03d40d98a5c16f7":"7","22e84a6eab784df29d8b4e3bf2d2ce8e":"6","ac2d0af9f5fe491eb741ea9d3caebf49":"6","e47c44c7574a4c1aac6de1d322c96168":"6","d75926ac587d489ea07cf36adf36c967":"6","56cc98aa112c4189adb7f5529b1f7ab2":"6","5f7dc383e8374f5f9fe4953b2482a4dd":"6","fa6bd7f742654f29b4acf9678252323b":"6","b31528c73dfd478caa7304f56a0a601b":"5","0e515ef90e2148d99dc6225ba109a57c":"5","f00448fa976444f0b57abb1c057b2e8e":"5","33f6bafcb5d643cfbcbf5d6829613b9d":"5","ed27311353fc425c870f87b1c776378e":"5","460dd3fdd314446ea919b3da943451b8":"5","fa73a70b6ff742a5bc6a9760d6b18298":"5","2b2ff9d9e3fd4973a871a4c14a3ee493":"5","a3e32a42d9db418190ae7130d1932263":"5","64b90a8d3cda4cb596ce4e6d1b6c91e0":"5","4b9eddfc789345a2abfab2e36c8d7377":"5","7cb3679d86064a26b53251dfa9938efe":"5","a44096b8bba84849812e8aab45606c44":"5","2a167edfccfe4fcca18d80deb868c648":"5","505bd60904ca486a951bad9f0e0444d8":"5","8f422f365f634af59173f084b15fd12b":"5","42001a7869db4cb38350d4e31b1ae8a0":"4","5871216a9de3488ca1c77d62134ae250":"4","1a16731f75404fec95af6c407d5a648e":"4","03344839ced3406e933eefeddadfe7a5":"4","9e197c5436cb45f0baad606ada626b43":"4","2386a9b109d5404db7fdd5c803ed1d59":"4","8985724bc788426b8d9e253de12a58e9":"4","75a3e0979c5d4c1b9f24f380ef0d21ed":"4","aead24a0df1a481ba8a2caa1b3477bfe":"4","ab5c7c7c8592480595fc6c44c77197df":"4","1bac4575175b4a02bd7f36bf15350cf1":"4","4fbc317cb59047ccbb56062b127832a9":"4","8a8feccad34f480a9bb64f1172aa2a9f":"4","b1422214573941d3882b83fc7937f3f9":"4","3263824ab58e4b4a850c4191212282bf":"4","c339ad7e10144f29804c4ae8c5a0a8c9":"4","95b8139aa34b49c3afe0bf34d459773b":"4","9e93e11332144029b238be9c104105eb":"4","8b0d5163c9784e1488ec30434e8c5ae0":"3","b0aca910ea2f46f6997ec9c18f363a13":"3","da43da0d5e6c48b9a456372fe0a91e70":"3","575603ac138d4acb9a33741ebe0ec140":"3","ba8dc4064ae94c46b8030bfc53bef2a0":"3","90019b70496247d48448f01255b282fb":"3","b67d42750ad440958c46250dc45f32cb":"3","1420eb5d336d4be99d033d86bff9e3b4":"3","7840b4ca4dbf4873929b82ef153019be":"3","870f6071a87a41d4829f929b076f51cf":"3","21b59a9615c4473aaa0dc8c62151fd59":"3","30631a675c5a4e8eb7fdbfa94ccb70b5":"3","96a96d4f435d4fc68c9a0ce0fd95108b":"3","28c6d1e51fb54107ba22662de01c9c53":"3","7d070e2f5ae9402bbad5f41f26ba2824":"3","e8e93e676cde4357bb1db6b1aff94eff":"3","f6c04897f9f440428265a553ccd6c2c3":"3","80ced25f6fe242edb4591875e9549275":"3","7d93d86053e04f04a69566264352a24d":"3","23a44bf982d34398b6b14f539a679fc1":"3","32ecb2bc54644f53911c14ceecfd7596":"2","12faed3dee784a7695d33128fd0d618f":"2","222f31c8df224e3c846ec9d276a9c69b":"2","afd996f9788b4b5d8633c6cd15429666":"2","c332abe2a20e40fba8d676cc34711116":"2","ea52e3790a8f4a6a99dce60e5b66d0fb":"2","2f33a0f1945b442dad6ccb02822dcec7":"2","0c0ed804855e49008563b459163de2d7":"2","f64ecacbdfaa4e39b89667aa47e9aac7":"2","e10b3d8cfb284ba0a23f0b0e9dc7aff6":"2","5fb3002312c34a68ba795c1a6fe20459":"2","8d18ec7b2fbb4cc6b993c604d3b2abdf":"2","e9f5924a89114608a5c98e926ed2546d":"2","40b7638b0c9e4d5693b1108f5067e189":"2","38084312e0e94dac814a3e218fa460d0":"2","cc617f82c261485c90a1591180a18ef7":"2","e0b3aca7afa34b4db727ac02dd143014":"2","4600f81b14f04595a035423eaf806206":"2","56aeb24f224c4821aa6129d7f76a1e37":"2","0ae7c61858054bfdbcff0829d95efbc3":"2","ceccc2dacfe84786907d0f210f48c333":"2","cc5064df050c4d019fd3765ed329489b":"2","e096b806058c4f1f82613addaf720f93":"2","dcbced10965447ecbfe75fe60a2f9214":"1","8827c133125c447bb55ade66b60f69e9":"1","a86cb40e1e1d442ca19dfb3b3fe11daf":"1","e796877b93664e1ead623e57c039921a":"1","f9f0ca9ab3734188ba88c3cf752839b4":"1","66563ba79d464812840259841aa0a0fd":"1","ccd54c62b1344f288a1c2c08bc8296ad":"1","f20e456024ec4b139fe51d1509a3b98c":"1","2806b7efcd4b4549aeb09ae487e69073":"1","37095d0c6d224c23be26d309a7f451a3":"1","8dae4d7006ab4fe1b346429bf14d02a9":"1","e071f1f8fcdf4a0eaaa4cf6bf56f7ca9":"1","13945e6cfa9b490d944150c1dda054f2":"1","5bd00399da624d73a430e40b0d0dcf61":"1","222":"1","31871473b9114852b7fa374c3d21827d":"1","9a8a842d77cc4a498992fbf28b0d9490":"1","584c562ba3264ef3b07dd952abdd1a31":"1","72e8cf655cdb4a1b9de3d69eac27fa7e":"1","e92545f63deb4a9d92e5d3122d0cf24c":"1","0f18dbb526e6477e8d6cf3ea061cca3e":"1","3aa9cb39fc474cd68a7196ad9d06a4ec":"1","6711f32af9eb4a1c97b2a5676c2bf232":"1","91fe296a9bfc4ae8a8dcf380c4489c6c":"1","a078cf789a0644f0b0b8ee625b93d717":"1","1a15557288504493b24c5cd2e1cb1e7f":"1","8f5e569fe799488bbf6b0b16f257b2f6":"1","f070200ea42845b5a380601215f30111":"1","359b313431e24e56a3a8f28a1ae1a61f":"1","dfc608db53ef4839b96762d698543249":"1","eeb1a05eeb434d9489e25daa2ddd5ce1":"1","338a2822f05343199516fdb3349ad6f6":"1","9bab6e40bb6f4b268fd11e727462d5f6":"1","eb92c173d0ba4332a9775ff50a5d623e":"1","996ba02c1fdf4e90aad491e8b12ca74d":"1","ae4a860e9dac45ac92289faf77d5741b":"1","1f9bfe24b47f409ca51f1612ad394359":"1","3d3fd0bfc9124d268d66c81c9cba154b":"1","bfcf488265f3445a9c55288b917adeca":"1","1b054162e9f744fd98a3e70ef0ff280d":"1","39416576cb0b419c89a93ef52677093a":"1","37fabda094ae4aa2b20515b39e0c2bf8":"1","b8f8d614cf8344bfb2be9de0aa6f17c7":"1","352de4cacb70499587ab0b01254703ee":"1","7c65614757e046328590c3b06af24419":"1","c828363caf8e4bf9bdb14768ecd94519":"1","edd5754937034948ba04613f35ba7689":"1"}
  [INFO] [2017-12-01 15:15:12][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Fri Dec 01 15:15:10 CST 2017]; root of context hierarchy
  [INFO] [2017-12-05 10:29:13][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-05 10:29:14][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Tue Dec 05 10:29:14 CST 2017]; root of context hierarchy
  [INFO] [2017-12-05 10:29:14][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-05 10:29:14][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-05 10:29:14][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Tue Dec 05 10:29:14 CST 2017]; root of context hierarchy
  [INFO] [2017-12-05 10:33:30][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-05 10:33:30][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Tue Dec 05 10:33:30 CST 2017]; root of context hierarchy
  [INFO] [2017-12-05 10:33:30][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-05 10:33:31][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-05 10:33:32][org.zyp.testmybatis.TestMyBatis][{"book_id":"440eef2f51dc46a68f58249de19228c6","num":2504,"rank":1},{"book_id":"03124c0da03f4ce1937a2a4564d52309","num":1439,"rank":2},{"book_id":"dc70f449c7a24b80970ba279b85c05b0","num":1232,"rank":3},{"book_id":"c41dcb4d3d054cad82760e081437ba55","num":1051,"rank":4},{"book_id":"396a65b821774507b5d0d2a92a1a287f","num":970,"rank":5},{"book_id":"45ff80ee9e84495a8a01880e125cc0b3","num":781,"rank":6},{"book_id":"ce1e9ab7e13543e6bb5cf13864f73060","num":388,"rank":7},{"book_id":"e8007f9dd2754d97afce055c13b2b867","num":337,"rank":8},{"book_id":"b62f2f28d02d4f888c4523cb0d87eae0","num":57,"rank":9},{"book_id":"5eb2a70a2453447998a63a543b23f87f","num":51,"rank":10},{"book_id":"00cab91f0a5a48cc86cfa4435c9c87b1","num":44,"rank":11},{"book_id":"851bb631ccc54564b0591cd073f779ff","num":42,"rank":12},{"book_id":"138f8de35b974cfda45903bc92e45d3d","num":40,"rank":13},{"book_id":"980f3a4e9108438b9e12b798bd970807","num":39,"rank":14},{"book_id":"99210b3f60db48aca913cb1973fec3c9","num":35,"rank":15},{"book_id":"1070a2c5adb240c692ec00e2dd83b456","num":29,"rank":16},{"book_id":"6db9cbca359a4824a76ba1f8706ba58a","num":20,"rank":17},{"book_id":"729540a00cc84fbb874c735f5d853b65","num":18,"rank":18},{"book_id":"8a20ae9b41078847014129922bc80d7c","num":16,"rank":19},{"book_id":"e262a7a3f62645ebba377a8b33cfc6e2","num":15,"rank":20},{"book_id":"5165c30e1ad7491eb6dd3e1536d7e614","num":14,"rank":21},{"book_id":"da1df87e65f84bcb8634d16cb808994e","num":13,"rank":22},{"book_id":"498befb008844ff28aa27eab31a669f1","num":10,"rank":23},{"book_id":"a9dc8c634f3c4936b923ef60f8891627","num":10,"rank":23},{"book_id":"1ad83507275c4294a739784887371b6f","num":7,"rank":24},{"book_id":"7d2cdcf9a5e54f3d835de6a063b95ad1","num":6,"rank":25},{"book_id":"38df5bd30f6b4ef48d95d89376cbd9cb","num":6,"rank":25},{"book_id":"a586975c0477462c8feb52bb1303bb1e","num":5,"rank":26},{"book_id":"4a62e15e685f4f4f96826db8da23dbf7","num":4,"rank":27},{"book_id":"097dbb8ebdd344379a0e0adfb84b27b9","num":4,"rank":27},{"book_id":"a4c5a068fad049cca30ad9c1cef568a1","num":4,"rank":27},{"book_id":"8481b5f2a3934aef8bd025f0f2c28d6d","num":3,"rank":28},{"book_id":"351bb43c60314784ad3a7e7d553cd5f8","num":3,"rank":28},{"book_id":"a847790d1e57438a94934b615299d1a5","num":3,"rank":28},{"book_id":"70539e880139461c846033ae15dba25b","num":3,"rank":28},{"book_id":"2e4cb5649a554fa5a5dc2022045e4cf7","num":3,"rank":28},{"book_id":"1aa2dc061d864f85b640551a0010f400","num":3,"rank":28},{"book_id":"1fce544e9230409e980aac04a9e1c487","num":2,"rank":29},{"book_id":"d8d9292061d34946abef24fabb606c8e","num":2,"rank":29},{"book_id":"40b4e061c08540a7990bff60f2ae59e1","num":2,"rank":29},{"book_id":"d75926ac587d489ea07cf36adf36c967","num":2,"rank":29},{"book_id":"c04ab8fac9af4ab097ecb1b28da9fe6f","num":2,"rank":29},{"book_id":"6711f32af9eb4a1c97b2a5676c2bf232","num":1,"rank":30},{"book_id":"3c7714df30104b52b0738f066229d701","num":1,"rank":30},{"book_id":"bbadb7da35e546e28eb4a4c816f55d3a","num":1,"rank":30},{"book_id":"1aa028da91c84f9292ef64caa2d43a12","num":1,"rank":30},{"book_id":"8318664c753d414d83eabedbc8a122e2","num":1,"rank":30},{"book_id":"66ddfafed082458089e64c5524568859","num":1,"rank":30},{"book_id":"253521df08d149c7bcc7c89eb897250f","num":1,"rank":30},{"book_id":"11d80bb736db47c9a3dda8fd438ce1a4","num":1,"rank":30},{"book_id":"8bd296afde7d4726b6cfa758773de030","num":1,"rank":30}]
  [INFO] [2017-12-05 10:33:32][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Tue Dec 05 10:33:30 CST 2017]; root of context hierarchy
  [WARN] [2017-12-06 16:34:48][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-06 16:35:06][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-06 16:36:20][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-06 17:13:50][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [INFO] [2017-12-07 09:46:20][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 09:46:20][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 09:46:20 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 09:46:21][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 09:46:21][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 09:46:23][org.zyp.testmybatis.TestMyBatis][{"book_id":"440eef2f51dc46a68f58249de19228c6","num":2504,"rank":1},{"book_id":"03124c0da03f4ce1937a2a4564d52309","num":1439,"rank":2},{"book_id":"dc70f449c7a24b80970ba279b85c05b0","num":1232,"rank":3},{"book_id":"c41dcb4d3d054cad82760e081437ba55","num":1051,"rank":4},{"book_id":"396a65b821774507b5d0d2a92a1a287f","num":970,"rank":5},{"book_id":"45ff80ee9e84495a8a01880e125cc0b3","num":781,"rank":6},{"book_id":"ce1e9ab7e13543e6bb5cf13864f73060","num":388,"rank":7},{"book_id":"e8007f9dd2754d97afce055c13b2b867","num":337,"rank":8},{"book_id":"b62f2f28d02d4f888c4523cb0d87eae0","num":57,"rank":9},{"book_id":"5eb2a70a2453447998a63a543b23f87f","num":51,"rank":10},{"book_id":"00cab91f0a5a48cc86cfa4435c9c87b1","num":44,"rank":11},{"book_id":"851bb631ccc54564b0591cd073f779ff","num":42,"rank":12},{"book_id":"138f8de35b974cfda45903bc92e45d3d","num":40,"rank":13},{"book_id":"980f3a4e9108438b9e12b798bd970807","num":39,"rank":14},{"book_id":"99210b3f60db48aca913cb1973fec3c9","num":35,"rank":15},{"book_id":"1070a2c5adb240c692ec00e2dd83b456","num":29,"rank":16},{"book_id":"6db9cbca359a4824a76ba1f8706ba58a","num":20,"rank":17},{"book_id":"729540a00cc84fbb874c735f5d853b65","num":18,"rank":18},{"book_id":"8a20ae9b41078847014129922bc80d7c","num":16,"rank":19},{"book_id":"e262a7a3f62645ebba377a8b33cfc6e2","num":15,"rank":20},{"book_id":"5165c30e1ad7491eb6dd3e1536d7e614","num":14,"rank":21},{"book_id":"da1df87e65f84bcb8634d16cb808994e","num":13,"rank":22},{"book_id":"498befb008844ff28aa27eab31a669f1","num":10,"rank":23},{"book_id":"a9dc8c634f3c4936b923ef60f8891627","num":10,"rank":23},{"book_id":"1ad83507275c4294a739784887371b6f","num":7,"rank":24},{"book_id":"7d2cdcf9a5e54f3d835de6a063b95ad1","num":6,"rank":25},{"book_id":"38df5bd30f6b4ef48d95d89376cbd9cb","num":6,"rank":25},{"book_id":"a586975c0477462c8feb52bb1303bb1e","num":5,"rank":26},{"book_id":"4a62e15e685f4f4f96826db8da23dbf7","num":4,"rank":27},{"book_id":"097dbb8ebdd344379a0e0adfb84b27b9","num":4,"rank":27},{"book_id":"a4c5a068fad049cca30ad9c1cef568a1","num":4,"rank":27},{"book_id":"a847790d1e57438a94934b615299d1a5","num":3,"rank":28},{"book_id":"70539e880139461c846033ae15dba25b","num":3,"rank":28},{"book_id":"2e4cb5649a554fa5a5dc2022045e4cf7","num":3,"rank":28},{"book_id":"1aa2dc061d864f85b640551a0010f400","num":3,"rank":28},{"book_id":"8481b5f2a3934aef8bd025f0f2c28d6d","num":3,"rank":28},{"book_id":"351bb43c60314784ad3a7e7d553cd5f8","num":3,"rank":28},{"book_id":"d8d9292061d34946abef24fabb606c8e","num":2,"rank":29},{"book_id":"40b4e061c08540a7990bff60f2ae59e1","num":2,"rank":29},{"book_id":"d75926ac587d489ea07cf36adf36c967","num":2,"rank":29},{"book_id":"c04ab8fac9af4ab097ecb1b28da9fe6f","num":2,"rank":29},{"book_id":"1fce544e9230409e980aac04a9e1c487","num":2,"rank":29},{"book_id":"1aa028da91c84f9292ef64caa2d43a12","num":1,"rank":30},{"book_id":"8318664c753d414d83eabedbc8a122e2","num":1,"rank":30},{"book_id":"66ddfafed082458089e64c5524568859","num":1,"rank":30},{"book_id":"253521df08d149c7bcc7c89eb897250f","num":1,"rank":30},{"book_id":"11d80bb736db47c9a3dda8fd438ce1a4","num":1,"rank":30},{"book_id":"8bd296afde7d4726b6cfa758773de030","num":1,"rank":30},{"book_id":"6711f32af9eb4a1c97b2a5676c2bf232","num":1,"rank":30},{"book_id":"3c7714df30104b52b0738f066229d701","num":1,"rank":30},{"book_id":"bbadb7da35e546e28eb4a4c816f55d3a","num":1,"rank":30}]
  [INFO] [2017-12-07 09:46:23][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 09:46:20 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 09:47:26][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 09:47:27][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 09:47:27 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 09:47:27][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 09:47:27][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 09:47:28][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 09:47:27 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 09:48:38][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 09:48:38][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 09:48:38 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 09:48:39][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 09:48:39][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 09:48:39][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 09:48:38 CST 2017]; root of context hierarchy
  [WARN] [2017-12-07 09:53:37][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [INFO] [2017-12-07 10:50:15][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 10:50:16][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 10:50:16 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 10:50:16][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 10:50:16][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 10:50:17][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 10:50:16 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 10:50:37][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 10:50:37][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 10:50:37 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 10:50:38][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 10:50:38][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 10:50:39][org.zyp.testmybatis.TestMyBatis][{"book_id":"440eef2f51dc46a68f58249de19228c6","num":2504,"rank":1},{"book_id":"03124c0da03f4ce1937a2a4564d52309","num":1439,"rank":2},{"book_id":"dc70f449c7a24b80970ba279b85c05b0","num":1232,"rank":3},{"book_id":"c41dcb4d3d054cad82760e081437ba55","num":1051,"rank":4},{"book_id":"396a65b821774507b5d0d2a92a1a287f","num":970,"rank":5},{"book_id":"45ff80ee9e84495a8a01880e125cc0b3","num":781,"rank":6},{"book_id":"ce1e9ab7e13543e6bb5cf13864f73060","num":388,"rank":7},{"book_id":"e8007f9dd2754d97afce055c13b2b867","num":337,"rank":8},{"book_id":"b62f2f28d02d4f888c4523cb0d87eae0","num":57,"rank":9},{"book_id":"5eb2a70a2453447998a63a543b23f87f","num":51,"rank":10},{"book_id":"00cab91f0a5a48cc86cfa4435c9c87b1","num":44,"rank":11},{"book_id":"851bb631ccc54564b0591cd073f779ff","num":42,"rank":12},{"book_id":"138f8de35b974cfda45903bc92e45d3d","num":40,"rank":13},{"book_id":"980f3a4e9108438b9e12b798bd970807","num":39,"rank":14},{"book_id":"99210b3f60db48aca913cb1973fec3c9","num":35,"rank":15},{"book_id":"1070a2c5adb240c692ec00e2dd83b456","num":29,"rank":16},{"book_id":"6db9cbca359a4824a76ba1f8706ba58a","num":20,"rank":17},{"book_id":"729540a00cc84fbb874c735f5d853b65","num":18,"rank":18},{"book_id":"8a20ae9b41078847014129922bc80d7c","num":16,"rank":19},{"book_id":"e262a7a3f62645ebba377a8b33cfc6e2","num":15,"rank":20},{"book_id":"5165c30e1ad7491eb6dd3e1536d7e614","num":14,"rank":21},{"book_id":"da1df87e65f84bcb8634d16cb808994e","num":13,"rank":22},{"book_id":"498befb008844ff28aa27eab31a669f1","num":10,"rank":23},{"book_id":"a9dc8c634f3c4936b923ef60f8891627","num":10,"rank":23},{"book_id":"1ad83507275c4294a739784887371b6f","num":7,"rank":24},{"book_id":"7d2cdcf9a5e54f3d835de6a063b95ad1","num":6,"rank":25},{"book_id":"38df5bd30f6b4ef48d95d89376cbd9cb","num":6,"rank":25},{"book_id":"a586975c0477462c8feb52bb1303bb1e","num":5,"rank":26},{"book_id":"097dbb8ebdd344379a0e0adfb84b27b9","num":4,"rank":27},{"book_id":"a4c5a068fad049cca30ad9c1cef568a1","num":4,"rank":27},{"book_id":"4a62e15e685f4f4f96826db8da23dbf7","num":4,"rank":27},{"book_id":"2e4cb5649a554fa5a5dc2022045e4cf7","num":3,"rank":28},{"book_id":"1aa2dc061d864f85b640551a0010f400","num":3,"rank":28},{"book_id":"8481b5f2a3934aef8bd025f0f2c28d6d","num":3,"rank":28},{"book_id":"351bb43c60314784ad3a7e7d553cd5f8","num":3,"rank":28},{"book_id":"a847790d1e57438a94934b615299d1a5","num":3,"rank":28},{"book_id":"70539e880139461c846033ae15dba25b","num":3,"rank":28},{"book_id":"d8d9292061d34946abef24fabb606c8e","num":2,"rank":29},{"book_id":"40b4e061c08540a7990bff60f2ae59e1","num":2,"rank":29},{"book_id":"d75926ac587d489ea07cf36adf36c967","num":2,"rank":29},{"book_id":"c04ab8fac9af4ab097ecb1b28da9fe6f","num":2,"rank":29},{"book_id":"1fce544e9230409e980aac04a9e1c487","num":2,"rank":29},{"book_id":"253521df08d149c7bcc7c89eb897250f","num":1,"rank":30},{"book_id":"11d80bb736db47c9a3dda8fd438ce1a4","num":1,"rank":30},{"book_id":"8bd296afde7d4726b6cfa758773de030","num":1,"rank":30},{"book_id":"6711f32af9eb4a1c97b2a5676c2bf232","num":1,"rank":30},{"book_id":"3c7714df30104b52b0738f066229d701","num":1,"rank":30},{"book_id":"bbadb7da35e546e28eb4a4c816f55d3a","num":1,"rank":30},{"book_id":"1aa028da91c84f9292ef64caa2d43a12","num":1,"rank":30},{"book_id":"8318664c753d414d83eabedbc8a122e2","num":1,"rank":30},{"book_id":"66ddfafed082458089e64c5524568859","num":1,"rank":30}]
  [INFO] [2017-12-07 10:50:39][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 10:50:37 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 10:53:08][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 10:53:09][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@6b9651f3: startup date [Thu Dec 07 10:53:09 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 10:53:09][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 10:53:09][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 10:53:10][org.zyp.testmybatis.TestMyBatis][{"book_id":"440eef2f51dc46a68f58249de19228c6","num":2504,"rank":1},{"book_id":"03124c0da03f4ce1937a2a4564d52309","num":1439,"rank":2},{"book_id":"dc70f449c7a24b80970ba279b85c05b0","num":1232,"rank":3},{"book_id":"c41dcb4d3d054cad82760e081437ba55","num":1051,"rank":4},{"book_id":"396a65b821774507b5d0d2a92a1a287f","num":970,"rank":5},{"book_id":"45ff80ee9e84495a8a01880e125cc0b3","num":781,"rank":6},{"book_id":"ce1e9ab7e13543e6bb5cf13864f73060","num":388,"rank":7},{"book_id":"e8007f9dd2754d97afce055c13b2b867","num":337,"rank":8},{"book_id":"b62f2f28d02d4f888c4523cb0d87eae0","num":57,"rank":9},{"book_id":"5eb2a70a2453447998a63a543b23f87f","num":51,"rank":10},{"book_id":"00cab91f0a5a48cc86cfa4435c9c87b1","num":44,"rank":11},{"book_id":"851bb631ccc54564b0591cd073f779ff","num":42,"rank":12},{"book_id":"138f8de35b974cfda45903bc92e45d3d","num":40,"rank":13},{"book_id":"980f3a4e9108438b9e12b798bd970807","num":39,"rank":14},{"book_id":"99210b3f60db48aca913cb1973fec3c9","num":35,"rank":15},{"book_id":"1070a2c5adb240c692ec00e2dd83b456","num":29,"rank":16},{"book_id":"6db9cbca359a4824a76ba1f8706ba58a","num":20,"rank":17},{"book_id":"729540a00cc84fbb874c735f5d853b65","num":18,"rank":18},{"book_id":"8a20ae9b41078847014129922bc80d7c","num":16,"rank":19},{"book_id":"e262a7a3f62645ebba377a8b33cfc6e2","num":15,"rank":20},{"book_id":"5165c30e1ad7491eb6dd3e1536d7e614","num":14,"rank":21},{"book_id":"da1df87e65f84bcb8634d16cb808994e","num":13,"rank":22},{"book_id":"498befb008844ff28aa27eab31a669f1","num":10,"rank":23},{"book_id":"a9dc8c634f3c4936b923ef60f8891627","num":10,"rank":23},{"book_id":"1ad83507275c4294a739784887371b6f","num":7,"rank":24},{"book_id":"7d2cdcf9a5e54f3d835de6a063b95ad1","num":6,"rank":25},{"book_id":"38df5bd30f6b4ef48d95d89376cbd9cb","num":6,"rank":25},{"book_id":"a586975c0477462c8feb52bb1303bb1e","num":5,"rank":26},{"book_id":"4a62e15e685f4f4f96826db8da23dbf7","num":4,"rank":27},{"book_id":"097dbb8ebdd344379a0e0adfb84b27b9","num":4,"rank":27},{"book_id":"a4c5a068fad049cca30ad9c1cef568a1","num":4,"rank":27},{"book_id":"70539e880139461c846033ae15dba25b","num":3,"rank":28},{"book_id":"2e4cb5649a554fa5a5dc2022045e4cf7","num":3,"rank":28},{"book_id":"1aa2dc061d864f85b640551a0010f400","num":3,"rank":28},{"book_id":"8481b5f2a3934aef8bd025f0f2c28d6d","num":3,"rank":28},{"book_id":"351bb43c60314784ad3a7e7d553cd5f8","num":3,"rank":28},{"book_id":"a847790d1e57438a94934b615299d1a5","num":3,"rank":28},{"book_id":"d8d9292061d34946abef24fabb606c8e","num":2,"rank":29},{"book_id":"40b4e061c08540a7990bff60f2ae59e1","num":2,"rank":29},{"book_id":"d75926ac587d489ea07cf36adf36c967","num":2,"rank":29},{"book_id":"c04ab8fac9af4ab097ecb1b28da9fe6f","num":2,"rank":29},{"book_id":"1fce544e9230409e980aac04a9e1c487","num":2,"rank":29},{"book_id":"8318664c753d414d83eabedbc8a122e2","num":1,"rank":30},{"book_id":"66ddfafed082458089e64c5524568859","num":1,"rank":30},{"book_id":"253521df08d149c7bcc7c89eb897250f","num":1,"rank":30},{"book_id":"11d80bb736db47c9a3dda8fd438ce1a4","num":1,"rank":30},{"book_id":"8bd296afde7d4726b6cfa758773de030","num":1,"rank":30},{"book_id":"6711f32af9eb4a1c97b2a5676c2bf232","num":1,"rank":30},{"book_id":"3c7714df30104b52b0738f066229d701","num":1,"rank":30},{"book_id":"bbadb7da35e546e28eb4a4c816f55d3a","num":1,"rank":30},{"book_id":"1aa028da91c84f9292ef64caa2d43a12","num":1,"rank":30}]
  [INFO] [2017-12-07 10:53:10][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@6b9651f3: startup date [Thu Dec 07 10:53:09 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:04:24][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 11:04:25][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:04:25 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:04:25][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 11:04:25][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 11:04:26][org.zyp.testmybatis.TestMyBatis]{"id":1,"user_id":"163ulcljupm"}
  [INFO] [2017-12-07 11:04:26][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:04:25 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:05:28][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 11:05:29][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:05:29 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:05:29][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 11:05:29][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 11:05:30][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:05:29 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:10:40][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 11:10:41][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:10:41 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:10:41][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 11:10:41][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 11:10:42][org.zyp.testmybatis.TestMyBatis]{"id":1,"user_id":"163ulcljupm"}
  [INFO] [2017-12-07 11:10:42][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:10:41 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:11:09][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 11:11:10][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:11:10 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:11:10][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 11:11:10][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 11:11:11][org.zyp.testmybatis.TestMyBatis]{"id":1,"user_id":"163ulcljupm"}
  [INFO] [2017-12-07 11:11:11][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:11:10 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:12:43][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 11:12:43][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:12:43 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:12:43][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 11:12:43][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 11:12:44][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:12:43 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:13:15][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 11:13:15][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:13:15 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:13:16][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 11:13:16][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 11:13:17][org.zyp.testmybatis.TestMyBatis]{"id":1,"user_id":"163ulcljupm"}
  [INFO] [2017-12-07 11:13:17][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:13:15 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:19:49][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 11:19:50][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:19:50 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:19:50][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 11:19:50][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 11:19:51][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:19:50 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:20:42][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 11:20:43][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:20:43 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:20:43][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 11:20:43][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 11:20:44][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:20:43 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:25:53][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 11:25:54][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:25:54 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:25:54][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 11:25:54][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 11:25:55][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:25:54 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:26:14][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 11:26:14][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:26:14 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:26:14][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 11:26:14][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 11:26:16][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:26:14 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:28:53][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 11:28:53][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:28:53 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:28:54][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 11:28:54][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 11:28:55][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:28:53 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:31:48][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 11:31:48][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:31:48 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:31:48][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 11:31:48][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 11:31:50][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:31:48 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:33:55][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 11:33:56][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:33:56 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:33:56][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 11:33:56][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 11:33:57][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:33:56 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:36:44][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 11:36:45][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:36:45 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:36:45][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 11:36:45][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 11:36:46][org.zyp.testmybatis.TestMyBatis]{"book10_no":"78-2.3679228","book1_no":"97-3.9992483","book2_no":"43-3.8377337","book3_no":"366-3.17177","book4_no":"264-3.0743296","book5_no":"256-2.9858916","book6_no":"54-2.7529395","book7_no":"55-2.4843833","book8_no":"211-2.450248","book9_no":"152-2.4386883","user_id":1}
  [INFO] [2017-12-07 11:36:46][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@9a7504c: startup date [Thu Dec 07 11:36:45 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:38:40][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 11:38:41][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2c039ac6: startup date [Thu Dec 07 11:38:41 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:38:41][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 11:38:41][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 11:38:42][org.zyp.testmybatis.TestMyBatis]{"book10_no":"78-2.3679228","book1_no":"97-3.9992483","book2_no":"43-3.8377337","book3_no":"366-3.17177","book4_no":"264-3.0743296","book5_no":"256-2.9858916","book6_no":"54-2.7529395","book7_no":"55-2.4843833","book8_no":"211-2.450248","book9_no":"152-2.4386883","user_id":1}
  [INFO] [2017-12-07 11:38:42][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2c039ac6: startup date [Thu Dec 07 11:38:41 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:39:42][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-07 11:39:42][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@587d1d39: startup date [Thu Dec 07 11:39:42 CST 2017]; root of context hierarchy
  [INFO] [2017-12-07 11:39:43][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-07 11:39:43][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-07 11:39:44][org.zyp.testmybatis.TestMyBatis]{"351bb43c60314784ad3a7e7d553cd5f8":"3.9992483","1aa028da91c84f9292ef64caa2d43a12":"3.8377337","e71b1876ae034f53a9944a2b47a2f2f6":"3.17177","9fa69b855999478daef0ed50c35da00e":"3.0743296","9c183a1696874f68bc0eebefa03c459b":"2.9858916","1fce544e9230409e980aac04a9e1c487":"2.7529395","21b4a6cb5fcd4c6a91afebb8b79b235a":"2.4843833","8481b5f2a3934aef8bd025f0f2c28d6d":"2.450248","597c591fb939412ab7addf4db4e85122":"2.4386883","2c29e880626845c28449db5fe5f38507":"2.3679228"}
  [INFO] [2017-12-07 11:39:44][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@587d1d39: startup date [Thu Dec 07 11:39:42 CST 2017]; root of context hierarchy
  [INFO] [2017-12-14 11:40:40][org.apache.spark.SparkContext]Running Spark version 2.2.0
  [WARN] [2017-12-14 11:40:40][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-14 11:40:41][org.apache.spark.util.Utils]Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.18.5 instead (on interface eth0)
  [WARN] [2017-12-14 11:40:41][org.apache.spark.util.Utils]Set SPARK_LOCAL_IP if you need to bind to another address
  [INFO] [2017-12-14 11:40:41][org.apache.spark.SparkContext]Submitted application: JavaKMeansExample
  [INFO] [2017-12-14 11:40:41][org.apache.spark.SecurityManager]Changing view acls to: hadoop
  [INFO] [2017-12-14 11:40:41][org.apache.spark.SecurityManager]Changing modify acls to: hadoop
  [INFO] [2017-12-14 11:40:41][org.apache.spark.SecurityManager]Changing view acls groups to: 
  [INFO] [2017-12-14 11:40:41][org.apache.spark.SecurityManager]Changing modify acls groups to: 
  [INFO] [2017-12-14 11:40:41][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
  [INFO] [2017-12-14 11:40:42][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 33243.
  [INFO] [2017-12-14 11:40:42][org.apache.spark.SparkEnv]Registering MapOutputTracker
  [INFO] [2017-12-14 11:40:42][org.apache.spark.SparkEnv]Registering BlockManagerMaster
  [INFO] [2017-12-14 11:40:42][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
  [INFO] [2017-12-14 11:40:42][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
  [INFO] [2017-12-14 11:40:42][org.apache.spark.storage.DiskBlockManager]Created local directory at /tmp/blockmgr-7daf0477-9384-41aa-94bb-37fe75a7c217
  [INFO] [2017-12-14 11:40:42][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 882.6 MB
  [INFO] [2017-12-14 11:40:42][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.util.log]Logging initialized @3790ms
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.Server]Started @3915ms
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@3347100e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 11:40:42][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b5176f2{/jobs,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f811d00{/jobs/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4089713{/jobs/job,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@b91d8c4{/jobs/job/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a77614d{/stages,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a067c25{/stages/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bde62ff{/stages/stage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@791cbf87{/stages/stage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@754777cd{/stages/pool,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@372ea2bc{/stages/pool/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f08c4b{/storage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7de0c6ae{/storage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@cdc3aae{/storage/rdd,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dcbb60{/storage/rdd/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21526f6c{/environment,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@299266e2{/environment/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66ea1466{/executors,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bffddff{/executors/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@50687efb{/executors/threadDump,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@142eef62{/executors/threadDump/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5990e6c5{/static,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21694e53{/,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@22c86919{/api,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ea1bcdc{/jobs/job/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64712be{/stages/stage/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:42][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.18.5:4040
  [INFO] [2017-12-14 11:40:43][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
  [INFO] [2017-12-14 11:40:43][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34357.
  [INFO] [2017-12-14 11:40:43][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.18.5:34357
  [INFO] [2017-12-14 11:40:43][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
  [INFO] [2017-12-14 11:40:43][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.18.5, 34357, None)
  [INFO] [2017-12-14 11:40:43][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.18.5:34357 with 882.6 MB RAM, BlockManagerId(driver, 192.168.18.5, 34357, None)
  [INFO] [2017-12-14 11:40:43][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.18.5, 34357, None)
  [INFO] [2017-12-14 11:40:43][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.18.5, 34357, None)
  [INFO] [2017-12-14 11:40:43][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1532c619{/metrics/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:44][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 209.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 11:40:44][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 11:40:44][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.18.5:34357 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 11:40:44][org.apache.spark.SparkContext]Created broadcast 0 from textFile at ModelTest.java:42
  [INFO] [2017-12-14 11:40:47][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 11:40:47][org.apache.spark.SparkContext]Starting job: takeSample at KMeans.scala:353
  [INFO] [2017-12-14 11:40:47][org.apache.spark.scheduler.DAGScheduler]Got job 0 (takeSample at KMeans.scala:353) with 1 output partitions
  [INFO] [2017-12-14 11:40:47][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (takeSample at KMeans.scala:353)
  [INFO] [2017-12-14 11:40:47][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 11:40:47][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 11:40:47][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[5] at map at KMeans.scala:224), which has no missing parents
  [INFO] [2017-12-14 11:40:47][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 4.5 KB, free 882.4 MB)
  [INFO] [2017-12-14 11:40:47][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 882.4 MB)
  [INFO] [2017-12-14 11:40:47][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.18.5:34357 (size: 2.5 KB, free: 882.6 MB)
  [INFO] [2017-12-14 11:40:47][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:40:47][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at map at KMeans.scala:224) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:40:47][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
  [INFO] [2017-12-14 11:40:47][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5195 bytes)
  [INFO] [2017-12-14 11:40:47][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
  [INFO] [2017-12-14 11:40:47][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/output/clean/JuLei/part-r-00000:0+4881501
  [INFO] [2017-12-14 11:40:49][org.apache.spark.storage.memory.MemoryStore]Block rdd_2_0 stored as values in memory (estimated size 11.6 MB, free 870.8 MB)
  [INFO] [2017-12-14 11:40:49][org.apache.spark.storage.BlockManagerInfo]Added rdd_2_0 in memory on 192.168.18.5:34357 (size: 11.6 MB, free: 871.0 MB)
  [INFO] [2017-12-14 11:40:49][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 11:40:49][org.apache.spark.storage.memory.MemoryStore]Block rdd_3_0 stored as values in memory (estimated size 1828.9 KB, free 869.0 MB)
  [INFO] [2017-12-14 11:40:49][org.apache.spark.storage.BlockManagerInfo]Added rdd_3_0 in memory on 192.168.18.5:34357 (size: 1828.9 KB, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:49][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 1662 bytes result sent to driver
  [INFO] [2017-12-14 11:40:49][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 1987 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:40:49][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (takeSample at KMeans.scala:353) finished in 2.020 s
  [INFO] [2017-12-14 11:40:49][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:40:49][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: takeSample at KMeans.scala:353, took 2.494182 s
  [INFO] [2017-12-14 11:40:49][org.apache.spark.SparkContext]Starting job: takeSample at KMeans.scala:353
  [INFO] [2017-12-14 11:40:49][org.apache.spark.scheduler.DAGScheduler]Got job 1 (takeSample at KMeans.scala:353) with 1 output partitions
  [INFO] [2017-12-14 11:40:49][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (takeSample at KMeans.scala:353)
  [INFO] [2017-12-14 11:40:49][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 11:40:49][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 11:40:49][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (PartitionwiseSampledRDD[7] at takeSample at KMeans.scala:353), which has no missing parents
  [INFO] [2017-12-14 11:40:49][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 5.2 KB, free 869.0 MB)
  [INFO] [2017-12-14 11:40:49][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.9 KB, free 869.0 MB)
  [INFO] [2017-12-14 11:40:49][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.18.5:34357 (size: 2.9 KB, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:49][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:40:49][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (PartitionwiseSampledRDD[7] at takeSample at KMeans.scala:353) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:40:49][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
  [INFO] [2017-12-14 11:40:49][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5304 bytes)
  [INFO] [2017-12-14 11:40:49][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
  [INFO] [2017-12-14 11:40:49][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 11:40:49][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 11:40:49][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.18.5:34357 in memory (size: 2.5 KB, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 1894 bytes result sent to driver
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 284 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (takeSample at KMeans.scala:353) finished in 0.283 s
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: takeSample at KMeans.scala:353, took 0.316900 s
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 136.0 B, free 869.0 MB)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 344.0 B, free 869.0 MB)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.18.5:34357 (size: 344.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at KMeans.scala:367
  [INFO] [2017-12-14 11:40:50][org.apache.spark.SparkContext]Starting job: sum at KMeans.scala:373
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]Got job 2 (sum at KMeans.scala:373) with 1 output partitions
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (sum at KMeans.scala:373)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[9] at map at KMeans.scala:370), which has no missing parents
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 5.4 KB, free 869.0 MB)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.9 KB, free 869.0 MB)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.18.5:34357 (size: 2.9 KB, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at map at KMeans.scala:370) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 5227 bytes)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [WARN] [2017-12-14 11:40:50][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
  [WARN] [2017-12-14 11:40:50][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.memory.MemoryStore]Block rdd_9_0 stored as values in memory (estimated size 1828.9 KB, free 867.2 MB)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.BlockManagerInfo]Added rdd_9_0 in memory on 192.168.18.5:34357 (size: 1828.9 KB, free: 867.4 MB)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 1531 bytes result sent to driver
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 642 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (sum at KMeans.scala:373) finished in 0.642 s
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: sum at KMeans.scala:373, took 0.665107 s
  [INFO] [2017-12-14 11:40:50][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 6 from persistence list
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.BlockManager]Removing RDD 6
  [INFO] [2017-12-14 11:40:50][org.apache.spark.SparkContext]Starting job: collect at KMeans.scala:381
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]Got job 3 (collect at KMeans.scala:381) with 1 output partitions
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (collect at KMeans.scala:381)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[11] at mapPartitionsWithIndex at KMeans.scala:378), which has no missing parents
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 6.0 KB, free 867.2 MB)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.2 KB, free 867.2 MB)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.18.5:34357 (size: 3.2 KB, free: 867.4 MB)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at mapPartitionsWithIndex at KMeans.scala:378) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 5259 bytes)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.BlockManager]Found block rdd_9_0 locally
  [INFO] [2017-12-14 11:40:50][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 1435 bytes result sent to driver
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 162 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (collect at KMeans.scala:381) finished in 0.163 s
  [INFO] [2017-12-14 11:40:50][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: collect at KMeans.scala:381, took 0.191524 s
  [INFO] [2017-12-14 11:40:50][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 672.0 B, free 867.2 MB)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 576.0 B, free 867.2 MB)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.18.5:34357 (size: 576.0 B, free: 867.4 MB)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at KMeans.scala:367
  [INFO] [2017-12-14 11:40:51][org.apache.spark.SparkContext]Starting job: sum at KMeans.scala:373
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.DAGScheduler]Got job 4 (sum at KMeans.scala:373) with 1 output partitions
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (sum at KMeans.scala:373)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[13] at map at KMeans.scala:370), which has no missing parents
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 5.6 KB, free 867.2 MB)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.0 KB, free 867.2 MB)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.18.5:34357 (size: 3.0 KB, free: 867.4 MB)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at map at KMeans.scala:370) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 5259 bytes)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.BlockManager]Found block rdd_9_0 locally
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.memory.MemoryStore]Block rdd_13_0 stored as values in memory (estimated size 1828.9 KB, free 865.4 MB)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.BlockManagerInfo]Added rdd_13_0 in memory on 192.168.18.5:34357 (size: 1828.9 KB, free: 865.6 MB)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 1531 bytes result sent to driver
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 336 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (sum at KMeans.scala:373) finished in 0.342 s
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: sum at KMeans.scala:373, took 0.371099 s
  [INFO] [2017-12-14 11:40:51][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.BlockManager]Removing RDD 9
  [INFO] [2017-12-14 11:40:51][org.apache.spark.SparkContext]Starting job: collect at KMeans.scala:381
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.DAGScheduler]Got job 5 (collect at KMeans.scala:381) with 1 output partitions
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (collect at KMeans.scala:381)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[15] at mapPartitionsWithIndex at KMeans.scala:378), which has no missing parents
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 6.3 KB, free 867.2 MB)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.3 KB, free 867.2 MB)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.18.5:34357 (size: 3.3 KB, free: 867.4 MB)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[15] at mapPartitionsWithIndex at KMeans.scala:378) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 5291 bytes)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.BlockManager]Found block rdd_13_0 locally
  [INFO] [2017-12-14 11:40:51][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 1530 bytes result sent to driver
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 165 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (collect at KMeans.scala:381) finished in 0.155 s
  [INFO] [2017-12-14 11:40:51][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: collect at KMeans.scala:381, took 0.212714 s
  [INFO] [2017-12-14 11:40:51][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.BlockManager]Removing RDD 13
  [INFO] [2017-12-14 11:40:51][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(3) (from destroy at KMeans.scala:388)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.18.5:34357 in memory (size: 344.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(6) (from destroy at KMeans.scala:388)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 192.168.18.5:34357 in memory (size: 576.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 1584.0 B, free 868.9 MB)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 952.0 B, free 868.9 MB)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.18.5:34357 (size: 952.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:51][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at KMeans.scala:398
  [INFO] [2017-12-14 11:40:51][org.apache.spark.SparkContext]Starting job: countByValue at KMeans.scala:399
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.DAGScheduler]Registering RDD 18 (countByValue at KMeans.scala:399)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.DAGScheduler]Got job 6 (countByValue at KMeans.scala:399) with 1 output partitions
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (countByValue at KMeans.scala:399)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 6)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 6)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 6 (MapPartitionsRDD[18] at countByValue at KMeans.scala:399), which has no missing parents
  [INFO] [2017-12-14 11:40:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 6.6 KB, free 868.9 MB)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.6 KB, free 868.9 MB)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 192.168.18.5:34357 (size: 3.6 KB, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[18] at countByValue at KMeans.scala:399) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 5184 bytes)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 11:40:52][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 11:40:52][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 192.168.18.5:34357 in memory (size: 3.0 KB, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.18.5:34357 in memory (size: 3.2 KB, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.18.5:34357 in memory (size: 2.9 KB, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.18.5:34357 in memory (size: 2.9 KB, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_8_piece0 on 192.168.18.5:34357 in memory (size: 3.3 KB, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 1196 bytes result sent to driver
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 805 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 6 (countByValue at KMeans.scala:399) finished in 0.805 s
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.DAGScheduler]running: Set()
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 7)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.DAGScheduler]failed: Set()
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (ShuffledRDD[19] at countByValue at KMeans.scala:399), which has no missing parents
  [INFO] [2017-12-14 11:40:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.2 KB, free 869.0 MB)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 1971.0 B, free 869.0 MB)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 192.168.18.5:34357 (size: 1971.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (ShuffledRDD[19] at countByValue at KMeans.scala:399) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
  [INFO] [2017-12-14 11:40:52][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4621 bytes)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
  [INFO] [2017-12-14 11:40:52][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 4 ms
  [INFO] [2017-12-14 11:40:53][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 1900 bytes result sent to driver
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 80 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (countByValue at KMeans.scala:399) finished in 0.089 s
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: countByValue at KMeans.scala:399, took 1.228554 s
  [INFO] [2017-12-14 11:40:53][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(9) (from destroy at KMeans.scala:401)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_9_piece0 on 192.168.18.5:34357 in memory (size: 952.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.mllib.clustering.LocalKMeans]Local KMeans++ converged in 3 iterations.
  [INFO] [2017-12-14 11:40:53][org.apache.spark.mllib.clustering.KMeans]Initialization with k-means|| took 7.888 seconds.
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 496.0 B, free 869.0 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 447.0 B, free 869.0 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 192.168.18.5:34357 (size: 447.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at KMeans.scala:273
  [INFO] [2017-12-14 11:40:53][org.apache.spark.SparkContext]Starting job: collectAsMap at KMeans.scala:295
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Registering RDD 20 (mapPartitions at KMeans.scala:276)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Got job 7 (collectAsMap at KMeans.scala:295) with 1 output partitions
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (collectAsMap at KMeans.scala:295)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 8)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 8)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 8 (MapPartitionsRDD[20] at mapPartitions at KMeans.scala:276), which has no missing parents
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 6.2 KB, free 869.0 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.4 KB, free 869.0 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 192.168.18.5:34357 (size: 3.4 KB, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[20] at mapPartitions at KMeans.scala:276) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 5184 bytes)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 11:40:53][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 1223 bytes result sent to driver
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 304 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 8 (mapPartitions at KMeans.scala:276) finished in 0.308 s
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]running: Set()
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 9)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]failed: Set()
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (ShuffledRDD[21] at reduceByKey at KMeans.scala:292), which has no missing parents
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 2.8 KB, free 869.0 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 1654.0 B, free 869.0 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 192.168.18.5:34357 (size: 1654.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (ShuffledRDD[21] at reduceByKey at KMeans.scala:292) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 4621 bytes)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
  [INFO] [2017-12-14 11:40:53][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 1787 bytes result sent to driver
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 13 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (collectAsMap at KMeans.scala:295) finished in 0.014 s
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: collectAsMap at KMeans.scala:295, took 0.381070 s
  [INFO] [2017-12-14 11:40:53][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(12) (from destroy at KMeans.scala:297)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_12_piece0 on 192.168.18.5:34357 in memory (size: 447.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 496.0 B, free 869.0 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 457.0 B, free 869.0 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 192.168.18.5:34357 (size: 457.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at KMeans.scala:273
  [INFO] [2017-12-14 11:40:53][org.apache.spark.SparkContext]Starting job: collectAsMap at KMeans.scala:295
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Registering RDD 22 (mapPartitions at KMeans.scala:276)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Got job 8 (collectAsMap at KMeans.scala:295) with 1 output partitions
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (collectAsMap at KMeans.scala:295)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 10)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 10)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 10 (MapPartitionsRDD[22] at mapPartitions at KMeans.scala:276), which has no missing parents
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 6.2 KB, free 868.9 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.4 KB, free 868.9 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 192.168.18.5:34357 (size: 3.4 KB, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[22] at mapPartitions at KMeans.scala:276) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 5184 bytes)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 11:40:53][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 1223 bytes result sent to driver
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 89 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 10 (mapPartitions at KMeans.scala:276) finished in 0.086 s
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]running: Set()
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 11)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]failed: Set()
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (ShuffledRDD[23] at reduceByKey at KMeans.scala:292), which has no missing parents
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17 stored as values in memory (estimated size 2.8 KB, free 868.9 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17_piece0 stored as bytes in memory (estimated size 1659.0 B, free 868.9 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManagerInfo]Added broadcast_17_piece0 in memory on 192.168.18.5:34357 (size: 1659.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.SparkContext]Created broadcast 17 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (ShuffledRDD[23] at reduceByKey at KMeans.scala:292) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, ANY, 4621 bytes)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
  [INFO] [2017-12-14 11:40:53][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 1744 bytes result sent to driver
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 7 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (collectAsMap at KMeans.scala:295) finished in 0.008 s
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: collectAsMap at KMeans.scala:295, took 0.124135 s
  [INFO] [2017-12-14 11:40:53][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(15) (from destroy at KMeans.scala:297)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18 stored as values in memory (estimated size 496.0 B, free 868.9 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_15_piece0 on 192.168.18.5:34357 in memory (size: 457.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18_piece0 stored as bytes in memory (estimated size 457.0 B, free 868.9 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManagerInfo]Added broadcast_18_piece0 in memory on 192.168.18.5:34357 (size: 457.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.SparkContext]Created broadcast 18 from broadcast at KMeans.scala:273
  [INFO] [2017-12-14 11:40:53][org.apache.spark.SparkContext]Starting job: collectAsMap at KMeans.scala:295
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Registering RDD 24 (mapPartitions at KMeans.scala:276)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Got job 9 (collectAsMap at KMeans.scala:295) with 1 output partitions
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (collectAsMap at KMeans.scala:295)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 12)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 12)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 12 (MapPartitionsRDD[24] at mapPartitions at KMeans.scala:276), which has no missing parents
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19 stored as values in memory (estimated size 6.2 KB, free 868.9 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19_piece0 stored as bytes in memory (estimated size 3.4 KB, free 868.9 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManagerInfo]Added broadcast_19_piece0 in memory on 192.168.18.5:34357 (size: 3.4 KB, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.SparkContext]Created broadcast 19 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[24] at mapPartitions at KMeans.scala:276) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 5184 bytes)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 11:40:53][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 1223 bytes result sent to driver
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 89 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 12 (mapPartitions at KMeans.scala:276) finished in 0.077 s
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]running: Set()
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 13)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]failed: Set()
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (ShuffledRDD[25] at reduceByKey at KMeans.scala:292), which has no missing parents
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20 stored as values in memory (estimated size 2.8 KB, free 868.9 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20_piece0 stored as bytes in memory (estimated size 1659.0 B, free 868.9 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManagerInfo]Added broadcast_20_piece0 in memory on 192.168.18.5:34357 (size: 1659.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.SparkContext]Created broadcast 20 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (ShuffledRDD[25] at reduceByKey at KMeans.scala:292) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, ANY, 4621 bytes)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 13)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
  [INFO] [2017-12-14 11:40:53][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 13). 1787 bytes result sent to driver
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 13) in 8 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (collectAsMap at KMeans.scala:295) finished in 0.010 s
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: collectAsMap at KMeans.scala:295, took 0.141142 s
  [INFO] [2017-12-14 11:40:53][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(18) (from destroy at KMeans.scala:297)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.mllib.clustering.KMeans]Iterations took 0.736 seconds.
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_18_piece0 on 192.168.18.5:34357 in memory (size: 457.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.mllib.clustering.KMeans]KMeans converged in 3 iterations.
  [INFO] [2017-12-14 11:40:53][org.apache.spark.mllib.clustering.KMeans]The cost is 277052.7154289775.
  [INFO] [2017-12-14 11:40:53][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManager]Removing RDD 3
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21 stored as values in memory (estimated size 209.1 KB, free 870.5 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21_piece0 stored as bytes in memory (estimated size 20.1 KB, free 870.5 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManagerInfo]Added broadcast_21_piece0 in memory on 192.168.18.5:34357 (size: 20.1 KB, free: 870.9 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.SparkContext]Created broadcast 21 from textFile at modelSaveLoad.scala:129
  [INFO] [2017-12-14 11:40:53][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 11:40:53][org.apache.spark.SparkContext]Starting job: first at modelSaveLoad.scala:129
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Got job 10 (first at modelSaveLoad.scala:129) with 1 output partitions
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 14 (first at modelSaveLoad.scala:129)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 14 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[27] at textFile at modelSaveLoad.scala:129), which has no missing parents
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22 stored as values in memory (estimated size 3.3 KB, free 870.5 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22_piece0 stored as bytes in memory (estimated size 1981.0 B, free 870.5 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.storage.BlockManagerInfo]Added broadcast_22_piece0 in memory on 192.168.18.5:34357 (size: 1981.0 B, free: 870.9 MB)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.SparkContext]Created broadcast 22 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 14 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[27] at textFile at modelSaveLoad.scala:129) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
  [INFO] [2017-12-14 11:40:53][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, ANY, 4926 bytes)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 14)
  [INFO] [2017-12-14 11:40:53][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata/part-00000:0+80
  [INFO] [2017-12-14 11:40:54][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 14). 876 bytes result sent to driver
  [INFO] [2017-12-14 11:40:54][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 14) in 46 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:40:54][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:40:54][org.apache.spark.scheduler.DAGScheduler]ResultStage 14 (first at modelSaveLoad.scala:129) finished in 0.030 s
  [INFO] [2017-12-14 11:40:54][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: first at modelSaveLoad.scala:129, took 0.073459 s
  [INFO] [2017-12-14 11:40:54][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse').
  [INFO] [2017-12-14 11:40:54][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse'.
  [INFO] [2017-12-14 11:40:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d53e6f7{/SQL,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3ece79fe{/SQL/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b4125ed{/SQL/execution,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@e7b265e{/SQL/execution/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4bb147ec{/static/sql,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 11:40:54][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
  [INFO] [2017-12-14 11:40:55][org.apache.spark.SparkContext]Starting job: parquet at KMeansModel.scala:142
  [INFO] [2017-12-14 11:40:55][org.apache.spark.scheduler.DAGScheduler]Got job 11 (parquet at KMeansModel.scala:142) with 1 output partitions
  [INFO] [2017-12-14 11:40:55][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (parquet at KMeansModel.scala:142)
  [INFO] [2017-12-14 11:40:55][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 11:40:55][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 11:40:55][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (MapPartitionsRDD[29] at parquet at KMeansModel.scala:142), which has no missing parents
  [INFO] [2017-12-14 11:40:55][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23 stored as values in memory (estimated size 62.1 KB, free 870.4 MB)
  [INFO] [2017-12-14 11:40:55][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23_piece0 stored as bytes in memory (estimated size 21.8 KB, free 870.4 MB)
  [INFO] [2017-12-14 11:40:55][org.apache.spark.storage.BlockManagerInfo]Added broadcast_23_piece0 in memory on 192.168.18.5:34357 (size: 21.8 KB, free: 870.9 MB)
  [INFO] [2017-12-14 11:40:55][org.apache.spark.SparkContext]Created broadcast 23 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:40:55][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[29] at parquet at KMeansModel.scala:142) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:40:55][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 1 tasks
  [INFO] [2017-12-14 11:40:55][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 5081 bytes)
  [INFO] [2017-12-14 11:40:55][org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 15)
  [INFO] [2017-12-14 11:40:55][org.apache.spark.ContextCleaner]Cleaned accumulator 216
  [INFO] [2017-12-14 11:40:55][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_17_piece0 on 192.168.18.5:34357 in memory (size: 1659.0 B, free: 870.9 MB)
  [INFO] [2017-12-14 11:40:55][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_14_piece0 on 192.168.18.5:34357 in memory (size: 1654.0 B, free: 870.9 MB)
  [INFO] [2017-12-14 11:40:55][org.apache.spark.ContextCleaner]Cleaned accumulator 265
  [INFO] [2017-12-14 11:40:55][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_11_piece0 on 192.168.18.5:34357 in memory (size: 1971.0 B, free: 870.9 MB)
  [INFO] [2017-12-14 11:40:55][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_20_piece0 on 192.168.18.5:34357 in memory (size: 1659.0 B, free: 870.9 MB)
  [INFO] [2017-12-14 11:40:55][org.apache.spark.ContextCleaner]Cleaned shuffle 2
  [INFO] [2017-12-14 11:40:55][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_21_piece0 on 192.168.18.5:34357 in memory (size: 20.1 KB, free: 870.9 MB)
  [INFO] [2017-12-14 11:40:55][org.apache.spark.storage.BlockManager]Removing RDD 13
  [INFO] [2017-12-14 11:40:55][org.apache.spark.ContextCleaner]Cleaned RDD 13
  [INFO] [2017-12-14 11:40:55][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_10_piece0 on 192.168.18.5:34357 in memory (size: 3.6 KB, free: 870.9 MB)
  [INFO] [2017-12-14 11:40:55][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_13_piece0 on 192.168.18.5:34357 in memory (size: 3.4 KB, free: 870.9 MB)
  [INFO] [2017-12-14 11:40:55][org.apache.spark.ContextCleaner]Cleaned shuffle 1
  [INFO] [2017-12-14 11:40:55][org.apache.spark.ContextCleaner]Cleaned shuffle 0
  [INFO] [2017-12-14 11:40:55][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_19_piece0 on 192.168.18.5:34357 in memory (size: 3.4 KB, free: 870.9 MB)
  [INFO] [2017-12-14 11:40:56][org.apache.spark.storage.BlockManager]Removing RDD 9
  [INFO] [2017-12-14 11:40:56][org.apache.spark.ContextCleaner]Cleaned RDD 9
  [INFO] [2017-12-14 11:40:56][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_22_piece0 on 192.168.18.5:34357 in memory (size: 1981.0 B, free: 871.0 MB)
  [INFO] [2017-12-14 11:40:56][org.apache.spark.ContextCleaner]Cleaned accumulator 314
  [INFO] [2017-12-14 11:40:56][org.apache.spark.ContextCleaner]Cleaned shuffle 3
  [INFO] [2017-12-14 11:40:56][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_16_piece0 on 192.168.18.5:34357 in memory (size: 3.4 KB, free: 871.0 MB)
  [INFO] [2017-12-14 11:40:56][org.apache.spark.storage.BlockManager]Removing RDD 3
  [INFO] [2017-12-14 11:40:56][org.apache.spark.ContextCleaner]Cleaned RDD 3
  [INFO] [2017-12-14 11:40:56][org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 15). 1687 bytes result sent to driver
  [INFO] [2017-12-14 11:40:56][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 15) in 1054 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:40:56][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:40:56][org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (parquet at KMeansModel.scala:142) finished in 1.055 s
  [INFO] [2017-12-14 11:40:56][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: parquet at KMeansModel.scala:142, took 1.141580 s
  [INFO] [2017-12-14 11:40:58][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
  [INFO] [2017-12-14 11:40:58][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: 
  [INFO] [2017-12-14 11:40:58][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<id: int, point: vector>
  [INFO] [2017-12-14 11:40:58][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
  [INFO] [2017-12-14 11:40:59][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 304.936119 ms
  [INFO] [2017-12-14 11:40:59][org.apache.spark.storage.memory.MemoryStore]Block broadcast_24 stored as values in memory (estimated size 222.5 KB, free 870.5 MB)
  [INFO] [2017-12-14 11:40:59][org.apache.spark.storage.memory.MemoryStore]Block broadcast_24_piece0 stored as bytes in memory (estimated size 21.2 KB, free 870.5 MB)
  [INFO] [2017-12-14 11:40:59][org.apache.spark.storage.BlockManagerInfo]Added broadcast_24_piece0 in memory on 192.168.18.5:34357 (size: 21.2 KB, free: 870.9 MB)
  [INFO] [2017-12-14 11:40:59][org.apache.spark.SparkContext]Created broadcast 24 from rdd at KMeansModel.scala:144
  [INFO] [2017-12-14 11:40:59][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 4195846 bytes, open cost is considered as scanning 4194304 bytes.
  [INFO] [2017-12-14 11:40:59][org.apache.spark.SparkContext]Starting job: collect at KMeansModel.scala:144
  [INFO] [2017-12-14 11:40:59][org.apache.spark.scheduler.DAGScheduler]Got job 12 (collect at KMeansModel.scala:144) with 1 output partitions
  [INFO] [2017-12-14 11:40:59][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 16 (collect at KMeansModel.scala:144)
  [INFO] [2017-12-14 11:40:59][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 11:40:59][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 11:40:59][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 16 (MapPartitionsRDD[34] at map at KMeansModel.scala:144), which has no missing parents
  [INFO] [2017-12-14 11:40:59][org.apache.spark.storage.memory.MemoryStore]Block broadcast_25 stored as values in memory (estimated size 15.9 KB, free 870.4 MB)
  [INFO] [2017-12-14 11:40:59][org.apache.spark.storage.memory.MemoryStore]Block broadcast_25_piece0 stored as bytes in memory (estimated size 7.3 KB, free 870.4 MB)
  [INFO] [2017-12-14 11:40:59][org.apache.spark.storage.BlockManagerInfo]Added broadcast_25_piece0 in memory on 192.168.18.5:34357 (size: 7.3 KB, free: 870.9 MB)
  [INFO] [2017-12-14 11:40:59][org.apache.spark.SparkContext]Created broadcast 25 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:40:59][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[34] at map at KMeansModel.scala:144) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:40:59][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 16.0 with 1 tasks
  [INFO] [2017-12-14 11:40:59][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, ANY, 5392 bytes)
  [INFO] [2017-12-14 11:40:59][org.apache.spark.executor.Executor]Running task 0.0 in stage 16.0 (TID 16)
  [INFO] [2017-12-14 11:40:59][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 42.702073 ms
  [INFO] [2017-12-14 11:40:59][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/data/part-00000-0d1f8488-b668-4e2d-b619-3e79f8cfb51c.snappy.parquet, range: 0-1542, partition values: [empty row]
  [INFO] [2017-12-14 11:40:59][org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport]Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
  [INFO] [2017-12-14 11:41:00][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 27.903983 ms
  [INFO] [2017-12-14 11:41:00][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 31.850148 ms
  [INFO] [2017-12-14 11:41:00][org.apache.parquet.hadoop.InternalParquetRecordReader]RecordReader initialized will read a total of 5 records.
  [INFO] [2017-12-14 11:41:00][org.apache.parquet.hadoop.InternalParquetRecordReader]at row 0. reading next block
  [INFO] [2017-12-14 11:41:00][org.apache.hadoop.io.compress.CodecPool]Got brand-new decompressor [.snappy]
  [INFO] [2017-12-14 11:41:00][org.apache.parquet.hadoop.InternalParquetRecordReader]block read in memory in 53 ms. row count = 5
  [INFO] [2017-12-14 11:41:00][org.apache.spark.executor.Executor]Finished task 0.0 in stage 16.0 (TID 16). 1841 bytes result sent to driver
  [INFO] [2017-12-14 11:41:00][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 16.0 (TID 16) in 782 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:41:00][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 16.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:41:00][org.apache.spark.scheduler.DAGScheduler]ResultStage 16 (collect at KMeansModel.scala:144) finished in 0.785 s
  [INFO] [2017-12-14 11:41:00][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: collect at KMeansModel.scala:144, took 0.801040 s
  [INFO] [2017-12-14 11:41:00][org.apache.spark.SparkContext]Starting job: collect at ModelTest.java:88
  [INFO] [2017-12-14 11:41:00][org.apache.spark.scheduler.DAGScheduler]Got job 13 (collect at ModelTest.java:88) with 1 output partitions
  [INFO] [2017-12-14 11:41:00][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 17 (collect at ModelTest.java:88)
  [INFO] [2017-12-14 11:41:00][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 11:41:00][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 11:41:00][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 17 (MapPartitionsRDD[35] at map at ModelTest.java:88), which has no missing parents
  [INFO] [2017-12-14 11:41:00][org.apache.spark.storage.memory.MemoryStore]Block broadcast_26 stored as values in memory (estimated size 5.4 KB, free 870.4 MB)
  [INFO] [2017-12-14 11:41:00][org.apache.spark.storage.memory.MemoryStore]Block broadcast_26_piece0 stored as bytes in memory (estimated size 3.1 KB, free 870.4 MB)
  [INFO] [2017-12-14 11:41:00][org.apache.spark.storage.BlockManagerInfo]Added broadcast_26_piece0 in memory on 192.168.18.5:34357 (size: 3.1 KB, free: 870.9 MB)
  [INFO] [2017-12-14 11:41:00][org.apache.spark.SparkContext]Created broadcast 26 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 11:41:00][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at map at ModelTest.java:88) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 11:41:00][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 17.0 with 1 tasks
  [INFO] [2017-12-14 11:41:00][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 4884 bytes)
  [INFO] [2017-12-14 11:41:00][org.apache.spark.executor.Executor]Running task 0.0 in stage 17.0 (TID 17)
  [INFO] [2017-12-14 11:41:00][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 11:41:01][org.apache.spark.storage.memory.MemoryStore]Block taskresult_17 stored as bytes in memory (estimated size 11.2 MB, free 859.2 MB)
  [INFO] [2017-12-14 11:41:01][org.apache.spark.storage.BlockManagerInfo]Added taskresult_17 in memory on 192.168.18.5:34357 (size: 11.2 MB, free: 859.7 MB)
  [INFO] [2017-12-14 11:41:01][org.apache.spark.executor.Executor]Finished task 0.0 in stage 17.0 (TID 17). 11728148 bytes result sent via BlockManager)
  [INFO] [2017-12-14 11:41:01][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.18.5:34357 after 46 ms (0 ms spent in bootstraps)
  [INFO] [2017-12-14 11:41:02][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 17.0 (TID 17) in 1825 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 11:41:02][org.apache.spark.scheduler.DAGScheduler]ResultStage 17 (collect at ModelTest.java:88) finished in 1.826 s
  [INFO] [2017-12-14 11:41:02][org.apache.spark.scheduler.DAGScheduler]Job 13 finished: collect at ModelTest.java:88, took 1.850609 s
  [INFO] [2017-12-14 11:41:02][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 17.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 11:41:02][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_17 on 192.168.18.5:34357 in memory (size: 11.2 MB, free: 870.9 MB)
  [INFO] [2017-12-14 11:41:05][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@3347100e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 11:41:05][org.apache.spark.storage.DiskBlockManager]Shutdown hook called
  [INFO] [2017-12-14 11:41:05][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.18.5:4040
  [INFO] [2017-12-14 11:41:05][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
  [INFO] [2017-12-14 11:41:05][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
  [INFO] [2017-12-14 11:41:05][org.apache.spark.storage.BlockManager]BlockManager stopped
  [INFO] [2017-12-14 11:41:05][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
  [INFO] [2017-12-14 11:41:05][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
  [INFO] [2017-12-14 11:41:05][org.apache.spark.util.ShutdownHookManager]Deleting directory /tmp/spark-ef88717d-9450-4d85-9c28-b2ad2ea795d3/userFiles-17a7db22-f8d0-4ec2-906f-409bc4bdce78
  [INFO] [2017-12-14 11:41:05][org.apache.spark.util.ShutdownHookManager]Deleting directory /tmp/spark-ef88717d-9450-4d85-9c28-b2ad2ea795d3
  [INFO] [2017-12-14 11:41:05][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
  [INFO] [2017-12-14 11:41:05][org.apache.spark.SparkContext]Successfully stopped SparkContext
  [INFO] [2017-12-14 12:05:38][org.apache.spark.SparkContext]Running Spark version 2.2.0
  [WARN] [2017-12-14 12:05:39][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-14 12:05:39][org.apache.spark.util.Utils]Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.18.5 instead (on interface eth0)
  [WARN] [2017-12-14 12:05:39][org.apache.spark.util.Utils]Set SPARK_LOCAL_IP if you need to bind to another address
  [INFO] [2017-12-14 12:05:39][org.apache.spark.SparkContext]Submitted application: JavaKMeansExample
  [INFO] [2017-12-14 12:05:39][org.apache.spark.SecurityManager]Changing view acls to: hadoop
  [INFO] [2017-12-14 12:05:39][org.apache.spark.SecurityManager]Changing modify acls to: hadoop
  [INFO] [2017-12-14 12:05:39][org.apache.spark.SecurityManager]Changing view acls groups to: 
  [INFO] [2017-12-14 12:05:39][org.apache.spark.SecurityManager]Changing modify acls groups to: 
  [INFO] [2017-12-14 12:05:39][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
  [INFO] [2017-12-14 12:05:39][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 46368.
  [INFO] [2017-12-14 12:05:39][org.apache.spark.SparkEnv]Registering MapOutputTracker
  [INFO] [2017-12-14 12:05:39][org.apache.spark.SparkEnv]Registering BlockManagerMaster
  [INFO] [2017-12-14 12:05:39][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
  [INFO] [2017-12-14 12:05:39][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
  [INFO] [2017-12-14 12:05:39][org.apache.spark.storage.DiskBlockManager]Created local directory at /tmp/blockmgr-44828bb9-e7fc-48f3-8300-c14303a6dd8a
  [INFO] [2017-12-14 12:05:40][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 882.6 MB
  [INFO] [2017-12-14 12:05:40][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.util.log]Logging initialized @2985ms
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.Server]Started @3164ms
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@4175a2a2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 12:05:40][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b5176f2{/jobs,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f811d00{/jobs/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4089713{/jobs/job,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@b91d8c4{/jobs/job/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a77614d{/stages,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a067c25{/stages/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bde62ff{/stages/stage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@791cbf87{/stages/stage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@754777cd{/stages/pool,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@372ea2bc{/stages/pool/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f08c4b{/storage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7de0c6ae{/storage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@cdc3aae{/storage/rdd,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dcbb60{/storage/rdd/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21526f6c{/environment,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@299266e2{/environment/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66ea1466{/executors,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bffddff{/executors/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@50687efb{/executors/threadDump,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@142eef62{/executors/threadDump/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5990e6c5{/static,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21694e53{/,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@22c86919{/api,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ea1bcdc{/jobs/job/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64712be{/stages/stage/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:40][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.18.5:4040
  [INFO] [2017-12-14 12:05:41][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
  [INFO] [2017-12-14 12:05:41][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43470.
  [INFO] [2017-12-14 12:05:41][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.18.5:43470
  [INFO] [2017-12-14 12:05:41][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
  [INFO] [2017-12-14 12:05:41][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.18.5, 43470, None)
  [INFO] [2017-12-14 12:05:41][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.18.5:43470 with 882.6 MB RAM, BlockManagerId(driver, 192.168.18.5, 43470, None)
  [INFO] [2017-12-14 12:05:41][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.18.5, 43470, None)
  [INFO] [2017-12-14 12:05:41][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.18.5, 43470, None)
  [INFO] [2017-12-14 12:05:41][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1532c619{/metrics/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:42][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 209.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 12:05:42][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 12:05:42][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.18.5:43470 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 12:05:42][org.apache.spark.SparkContext]Created broadcast 0 from textFile at ModelTest.java:44
  [INFO] [2017-12-14 12:05:43][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 12:05:43][org.apache.spark.SparkContext]Starting job: takeSample at KMeans.scala:353
  [INFO] [2017-12-14 12:05:43][org.apache.spark.scheduler.DAGScheduler]Got job 0 (takeSample at KMeans.scala:353) with 1 output partitions
  [INFO] [2017-12-14 12:05:43][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (takeSample at KMeans.scala:353)
  [INFO] [2017-12-14 12:05:43][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 12:05:43][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 12:05:43][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[5] at map at KMeans.scala:224), which has no missing parents
  [INFO] [2017-12-14 12:05:43][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 4.5 KB, free 882.4 MB)
  [INFO] [2017-12-14 12:05:43][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 882.4 MB)
  [INFO] [2017-12-14 12:05:43][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.18.5:43470 (size: 2.5 KB, free: 882.6 MB)
  [INFO] [2017-12-14 12:05:43][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 12:05:43][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at map at KMeans.scala:224) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 12:05:43][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
  [INFO] [2017-12-14 12:05:43][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5195 bytes)
  [INFO] [2017-12-14 12:05:43][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
  [INFO] [2017-12-14 12:05:44][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/output/clean/JuLei/part-r-00000:0+4881501
  [INFO] [2017-12-14 12:05:45][org.apache.spark.storage.memory.MemoryStore]Block rdd_2_0 stored as values in memory (estimated size 11.6 MB, free 870.8 MB)
  [INFO] [2017-12-14 12:05:45][org.apache.spark.storage.BlockManagerInfo]Added rdd_2_0 in memory on 192.168.18.5:43470 (size: 11.6 MB, free: 871.0 MB)
  [INFO] [2017-12-14 12:05:45][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 12:05:45][org.apache.spark.storage.memory.MemoryStore]Block rdd_3_0 stored as values in memory (estimated size 1828.9 KB, free 869.0 MB)
  [INFO] [2017-12-14 12:05:45][org.apache.spark.storage.BlockManagerInfo]Added rdd_3_0 in memory on 192.168.18.5:43470 (size: 1828.9 KB, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:45][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 1662 bytes result sent to driver
  [INFO] [2017-12-14 12:05:45][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 2070 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 12:05:45][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 12:05:45][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (takeSample at KMeans.scala:353) finished in 2.119 s
  [INFO] [2017-12-14 12:05:45][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: takeSample at KMeans.scala:353, took 2.405392 s
  [INFO] [2017-12-14 12:05:46][org.apache.spark.SparkContext]Starting job: takeSample at KMeans.scala:353
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Got job 1 (takeSample at KMeans.scala:353) with 1 output partitions
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (takeSample at KMeans.scala:353)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (PartitionwiseSampledRDD[7] at takeSample at KMeans.scala:353), which has no missing parents
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 5.2 KB, free 869.0 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.9 KB, free 869.0 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.18.5:43470 (size: 2.9 KB, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (PartitionwiseSampledRDD[7] at takeSample at KMeans.scala:353) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5304 bytes)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 12:05:46][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 2176 bytes result sent to driver
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 144 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (takeSample at KMeans.scala:353) finished in 0.144 s
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: takeSample at KMeans.scala:353, took 0.173884 s
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 136.0 B, free 869.0 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 344.0 B, free 869.0 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.18.5:43470 (size: 344.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at KMeans.scala:367
  [INFO] [2017-12-14 12:05:46][org.apache.spark.SparkContext]Starting job: sum at KMeans.scala:373
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Got job 2 (sum at KMeans.scala:373) with 1 output partitions
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (sum at KMeans.scala:373)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[9] at map at KMeans.scala:370), which has no missing parents
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 5.4 KB, free 869.0 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.9 KB, free 869.0 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.18.5:43470 (size: 2.9 KB, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at map at KMeans.scala:370) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 5227 bytes)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [WARN] [2017-12-14 12:05:46][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
  [WARN] [2017-12-14 12:05:46][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.memory.MemoryStore]Block rdd_9_0 stored as values in memory (estimated size 1828.9 KB, free 867.2 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManagerInfo]Added rdd_9_0 in memory on 192.168.18.5:43470 (size: 1828.9 KB, free: 867.4 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 1531 bytes result sent to driver
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 330 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (sum at KMeans.scala:373) finished in 0.327 s
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: sum at KMeans.scala:373, took 0.350137 s
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 12:05:46][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 6 from persistence list
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManager]Removing RDD 6
  [INFO] [2017-12-14 12:05:46][org.apache.spark.SparkContext]Starting job: collect at KMeans.scala:381
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Got job 3 (collect at KMeans.scala:381) with 1 output partitions
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (collect at KMeans.scala:381)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[11] at mapPartitionsWithIndex at KMeans.scala:378), which has no missing parents
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 6.0 KB, free 867.2 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.2 KB, free 867.2 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.18.5:43470 (size: 3.2 KB, free: 867.4 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at mapPartitionsWithIndex at KMeans.scala:378) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 5259 bytes)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManager]Found block rdd_9_0 locally
  [INFO] [2017-12-14 12:05:46][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 1760 bytes result sent to driver
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 135 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (collect at KMeans.scala:381) finished in 0.135 s
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: collect at KMeans.scala:381, took 0.163300 s
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 1280.0 B, free 867.2 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 798.0 B, free 867.2 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.18.5:43470 (size: 798.0 B, free: 867.4 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at KMeans.scala:367
  [INFO] [2017-12-14 12:05:46][org.apache.spark.SparkContext]Starting job: sum at KMeans.scala:373
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Got job 4 (sum at KMeans.scala:373) with 1 output partitions
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (sum at KMeans.scala:373)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[13] at map at KMeans.scala:370), which has no missing parents
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 5.6 KB, free 867.2 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.0 KB, free 867.2 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.18.5:43470 (size: 3.0 KB, free: 867.4 MB)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at map at KMeans.scala:370) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
  [INFO] [2017-12-14 12:05:46][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 5259 bytes)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 12:05:46][org.apache.spark.storage.BlockManager]Found block rdd_9_0 locally
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.18.5:43470 in memory (size: 2.5 KB, free: 867.4 MB)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.18.5:43470 in memory (size: 3.2 KB, free: 867.4 MB)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.18.5:43470 in memory (size: 2.9 KB, free: 867.4 MB)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.18.5:43470 in memory (size: 2.9 KB, free: 867.4 MB)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.memory.MemoryStore]Block rdd_13_0 stored as values in memory (estimated size 1828.9 KB, free 865.4 MB)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.BlockManagerInfo]Added rdd_13_0 in memory on 192.168.18.5:43470 (size: 1828.9 KB, free: 865.6 MB)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 1574 bytes result sent to driver
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 486 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (sum at KMeans.scala:373) finished in 0.488 s
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: sum at KMeans.scala:373, took 0.513373 s
  [INFO] [2017-12-14 12:05:47][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.BlockManager]Removing RDD 9
  [INFO] [2017-12-14 12:05:47][org.apache.spark.SparkContext]Starting job: collect at KMeans.scala:381
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.DAGScheduler]Got job 5 (collect at KMeans.scala:381) with 1 output partitions
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (collect at KMeans.scala:381)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[15] at mapPartitionsWithIndex at KMeans.scala:378), which has no missing parents
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 6.3 KB, free 867.2 MB)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.3 KB, free 867.2 MB)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.18.5:43470 (size: 3.3 KB, free: 867.4 MB)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[15] at mapPartitionsWithIndex at KMeans.scala:378) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 5291 bytes)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.BlockManager]Found block rdd_13_0 locally
  [INFO] [2017-12-14 12:05:47][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 1622 bytes result sent to driver
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 46 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (collect at KMeans.scala:381) finished in 0.047 s
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: collect at KMeans.scala:381, took 0.063741 s
  [INFO] [2017-12-14 12:05:47][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.BlockManager]Removing RDD 13
  [INFO] [2017-12-14 12:05:47][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(3) (from destroy at KMeans.scala:388)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.18.5:43470 in memory (size: 344.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(6) (from destroy at KMeans.scala:388)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 192.168.18.5:43470 in memory (size: 798.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 2.3 KB, free 869.0 MB)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 1236.0 B, free 869.0 MB)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.18.5:43470 (size: 1236.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at KMeans.scala:398
  [INFO] [2017-12-14 12:05:47][org.apache.spark.SparkContext]Starting job: countByValue at KMeans.scala:399
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.DAGScheduler]Registering RDD 18 (countByValue at KMeans.scala:399)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.DAGScheduler]Got job 6 (countByValue at KMeans.scala:399) with 1 output partitions
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (countByValue at KMeans.scala:399)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 6)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 6)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 6 (MapPartitionsRDD[18] at countByValue at KMeans.scala:399), which has no missing parents
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 6.6 KB, free 869.0 MB)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.6 KB, free 869.0 MB)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 192.168.18.5:43470 (size: 3.6 KB, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[18] at countByValue at KMeans.scala:399) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
  [INFO] [2017-12-14 12:05:47][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 5184 bytes)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 12:05:47][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 12:05:48][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 1153 bytes result sent to driver
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 356 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 6 (countByValue at KMeans.scala:399) finished in 0.358 s
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]running: Set()
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 7)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]failed: Set()
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (ShuffledRDD[19] at countByValue at KMeans.scala:399), which has no missing parents
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.2 KB, free 869.0 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 1971.0 B, free 868.9 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 192.168.18.5:43470 (size: 1971.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (ShuffledRDD[19] at countByValue at KMeans.scala:399) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4621 bytes)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 5 ms
  [INFO] [2017-12-14 12:05:48][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 2202 bytes result sent to driver
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 58 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (countByValue at KMeans.scala:399) finished in 0.059 s
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: countByValue at KMeans.scala:399, took 0.726532 s
  [INFO] [2017-12-14 12:05:48][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(9) (from destroy at KMeans.scala:401)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_9_piece0 on 192.168.18.5:43470 in memory (size: 1236.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.mllib.clustering.LocalKMeans]Local KMeans++ converged in 3 iterations.
  [INFO] [2017-12-14 12:05:48][org.apache.spark.mllib.clustering.KMeans]Initialization with k-means|| took 5.789 seconds.
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 496.0 B, free 869.0 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 449.0 B, free 869.0 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 192.168.18.5:43470 (size: 449.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at KMeans.scala:273
  [INFO] [2017-12-14 12:05:48][org.apache.spark.SparkContext]Starting job: collectAsMap at KMeans.scala:295
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Registering RDD 20 (mapPartitions at KMeans.scala:276)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Got job 7 (collectAsMap at KMeans.scala:295) with 1 output partitions
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (collectAsMap at KMeans.scala:295)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 8)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 8)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 8 (MapPartitionsRDD[20] at mapPartitions at KMeans.scala:276), which has no missing parents
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 6.2 KB, free 868.9 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.4 KB, free 868.9 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 192.168.18.5:43470 (size: 3.4 KB, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[20] at mapPartitions at KMeans.scala:276) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 5184 bytes)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 12:05:48][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 1223 bytes result sent to driver
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 179 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 8 (mapPartitions at KMeans.scala:276) finished in 0.169 s
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]running: Set()
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 9)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]failed: Set()
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (ShuffledRDD[21] at reduceByKey at KMeans.scala:292), which has no missing parents
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 2.8 KB, free 868.9 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 1654.0 B, free 868.9 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 192.168.18.5:43470 (size: 1654.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (ShuffledRDD[21] at reduceByKey at KMeans.scala:292) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 4621 bytes)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
  [INFO] [2017-12-14 12:05:48][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 1787 bytes result sent to driver
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 8 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (collectAsMap at KMeans.scala:295) finished in 0.013 s
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: collectAsMap at KMeans.scala:295, took 0.223044 s
  [INFO] [2017-12-14 12:05:48][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(12) (from destroy at KMeans.scala:297)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_12_piece0 on 192.168.18.5:43470 in memory (size: 449.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 496.0 B, free 868.9 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 457.0 B, free 868.9 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 192.168.18.5:43470 (size: 457.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at KMeans.scala:273
  [INFO] [2017-12-14 12:05:48][org.apache.spark.SparkContext]Starting job: collectAsMap at KMeans.scala:295
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Registering RDD 22 (mapPartitions at KMeans.scala:276)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Got job 8 (collectAsMap at KMeans.scala:295) with 1 output partitions
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (collectAsMap at KMeans.scala:295)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 10)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 10)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 10 (MapPartitionsRDD[22] at mapPartitions at KMeans.scala:276), which has no missing parents
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 6.2 KB, free 868.9 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.4 KB, free 868.9 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 192.168.18.5:43470 (size: 3.4 KB, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[22] at mapPartitions at KMeans.scala:276) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 5184 bytes)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_13_piece0 on 192.168.18.5:43470 in memory (size: 3.4 KB, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.ContextCleaner]Cleaned accumulator 216
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManager]Removing RDD 13
  [INFO] [2017-12-14 12:05:48][org.apache.spark.ContextCleaner]Cleaned RDD 13
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_8_piece0 on 192.168.18.5:43470 in memory (size: 3.3 KB, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_10_piece0 on 192.168.18.5:43470 in memory (size: 3.6 KB, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.ContextCleaner]Cleaned shuffle 1
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManager]Removing RDD 9
  [INFO] [2017-12-14 12:05:48][org.apache.spark.ContextCleaner]Cleaned RDD 9
  [INFO] [2017-12-14 12:05:48][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 1309 bytes result sent to driver
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 192.168.18.5:43470 in memory (size: 3.0 KB, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 118 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 10 (mapPartitions at KMeans.scala:276) finished in 0.119 s
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]running: Set()
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 11)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]failed: Set()
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (ShuffledRDD[23] at reduceByKey at KMeans.scala:292), which has no missing parents
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17 stored as values in memory (estimated size 2.8 KB, free 869.0 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17_piece0 stored as bytes in memory (estimated size 1659.0 B, free 869.0 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManagerInfo]Added broadcast_17_piece0 in memory on 192.168.18.5:43470 (size: 1659.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.SparkContext]Created broadcast 17 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (ShuffledRDD[23] at reduceByKey at KMeans.scala:292) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, ANY, 4621 bytes)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_11_piece0 on 192.168.18.5:43470 in memory (size: 1971.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_14_piece0 on 192.168.18.5:43470 in memory (size: 1654.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.ContextCleaner]Cleaned shuffle 0
  [INFO] [2017-12-14 12:05:48][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 1787 bytes result sent to driver
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 10 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (collectAsMap at KMeans.scala:295) finished in 0.011 s
  [INFO] [2017-12-14 12:05:48][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: collectAsMap at KMeans.scala:295, took 0.157784 s
  [INFO] [2017-12-14 12:05:48][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(15) (from destroy at KMeans.scala:297)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_15_piece0 on 192.168.18.5:43470 in memory (size: 457.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 12:05:48][org.apache.spark.mllib.clustering.KMeans]Iterations took 0.493 seconds.
  [INFO] [2017-12-14 12:05:48][org.apache.spark.mllib.clustering.KMeans]KMeans converged in 2 iterations.
  [INFO] [2017-12-14 12:05:48][org.apache.spark.mllib.clustering.KMeans]The cost is 309319.86967559403.
  [INFO] [2017-12-14 12:05:48][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.BlockManager]Removing RDD 3
  [INFO] [2017-12-14 12:05:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18 stored as values in memory (estimated size 209.1 KB, free 870.6 MB)
  [INFO] [2017-12-14 12:05:49][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18_piece0 stored as bytes in memory (estimated size 20.1 KB, free 870.5 MB)
  [INFO] [2017-12-14 12:05:49][org.apache.spark.storage.BlockManagerInfo]Added broadcast_18_piece0 in memory on 192.168.18.5:43470 (size: 20.1 KB, free: 871.0 MB)
  [INFO] [2017-12-14 12:05:49][org.apache.spark.SparkContext]Created broadcast 18 from textFile at modelSaveLoad.scala:129
  [INFO] [2017-12-14 12:05:49][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 12:05:49][org.apache.spark.SparkContext]Starting job: first at modelSaveLoad.scala:129
  [INFO] [2017-12-14 12:05:49][org.apache.spark.scheduler.DAGScheduler]Got job 9 (first at modelSaveLoad.scala:129) with 1 output partitions
  [INFO] [2017-12-14 12:05:49][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 12 (first at modelSaveLoad.scala:129)
  [INFO] [2017-12-14 12:05:49][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 12:05:49][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 12:05:49][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 12 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[25] at textFile at modelSaveLoad.scala:129), which has no missing parents
  [INFO] [2017-12-14 12:05:49][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19 stored as values in memory (estimated size 3.3 KB, free 870.5 MB)
  [INFO] [2017-12-14 12:05:49][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19_piece0 stored as bytes in memory (estimated size 1981.0 B, free 870.5 MB)
  [INFO] [2017-12-14 12:05:49][org.apache.spark.storage.BlockManagerInfo]Added broadcast_19_piece0 in memory on 192.168.18.5:43470 (size: 1981.0 B, free: 870.9 MB)
  [INFO] [2017-12-14 12:05:49][org.apache.spark.SparkContext]Created broadcast 19 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 12:05:49][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 12 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[25] at textFile at modelSaveLoad.scala:129) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 12:05:49][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
  [INFO] [2017-12-14 12:05:49][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, ANY, 4926 bytes)
  [INFO] [2017-12-14 12:05:49][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
  [INFO] [2017-12-14 12:05:49][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata/part-00000:0+80
  [INFO] [2017-12-14 12:05:49][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 876 bytes result sent to driver
  [INFO] [2017-12-14 12:05:49][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 27 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 12:05:49][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 12:05:49][org.apache.spark.scheduler.DAGScheduler]ResultStage 12 (first at modelSaveLoad.scala:129) finished in 0.030 s
  [INFO] [2017-12-14 12:05:49][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: first at modelSaveLoad.scala:129, took 0.043184 s
  [INFO] [2017-12-14 12:05:49][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/').
  [INFO] [2017-12-14 12:05:49][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/'.
  [INFO] [2017-12-14 12:05:49][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4998e74b{/SQL,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:49][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28279a49{/SQL/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:49][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@566e142{/SQL/execution,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:49][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1f992a3a{/SQL/execution/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:49][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f4596d0{/static/sql,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 12:05:49][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
  [INFO] [2017-12-14 12:05:50][org.apache.spark.SparkContext]Starting job: parquet at KMeansModel.scala:142
  [INFO] [2017-12-14 12:05:50][org.apache.spark.scheduler.DAGScheduler]Got job 10 (parquet at KMeansModel.scala:142) with 1 output partitions
  [INFO] [2017-12-14 12:05:50][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (parquet at KMeansModel.scala:142)
  [INFO] [2017-12-14 12:05:50][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 12:05:50][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 12:05:50][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (MapPartitionsRDD[27] at parquet at KMeansModel.scala:142), which has no missing parents
  [INFO] [2017-12-14 12:05:50][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20 stored as values in memory (estimated size 62.1 KB, free 870.5 MB)
  [INFO] [2017-12-14 12:05:50][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20_piece0 stored as bytes in memory (estimated size 21.8 KB, free 870.4 MB)
  [INFO] [2017-12-14 12:05:50][org.apache.spark.storage.BlockManagerInfo]Added broadcast_20_piece0 in memory on 192.168.18.5:43470 (size: 21.8 KB, free: 870.9 MB)
  [INFO] [2017-12-14 12:05:50][org.apache.spark.SparkContext]Created broadcast 20 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 12:05:50][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at parquet at KMeansModel.scala:142) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 12:05:50][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
  [INFO] [2017-12-14 12:05:50][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 5081 bytes)
  [INFO] [2017-12-14 12:05:50][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 13)
  [INFO] [2017-12-14 12:05:50][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_18_piece0 on 192.168.18.5:43470 in memory (size: 20.1 KB, free: 870.9 MB)
  [INFO] [2017-12-14 12:05:50][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_17_piece0 on 192.168.18.5:43470 in memory (size: 1659.0 B, free: 870.9 MB)
  [INFO] [2017-12-14 12:05:50][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_19_piece0 on 192.168.18.5:43470 in memory (size: 1981.0 B, free: 871.0 MB)
  [INFO] [2017-12-14 12:05:50][org.apache.spark.ContextCleaner]Cleaned accumulator 265
  [INFO] [2017-12-14 12:05:50][org.apache.spark.storage.BlockManager]Removing RDD 3
  [INFO] [2017-12-14 12:05:50][org.apache.spark.ContextCleaner]Cleaned RDD 3
  [INFO] [2017-12-14 12:05:50][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_16_piece0 on 192.168.18.5:43470 in memory (size: 3.4 KB, free: 871.0 MB)
  [INFO] [2017-12-14 12:05:50][org.apache.spark.ContextCleaner]Cleaned shuffle 2
  [INFO] [2017-12-14 12:05:51][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 13). 1687 bytes result sent to driver
  [INFO] [2017-12-14 12:05:51][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 13) in 831 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 12:05:51][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 12:05:51][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (parquet at KMeansModel.scala:142) finished in 0.827 s
  [INFO] [2017-12-14 12:05:51][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: parquet at KMeansModel.scala:142, took 0.931955 s
  [INFO] [2017-12-14 12:05:53][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
  [INFO] [2017-12-14 12:05:53][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: 
  [INFO] [2017-12-14 12:05:53][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<id: int, point: vector>
  [INFO] [2017-12-14 12:05:53][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
  [INFO] [2017-12-14 12:05:54][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 345.658512 ms
  [INFO] [2017-12-14 12:05:54][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21 stored as values in memory (estimated size 222.5 KB, free 870.5 MB)
  [INFO] [2017-12-14 12:05:54][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21_piece0 stored as bytes in memory (estimated size 21.2 KB, free 870.5 MB)
  [INFO] [2017-12-14 12:05:54][org.apache.spark.storage.BlockManagerInfo]Added broadcast_21_piece0 in memory on 192.168.18.5:43470 (size: 21.2 KB, free: 870.9 MB)
  [INFO] [2017-12-14 12:05:54][org.apache.spark.SparkContext]Created broadcast 21 from rdd at KMeansModel.scala:144
  [INFO] [2017-12-14 12:05:54][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 4195846 bytes, open cost is considered as scanning 4194304 bytes.
  [INFO] [2017-12-14 12:05:54][org.apache.spark.SparkContext]Starting job: collect at KMeansModel.scala:144
  [INFO] [2017-12-14 12:05:54][org.apache.spark.scheduler.DAGScheduler]Got job 11 (collect at KMeansModel.scala:144) with 1 output partitions
  [INFO] [2017-12-14 12:05:54][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 14 (collect at KMeansModel.scala:144)
  [INFO] [2017-12-14 12:05:54][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 12:05:54][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 12:05:54][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 14 (MapPartitionsRDD[32] at map at KMeansModel.scala:144), which has no missing parents
  [INFO] [2017-12-14 12:05:54][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22 stored as values in memory (estimated size 15.9 KB, free 870.4 MB)
  [INFO] [2017-12-14 12:05:54][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22_piece0 stored as bytes in memory (estimated size 7.3 KB, free 870.4 MB)
  [INFO] [2017-12-14 12:05:54][org.apache.spark.storage.BlockManagerInfo]Added broadcast_22_piece0 in memory on 192.168.18.5:43470 (size: 7.3 KB, free: 870.9 MB)
  [INFO] [2017-12-14 12:05:54][org.apache.spark.SparkContext]Created broadcast 22 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 12:05:54][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[32] at map at KMeansModel.scala:144) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 12:05:54][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
  [INFO] [2017-12-14 12:05:54][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, ANY, 5392 bytes)
  [INFO] [2017-12-14 12:05:54][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 14)
  [INFO] [2017-12-14 12:05:54][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 26.121514 ms
  [INFO] [2017-12-14 12:05:54][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/data/part-00000-0d1f8488-b668-4e2d-b619-3e79f8cfb51c.snappy.parquet, range: 0-1542, partition values: [empty row]
  [INFO] [2017-12-14 12:05:54][org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport]Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
  [INFO] [2017-12-14 12:05:54][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 40.725261 ms
  [INFO] [2017-12-14 12:05:54][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 30.648463 ms
  [INFO] [2017-12-14 12:05:54][org.apache.parquet.hadoop.InternalParquetRecordReader]RecordReader initialized will read a total of 5 records.
  [INFO] [2017-12-14 12:05:54][org.apache.parquet.hadoop.InternalParquetRecordReader]at row 0. reading next block
  [INFO] [2017-12-14 12:05:54][org.apache.hadoop.io.compress.CodecPool]Got brand-new decompressor [.snappy]
  [INFO] [2017-12-14 12:05:54][org.apache.parquet.hadoop.InternalParquetRecordReader]block read in memory in 50 ms. row count = 5
  [INFO] [2017-12-14 12:05:55][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 14). 1884 bytes result sent to driver
  [INFO] [2017-12-14 12:05:55][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 14) in 583 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 12:05:55][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 12:05:55][org.apache.spark.scheduler.DAGScheduler]ResultStage 14 (collect at KMeansModel.scala:144) finished in 0.584 s
  [INFO] [2017-12-14 12:05:55][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: collect at KMeansModel.scala:144, took 0.609690 s
  [INFO] [2017-12-14 12:05:55][org.apache.spark.SparkContext]Starting job: collect at ModelTest.java:91
  [INFO] [2017-12-14 12:05:55][org.apache.spark.scheduler.DAGScheduler]Got job 12 (collect at ModelTest.java:91) with 1 output partitions
  [INFO] [2017-12-14 12:05:55][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (collect at ModelTest.java:91)
  [INFO] [2017-12-14 12:05:55][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 12:05:55][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 12:05:55][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (MapPartitionsRDD[33] at map at ModelTest.java:91), which has no missing parents
  [INFO] [2017-12-14 12:05:55][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23 stored as values in memory (estimated size 5.4 KB, free 870.4 MB)
  [INFO] [2017-12-14 12:05:55][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23_piece0 stored as bytes in memory (estimated size 3.1 KB, free 870.4 MB)
  [INFO] [2017-12-14 12:05:55][org.apache.spark.storage.BlockManagerInfo]Added broadcast_23_piece0 in memory on 192.168.18.5:43470 (size: 3.1 KB, free: 870.9 MB)
  [INFO] [2017-12-14 12:05:55][org.apache.spark.SparkContext]Created broadcast 23 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 12:05:55][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[33] at map at ModelTest.java:91) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 12:05:55][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 1 tasks
  [INFO] [2017-12-14 12:05:55][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 4884 bytes)
  [INFO] [2017-12-14 12:05:55][org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 15)
  [INFO] [2017-12-14 12:05:55][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 12:05:55][org.apache.spark.ContextCleaner]Cleaned accumulator 365
  [INFO] [2017-12-14 12:05:55][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_22_piece0 on 192.168.18.5:43470 in memory (size: 7.3 KB, free: 870.9 MB)
  [INFO] [2017-12-14 12:05:55][org.apache.spark.ContextCleaner]Cleaned accumulator 362
  [INFO] [2017-12-14 12:05:55][org.apache.spark.ContextCleaner]Cleaned accumulator 366
  [INFO] [2017-12-14 12:05:55][org.apache.spark.ContextCleaner]Cleaned accumulator 363
  [INFO] [2017-12-14 12:05:55][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_21_piece0 on 192.168.18.5:43470 in memory (size: 21.2 KB, free: 871.0 MB)
  [INFO] [2017-12-14 12:05:55][org.apache.spark.ContextCleaner]Cleaned accumulator 364
  [INFO] [2017-12-14 12:05:56][org.apache.spark.storage.memory.MemoryStore]Block taskresult_15 stored as bytes in memory (estimated size 10.7 MB, free 859.9 MB)
  [INFO] [2017-12-14 12:05:56][org.apache.spark.storage.BlockManagerInfo]Added taskresult_15 in memory on 192.168.18.5:43470 (size: 10.7 MB, free: 860.2 MB)
  [INFO] [2017-12-14 12:05:56][org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 15). 11257671 bytes result sent via BlockManager)
  [INFO] [2017-12-14 12:05:56][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.18.5:43470 after 35 ms (0 ms spent in bootstraps)
  [INFO] [2017-12-14 12:05:56][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_15 on 192.168.18.5:43470 in memory (size: 10.7 MB, free: 871.0 MB)
  [INFO] [2017-12-14 12:05:56][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 15) in 1461 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 12:05:56][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 12:05:56][org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (collect at ModelTest.java:91) finished in 1.463 s
  [INFO] [2017-12-14 12:05:56][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: collect at ModelTest.java:91, took 1.473346 s
  [INFO] [2017-12-14 12:05:56][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@4175a2a2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 12:05:56][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.18.5:4040
  [INFO] [2017-12-14 12:05:56][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
  [INFO] [2017-12-14 12:05:56][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
  [INFO] [2017-12-14 12:05:56][org.apache.spark.storage.BlockManager]BlockManager stopped
  [INFO] [2017-12-14 12:05:56][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
  [INFO] [2017-12-14 12:05:56][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
  [INFO] [2017-12-14 12:05:56][org.apache.spark.SparkContext]Successfully stopped SparkContext
  [INFO] [2017-12-14 12:05:56][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
  [INFO] [2017-12-14 12:05:56][org.apache.spark.util.ShutdownHookManager]Deleting directory /tmp/spark-7016e25d-7051-4b9e-8260-a4c024fe7e6b
  [INFO] [2017-12-14 14:15:42][org.apache.spark.SparkContext]Running Spark version 2.2.0
  [WARN] [2017-12-14 14:15:43][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-14 14:15:43][org.apache.spark.util.Utils]Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.18.5 instead (on interface eth0)
  [WARN] [2017-12-14 14:15:43][org.apache.spark.util.Utils]Set SPARK_LOCAL_IP if you need to bind to another address
  [INFO] [2017-12-14 14:15:43][org.apache.spark.SparkContext]Submitted application: JavaKMeansExample
  [INFO] [2017-12-14 14:15:43][org.apache.spark.SecurityManager]Changing view acls to: hadoop
  [INFO] [2017-12-14 14:15:43][org.apache.spark.SecurityManager]Changing modify acls to: hadoop
  [INFO] [2017-12-14 14:15:43][org.apache.spark.SecurityManager]Changing view acls groups to: 
  [INFO] [2017-12-14 14:15:43][org.apache.spark.SecurityManager]Changing modify acls groups to: 
  [INFO] [2017-12-14 14:15:43][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
  [INFO] [2017-12-14 14:15:44][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 44110.
  [INFO] [2017-12-14 14:15:44][org.apache.spark.SparkEnv]Registering MapOutputTracker
  [INFO] [2017-12-14 14:15:44][org.apache.spark.SparkEnv]Registering BlockManagerMaster
  [INFO] [2017-12-14 14:15:44][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
  [INFO] [2017-12-14 14:15:44][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
  [INFO] [2017-12-14 14:15:44][org.apache.spark.storage.DiskBlockManager]Created local directory at /tmp/blockmgr-1185cb49-7f56-4d04-a518-87c11fd242eb
  [INFO] [2017-12-14 14:15:44][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 882.6 MB
  [INFO] [2017-12-14 14:15:44][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.util.log]Logging initialized @2548ms
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.Server]Started @2663ms
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@4da6bb31{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:15:44][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b5176f2{/jobs,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f811d00{/jobs/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4089713{/jobs/job,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@b91d8c4{/jobs/job/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a77614d{/stages,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a067c25{/stages/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bde62ff{/stages/stage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@791cbf87{/stages/stage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@754777cd{/stages/pool,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@372ea2bc{/stages/pool/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f08c4b{/storage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7de0c6ae{/storage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@cdc3aae{/storage/rdd,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dcbb60{/storage/rdd/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21526f6c{/environment,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@299266e2{/environment/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66ea1466{/executors,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bffddff{/executors/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@50687efb{/executors/threadDump,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@142eef62{/executors/threadDump/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5990e6c5{/static,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21694e53{/,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@22c86919{/api,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ea1bcdc{/jobs/job/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64712be{/stages/stage/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:44][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:15:44][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
  [INFO] [2017-12-14 14:15:44][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33231.
  [INFO] [2017-12-14 14:15:44][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.18.5:33231
  [INFO] [2017-12-14 14:15:44][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
  [INFO] [2017-12-14 14:15:44][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.18.5, 33231, None)
  [INFO] [2017-12-14 14:15:44][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.18.5:33231 with 882.6 MB RAM, BlockManagerId(driver, 192.168.18.5, 33231, None)
  [INFO] [2017-12-14 14:15:44][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.18.5, 33231, None)
  [INFO] [2017-12-14 14:15:44][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.18.5, 33231, None)
  [INFO] [2017-12-14 14:15:45][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1532c619{/metrics/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:45][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 209.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:15:45][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:15:45][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.18.5:33231 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:15:45][org.apache.spark.SparkContext]Created broadcast 0 from textFile at ModelTest.java:44
  [INFO] [2017-12-14 14:15:47][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:15:47][org.apache.spark.SparkContext]Starting job: takeSample at KMeans.scala:353
  [INFO] [2017-12-14 14:15:47][org.apache.spark.scheduler.DAGScheduler]Got job 0 (takeSample at KMeans.scala:353) with 1 output partitions
  [INFO] [2017-12-14 14:15:47][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (takeSample at KMeans.scala:353)
  [INFO] [2017-12-14 14:15:47][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:15:47][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:15:47][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[5] at map at KMeans.scala:224), which has no missing parents
  [INFO] [2017-12-14 14:15:47][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 4.5 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:15:47][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:15:47][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.18.5:33231 (size: 2.5 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:15:47][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:47][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at map at KMeans.scala:224) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:47][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
  [INFO] [2017-12-14 14:15:47][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5195 bytes)
  [INFO] [2017-12-14 14:15:47][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
  [INFO] [2017-12-14 14:15:47][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/output/clean/JuLei/part-r-00000:0+4881501
  [INFO] [2017-12-14 14:15:48][org.apache.spark.storage.memory.MemoryStore]Block rdd_2_0 stored as values in memory (estimated size 11.6 MB, free 870.8 MB)
  [INFO] [2017-12-14 14:15:48][org.apache.spark.storage.BlockManagerInfo]Added rdd_2_0 in memory on 192.168.18.5:33231 (size: 11.6 MB, free: 871.0 MB)
  [INFO] [2017-12-14 14:15:48][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.memory.MemoryStore]Block rdd_3_0 stored as values in memory (estimated size 1828.9 KB, free 869.0 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManagerInfo]Added rdd_3_0 in memory on 192.168.18.5:33231 (size: 1828.9 KB, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 1662 bytes result sent to driver
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 1809 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (takeSample at KMeans.scala:353) finished in 1.840 s
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: takeSample at KMeans.scala:353, took 2.083756 s
  [INFO] [2017-12-14 14:15:49][org.apache.spark.SparkContext]Starting job: takeSample at KMeans.scala:353
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Got job 1 (takeSample at KMeans.scala:353) with 1 output partitions
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (takeSample at KMeans.scala:353)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (PartitionwiseSampledRDD[7] at takeSample at KMeans.scala:353), which has no missing parents
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 5.2 KB, free 869.0 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.9 KB, free 869.0 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.18.5:33231 (size: 2.9 KB, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (PartitionwiseSampledRDD[7] at takeSample at KMeans.scala:353) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5304 bytes)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 14:15:49][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 1943 bytes result sent to driver
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 157 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (takeSample at KMeans.scala:353) finished in 0.158 s
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: takeSample at KMeans.scala:353, took 0.199980 s
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 136.0 B, free 869.0 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 344.0 B, free 869.0 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.18.5:33231 (size: 344.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at KMeans.scala:367
  [INFO] [2017-12-14 14:15:49][org.apache.spark.SparkContext]Starting job: sum at KMeans.scala:373
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Got job 2 (sum at KMeans.scala:373) with 1 output partitions
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (sum at KMeans.scala:373)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[9] at map at KMeans.scala:370), which has no missing parents
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 5.4 KB, free 869.0 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.9 KB, free 869.0 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.18.5:33231 (size: 2.9 KB, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at map at KMeans.scala:370) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 5227 bytes)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [WARN] [2017-12-14 14:15:49][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
  [WARN] [2017-12-14 14:15:49][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.memory.MemoryStore]Block rdd_9_0 stored as values in memory (estimated size 1828.9 KB, free 867.2 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManagerInfo]Added rdd_9_0 in memory on 192.168.18.5:33231 (size: 1828.9 KB, free: 867.4 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 1574 bytes result sent to driver
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 236 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (sum at KMeans.scala:373) finished in 0.237 s
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: sum at KMeans.scala:373, took 0.252367 s
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:49][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 6 from persistence list
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManager]Removing RDD 6
  [INFO] [2017-12-14 14:15:49][org.apache.spark.SparkContext]Starting job: collect at KMeans.scala:381
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Got job 3 (collect at KMeans.scala:381) with 1 output partitions
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (collect at KMeans.scala:381)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[11] at mapPartitionsWithIndex at KMeans.scala:378), which has no missing parents
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 6.0 KB, free 867.2 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.2 KB, free 867.2 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.18.5:33231 (size: 3.2 KB, free: 867.4 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at mapPartitionsWithIndex at KMeans.scala:378) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 5259 bytes)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManager]Found block rdd_9_0 locally
  [INFO] [2017-12-14 14:15:49][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 1576 bytes result sent to driver
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 119 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (collect at KMeans.scala:381) finished in 0.120 s
  [INFO] [2017-12-14 14:15:49][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: collect at KMeans.scala:381, took 0.138126 s
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 976.0 B, free 867.2 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.18.5:33231 in memory (size: 2.9 KB, free: 867.4 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 689.0 B, free 867.2 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.18.5:33231 (size: 689.0 B, free: 867.4 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.18.5:33231 in memory (size: 2.5 KB, free: 867.4 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.18.5:33231 in memory (size: 2.9 KB, free: 867.4 MB)
  [INFO] [2017-12-14 14:15:49][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at KMeans.scala:367
  [INFO] [2017-12-14 14:15:50][org.apache.spark.SparkContext]Starting job: sum at KMeans.scala:373
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Got job 4 (sum at KMeans.scala:373) with 1 output partitions
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (sum at KMeans.scala:373)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (MapPartitionsRDD[13] at map at KMeans.scala:370), which has no missing parents
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 5.6 KB, free 867.2 MB)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.0 KB, free 867.2 MB)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.18.5:33231 (size: 3.0 KB, free: 867.4 MB)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at map at KMeans.scala:370) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 5259 bytes)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 4)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.BlockManager]Found block rdd_9_0 locally
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.memory.MemoryStore]Block rdd_13_0 stored as values in memory (estimated size 1828.9 KB, free 865.4 MB)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.BlockManagerInfo]Added rdd_13_0 in memory on 192.168.18.5:33231 (size: 1828.9 KB, free: 865.6 MB)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 4). 1574 bytes result sent to driver
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 4) in 379 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (sum at KMeans.scala:373) finished in 0.380 s
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: sum at KMeans.scala:373, took 0.399808 s
  [INFO] [2017-12-14 14:15:50][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 9 from persistence list
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.BlockManager]Removing RDD 9
  [INFO] [2017-12-14 14:15:50][org.apache.spark.SparkContext]Starting job: collect at KMeans.scala:381
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Got job 5 (collect at KMeans.scala:381) with 1 output partitions
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (collect at KMeans.scala:381)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[15] at mapPartitionsWithIndex at KMeans.scala:378), which has no missing parents
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 6.3 KB, free 867.2 MB)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.3 KB, free 867.2 MB)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.18.5:33231 (size: 3.3 KB, free: 867.4 MB)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[15] at mapPartitionsWithIndex at KMeans.scala:378) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 5291 bytes)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 5)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.BlockManager]Found block rdd_13_0 locally
  [INFO] [2017-12-14 14:15:50][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 5). 1300 bytes result sent to driver
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 5) in 78 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (collect at KMeans.scala:381) finished in 0.079 s
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: collect at KMeans.scala:381, took 0.202284 s
  [INFO] [2017-12-14 14:15:50][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.BlockManager]Removing RDD 13
  [INFO] [2017-12-14 14:15:50][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(3) (from destroy at KMeans.scala:388)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.18.5:33231 in memory (size: 344.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(6) (from destroy at KMeans.scala:388)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 192.168.18.5:33231 in memory (size: 689.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 1512.0 B, free 869.0 MB)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 928.0 B, free 869.0 MB)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.18.5:33231 (size: 928.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at KMeans.scala:398
  [INFO] [2017-12-14 14:15:50][org.apache.spark.SparkContext]Starting job: countByValue at KMeans.scala:399
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Registering RDD 18 (countByValue at KMeans.scala:399)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Got job 6 (countByValue at KMeans.scala:399) with 1 output partitions
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (countByValue at KMeans.scala:399)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 6)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 6)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 6 (MapPartitionsRDD[18] at countByValue at KMeans.scala:399), which has no missing parents
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 6.6 KB, free 869.0 MB)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.6 KB, free 868.9 MB)
  [INFO] [2017-12-14 14:15:50][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 192.168.18.5:33231 (size: 3.6 KB, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[18] at countByValue at KMeans.scala:399) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 5184 bytes)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 6)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.18.5:33231 in memory (size: 3.2 KB, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 192.168.18.5:33231 in memory (size: 3.0 KB, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_8_piece0 on 192.168.18.5:33231 in memory (size: 3.3 KB, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 6). 1196 bytes result sent to driver
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 6) in 369 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 6 (countByValue at KMeans.scala:399) finished in 0.371 s
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]running: Set()
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 7)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]failed: Set()
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (ShuffledRDD[19] at countByValue at KMeans.scala:399), which has no missing parents
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.2 KB, free 869.0 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 1971.0 B, free 869.0 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 192.168.18.5:33231 (size: 1971.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (ShuffledRDD[19] at countByValue at KMeans.scala:399) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 4621 bytes)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 7)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 5 ms
  [INFO] [2017-12-14 14:15:51][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 7). 1827 bytes result sent to driver
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 7) in 63 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (countByValue at KMeans.scala:399) finished in 0.064 s
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: countByValue at KMeans.scala:399, took 0.727346 s
  [INFO] [2017-12-14 14:15:51][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(9) (from destroy at KMeans.scala:401)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_9_piece0 on 192.168.18.5:33231 in memory (size: 928.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.mllib.clustering.LocalKMeans]Local KMeans++ converged in 3 iterations.
  [INFO] [2017-12-14 14:15:51][org.apache.spark.mllib.clustering.KMeans]Initialization with k-means|| took 5.337 seconds.
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 496.0 B, free 869.0 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 452.0 B, free 869.0 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 192.168.18.5:33231 (size: 452.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at KMeans.scala:273
  [INFO] [2017-12-14 14:15:51][org.apache.spark.SparkContext]Starting job: collectAsMap at KMeans.scala:295
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Registering RDD 20 (mapPartitions at KMeans.scala:276)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Got job 7 (collectAsMap at KMeans.scala:295) with 1 output partitions
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (collectAsMap at KMeans.scala:295)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 8)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 8)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 8 (MapPartitionsRDD[20] at mapPartitions at KMeans.scala:276), which has no missing parents
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 6.2 KB, free 869.0 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.4 KB, free 869.0 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 192.168.18.5:33231 (size: 3.4 KB, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[20] at mapPartitions at KMeans.scala:276) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 5184 bytes)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 8)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 14:15:51][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 8). 1223 bytes result sent to driver
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 8) in 153 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 8 (mapPartitions at KMeans.scala:276) finished in 0.147 s
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]running: Set()
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 9)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]failed: Set()
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (ShuffledRDD[21] at reduceByKey at KMeans.scala:292), which has no missing parents
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 2.8 KB, free 869.0 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 1654.0 B, free 869.0 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 192.168.18.5:33231 (size: 1654.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (ShuffledRDD[21] at reduceByKey at KMeans.scala:292) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 4621 bytes)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 9)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
  [INFO] [2017-12-14 14:15:51][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 9). 1744 bytes result sent to driver
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 9) in 13 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (collectAsMap at KMeans.scala:295) finished in 0.009 s
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: collectAsMap at KMeans.scala:295, took 0.205418 s
  [INFO] [2017-12-14 14:15:51][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(12) (from destroy at KMeans.scala:297)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_12_piece0 on 192.168.18.5:33231 in memory (size: 452.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 496.0 B, free 869.0 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 455.0 B, free 869.0 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 192.168.18.5:33231 (size: 455.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at KMeans.scala:273
  [INFO] [2017-12-14 14:15:51][org.apache.spark.SparkContext]Starting job: collectAsMap at KMeans.scala:295
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Registering RDD 22 (mapPartitions at KMeans.scala:276)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Got job 8 (collectAsMap at KMeans.scala:295) with 1 output partitions
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (collectAsMap at KMeans.scala:295)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 10)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 10)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 10 (MapPartitionsRDD[22] at mapPartitions at KMeans.scala:276), which has no missing parents
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 6.2 KB, free 868.9 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.4 KB, free 868.9 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 192.168.18.5:33231 (size: 3.4 KB, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[22] at mapPartitions at KMeans.scala:276) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 5184 bytes)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 10)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 14:15:51][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 10). 1223 bytes result sent to driver
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 10) in 116 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 10 (mapPartitions at KMeans.scala:276) finished in 0.118 s
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]running: Set()
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 11)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]failed: Set()
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (ShuffledRDD[23] at reduceByKey at KMeans.scala:292), which has no missing parents
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17 stored as values in memory (estimated size 2.8 KB, free 868.9 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_17_piece0 stored as bytes in memory (estimated size 1659.0 B, free 868.9 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_17_piece0 in memory on 192.168.18.5:33231 (size: 1659.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.SparkContext]Created broadcast 17 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (ShuffledRDD[23] at reduceByKey at KMeans.scala:292) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, ANY, 4621 bytes)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 11)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
  [INFO] [2017-12-14 14:15:51][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 11). 1744 bytes result sent to driver
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 11) in 12 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (collectAsMap at KMeans.scala:295) finished in 0.014 s
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Job 8 finished: collectAsMap at KMeans.scala:295, took 0.165213 s
  [INFO] [2017-12-14 14:15:51][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(15) (from destroy at KMeans.scala:297)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_15_piece0 on 192.168.18.5:33231 in memory (size: 455.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18 stored as values in memory (estimated size 496.0 B, free 868.9 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_18_piece0 stored as bytes in memory (estimated size 458.0 B, free 868.9 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_18_piece0 in memory on 192.168.18.5:33231 (size: 458.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.SparkContext]Created broadcast 18 from broadcast at KMeans.scala:273
  [INFO] [2017-12-14 14:15:51][org.apache.spark.SparkContext]Starting job: collectAsMap at KMeans.scala:295
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Registering RDD 24 (mapPartitions at KMeans.scala:276)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Got job 9 (collectAsMap at KMeans.scala:295) with 1 output partitions
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (collectAsMap at KMeans.scala:295)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 12)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 12)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 12 (MapPartitionsRDD[24] at mapPartitions at KMeans.scala:276), which has no missing parents
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19 stored as values in memory (estimated size 6.2 KB, free 868.9 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_19_piece0 stored as bytes in memory (estimated size 3.4 KB, free 868.9 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_19_piece0 in memory on 192.168.18.5:33231 (size: 3.4 KB, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.SparkContext]Created broadcast 19 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[24] at mapPartitions at KMeans.scala:276) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
  [INFO] [2017-12-14 14:15:51][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 5184 bytes)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 12)
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 14:15:51][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 14:15:52][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 12). 1223 bytes result sent to driver
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 12) in 138 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 12 (mapPartitions at KMeans.scala:276) finished in 0.139 s
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]running: Set()
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 13)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]failed: Set()
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (ShuffledRDD[25] at reduceByKey at KMeans.scala:292), which has no missing parents
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20 stored as values in memory (estimated size 2.8 KB, free 868.9 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_20_piece0 stored as bytes in memory (estimated size 1659.0 B, free 868.9 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManagerInfo]Added broadcast_20_piece0 in memory on 192.168.18.5:33231 (size: 1659.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.ContextCleaner]Cleaned accumulator 216
  [INFO] [2017-12-14 14:15:52][org.apache.spark.SparkContext]Created broadcast 20 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (ShuffledRDD[25] at reduceByKey at KMeans.scala:292) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, ANY, 4621 bytes)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 13)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.ContextCleaner]Cleaned shuffle 1
  [INFO] [2017-12-14 14:15:52][org.apache.spark.ContextCleaner]Cleaned accumulator 265
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_17_piece0 on 192.168.18.5:33231 in memory (size: 1659.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 13). 1744 bytes result sent to driver
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 13) in 15 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (collectAsMap at KMeans.scala:295) finished in 0.007 s
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Job 9 finished: collectAsMap at KMeans.scala:295, took 0.231975 s
  [INFO] [2017-12-14 14:15:52][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(18) (from destroy at KMeans.scala:297)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21 stored as values in memory (estimated size 496.0 B, free 868.9 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_18_piece0 on 192.168.18.5:33231 in memory (size: 458.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_21_piece0 stored as bytes in memory (estimated size 458.0 B, free 868.9 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_13_piece0 on 192.168.18.5:33231 in memory (size: 3.4 KB, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManagerInfo]Added broadcast_21_piece0 in memory on 192.168.18.5:33231 (size: 458.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.SparkContext]Created broadcast 21 from broadcast at KMeans.scala:273
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_16_piece0 on 192.168.18.5:33231 in memory (size: 3.4 KB, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_10_piece0 on 192.168.18.5:33231 in memory (size: 3.6 KB, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.SparkContext]Starting job: collectAsMap at KMeans.scala:295
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Registering RDD 26 (mapPartitions at KMeans.scala:276)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Got job 10 (collectAsMap at KMeans.scala:295) with 1 output partitions
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (collectAsMap at KMeans.scala:295)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 14)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 14)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_14_piece0 on 192.168.18.5:33231 in memory (size: 1654.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.ContextCleaner]Cleaned shuffle 2
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 14 (MapPartitionsRDD[26] at mapPartitions at KMeans.scala:276), which has no missing parents
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManager]Removing RDD 9
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22 stored as values in memory (estimated size 6.2 KB, free 869.0 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.ContextCleaner]Cleaned RDD 9
  [INFO] [2017-12-14 14:15:52][org.apache.spark.ContextCleaner]Cleaned shuffle 0
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManager]Removing RDD 13
  [INFO] [2017-12-14 14:15:52][org.apache.spark.ContextCleaner]Cleaned RDD 13
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_11_piece0 on 192.168.18.5:33231 in memory (size: 1971.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_22_piece0 stored as bytes in memory (estimated size 3.4 KB, free 869.0 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManagerInfo]Added broadcast_22_piece0 in memory on 192.168.18.5:33231 (size: 3.4 KB, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.SparkContext]Created broadcast 22 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[26] at mapPartitions at KMeans.scala:276) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 5184 bytes)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 14)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManager]Found block rdd_3_0 locally
  [INFO] [2017-12-14 14:15:52][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 14). 1223 bytes result sent to driver
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 14) in 103 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 14 (mapPartitions at KMeans.scala:276) finished in 0.105 s
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]running: Set()
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 15)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]failed: Set()
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (ShuffledRDD[27] at reduceByKey at KMeans.scala:292), which has no missing parents
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23 stored as values in memory (estimated size 2.8 KB, free 869.0 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_23_piece0 stored as bytes in memory (estimated size 1659.0 B, free 869.0 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManagerInfo]Added broadcast_23_piece0 in memory on 192.168.18.5:33231 (size: 1659.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.SparkContext]Created broadcast 23 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 15 (ShuffledRDD[27] at reduceByKey at KMeans.scala:292) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 1 tasks
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, ANY, 4621 bytes)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 15)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 1 non-empty blocks out of 1 blocks
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
  [INFO] [2017-12-14 14:15:52][org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 15). 1744 bytes result sent to driver
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 15) in 9 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (collectAsMap at KMeans.scala:295) finished in 0.009 s
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Job 10 finished: collectAsMap at KMeans.scala:295, took 0.143643 s
  [INFO] [2017-12-14 14:15:52][org.apache.spark.broadcast.TorrentBroadcast]Destroying Broadcast(21) (from destroy at KMeans.scala:297)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_21_piece0 on 192.168.18.5:33231 in memory (size: 458.0 B, free: 869.2 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.mllib.clustering.KMeans]Iterations took 0.870 seconds.
  [INFO] [2017-12-14 14:15:52][org.apache.spark.mllib.clustering.KMeans]KMeans converged in 4 iterations.
  [INFO] [2017-12-14 14:15:52][org.apache.spark.mllib.clustering.KMeans]The cost is 278902.29795022146.
  [INFO] [2017-12-14 14:15:52][org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManager]Removing RDD 3
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_24 stored as values in memory (estimated size 209.1 KB, free 870.5 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_24_piece0 stored as bytes in memory (estimated size 20.1 KB, free 870.5 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManagerInfo]Added broadcast_24_piece0 in memory on 192.168.18.5:33231 (size: 20.1 KB, free: 870.9 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.SparkContext]Created broadcast 24 from textFile at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:15:52][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:15:52][org.apache.spark.SparkContext]Starting job: first at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Got job 11 (first at modelSaveLoad.scala:129) with 1 output partitions
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 16 (first at modelSaveLoad.scala:129)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 16 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[29] at textFile at modelSaveLoad.scala:129), which has no missing parents
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_25 stored as values in memory (estimated size 3.3 KB, free 870.5 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_25_piece0 stored as bytes in memory (estimated size 1982.0 B, free 870.5 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.storage.BlockManagerInfo]Added broadcast_25_piece0 in memory on 192.168.18.5:33231 (size: 1982.0 B, free: 870.9 MB)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.SparkContext]Created broadcast 25 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 16 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[29] at textFile at modelSaveLoad.scala:129) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 16.0 with 1 tasks
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, ANY, 4926 bytes)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.executor.Executor]Running task 0.0 in stage 16.0 (TID 16)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata/part-00000:0+80
  [INFO] [2017-12-14 14:15:52][org.apache.spark.executor.Executor]Finished task 0.0 in stage 16.0 (TID 16). 876 bytes result sent to driver
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 16.0 (TID 16) in 14 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 16.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]ResultStage 16 (first at modelSaveLoad.scala:129) finished in 0.015 s
  [INFO] [2017-12-14 14:15:52][org.apache.spark.scheduler.DAGScheduler]Job 11 finished: first at modelSaveLoad.scala:129, took 0.030589 s
  [INFO] [2017-12-14 14:15:52][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/').
  [INFO] [2017-12-14 14:15:52][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/'.
  [INFO] [2017-12-14 14:15:52][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1c8f6c66{/SQL,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:52][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1b7f06c7{/SQL/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:52][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@feb098f{/SQL/execution,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:52][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@31e739bf{/SQL/execution/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:52][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2efd2f21{/static/sql,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:15:53][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
  [INFO] [2017-12-14 14:15:53][org.apache.spark.SparkContext]Starting job: parquet at KMeansModel.scala:142
  [INFO] [2017-12-14 14:15:53][org.apache.spark.scheduler.DAGScheduler]Got job 12 (parquet at KMeansModel.scala:142) with 1 output partitions
  [INFO] [2017-12-14 14:15:53][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 17 (parquet at KMeansModel.scala:142)
  [INFO] [2017-12-14 14:15:53][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:15:53][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:15:53][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 17 (MapPartitionsRDD[31] at parquet at KMeansModel.scala:142), which has no missing parents
  [INFO] [2017-12-14 14:15:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_26 stored as values in memory (estimated size 62.1 KB, free 870.5 MB)
  [INFO] [2017-12-14 14:15:53][org.apache.spark.storage.memory.MemoryStore]Block broadcast_26_piece0 stored as bytes in memory (estimated size 21.8 KB, free 870.4 MB)
  [INFO] [2017-12-14 14:15:53][org.apache.spark.storage.BlockManagerInfo]Added broadcast_26_piece0 in memory on 192.168.18.5:33231 (size: 21.8 KB, free: 870.9 MB)
  [INFO] [2017-12-14 14:15:53][org.apache.spark.SparkContext]Created broadcast 26 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:53][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[31] at parquet at KMeansModel.scala:142) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:53][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 17.0 with 1 tasks
  [INFO] [2017-12-14 14:15:53][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 5086 bytes)
  [INFO] [2017-12-14 14:15:53][org.apache.spark.executor.Executor]Running task 0.0 in stage 17.0 (TID 17)
  [INFO] [2017-12-14 14:15:54][org.apache.spark.ContextCleaner]Cleaned accumulator 363
  [INFO] [2017-12-14 14:15:54][org.apache.spark.ContextCleaner]Cleaned shuffle 3
  [INFO] [2017-12-14 14:15:54][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_22_piece0 on 192.168.18.5:33231 in memory (size: 3.4 KB, free: 870.9 MB)
  [INFO] [2017-12-14 14:15:54][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_20_piece0 on 192.168.18.5:33231 in memory (size: 1659.0 B, free: 870.9 MB)
  [INFO] [2017-12-14 14:15:54][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_19_piece0 on 192.168.18.5:33231 in memory (size: 3.4 KB, free: 870.9 MB)
  [INFO] [2017-12-14 14:15:54][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_25_piece0 on 192.168.18.5:33231 in memory (size: 1982.0 B, free: 870.9 MB)
  [INFO] [2017-12-14 14:15:54][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_24_piece0 on 192.168.18.5:33231 in memory (size: 20.1 KB, free: 871.0 MB)
  [INFO] [2017-12-14 14:15:54][org.apache.spark.ContextCleaner]Cleaned accumulator 314
  [INFO] [2017-12-14 14:15:54][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_23_piece0 on 192.168.18.5:33231 in memory (size: 1659.0 B, free: 871.0 MB)
  [INFO] [2017-12-14 14:15:54][org.apache.spark.ContextCleaner]Cleaned shuffle 4
  [INFO] [2017-12-14 14:15:54][org.apache.spark.storage.BlockManager]Removing RDD 3
  [INFO] [2017-12-14 14:15:54][org.apache.spark.ContextCleaner]Cleaned RDD 3
  [INFO] [2017-12-14 14:15:54][org.apache.spark.executor.Executor]Finished task 0.0 in stage 17.0 (TID 17). 1687 bytes result sent to driver
  [INFO] [2017-12-14 14:15:54][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 17.0 (TID 17) in 716 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:54][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 17.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:54][org.apache.spark.scheduler.DAGScheduler]ResultStage 17 (parquet at KMeansModel.scala:142) finished in 0.706 s
  [INFO] [2017-12-14 14:15:54][org.apache.spark.scheduler.DAGScheduler]Job 12 finished: parquet at KMeansModel.scala:142, took 0.838080 s
  [INFO] [2017-12-14 14:15:56][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
  [INFO] [2017-12-14 14:15:56][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: 
  [INFO] [2017-12-14 14:15:56][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<id: int, point: vector>
  [INFO] [2017-12-14 14:15:56][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
  [INFO] [2017-12-14 14:15:57][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 429.817826 ms
  [INFO] [2017-12-14 14:15:57][org.apache.spark.storage.memory.MemoryStore]Block broadcast_27 stored as values in memory (estimated size 222.5 KB, free 870.5 MB)
  [INFO] [2017-12-14 14:15:57][org.apache.spark.storage.memory.MemoryStore]Block broadcast_27_piece0 stored as bytes in memory (estimated size 21.2 KB, free 870.5 MB)
  [INFO] [2017-12-14 14:15:57][org.apache.spark.storage.BlockManagerInfo]Added broadcast_27_piece0 in memory on 192.168.18.5:33231 (size: 21.2 KB, free: 870.9 MB)
  [INFO] [2017-12-14 14:15:57][org.apache.spark.SparkContext]Created broadcast 27 from rdd at KMeansModel.scala:144
  [INFO] [2017-12-14 14:15:57][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 4195868 bytes, open cost is considered as scanning 4194304 bytes.
  [INFO] [2017-12-14 14:15:57][org.apache.spark.SparkContext]Starting job: collect at KMeansModel.scala:144
  [INFO] [2017-12-14 14:15:57][org.apache.spark.scheduler.DAGScheduler]Got job 13 (collect at KMeansModel.scala:144) with 1 output partitions
  [INFO] [2017-12-14 14:15:57][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 18 (collect at KMeansModel.scala:144)
  [INFO] [2017-12-14 14:15:57][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:15:57][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:15:57][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 18 (MapPartitionsRDD[36] at map at KMeansModel.scala:144), which has no missing parents
  [INFO] [2017-12-14 14:15:57][org.apache.spark.storage.memory.MemoryStore]Block broadcast_28 stored as values in memory (estimated size 15.9 KB, free 870.4 MB)
  [INFO] [2017-12-14 14:15:57][org.apache.spark.storage.memory.MemoryStore]Block broadcast_28_piece0 stored as bytes in memory (estimated size 7.3 KB, free 870.4 MB)
  [INFO] [2017-12-14 14:15:57][org.apache.spark.storage.BlockManagerInfo]Added broadcast_28_piece0 in memory on 192.168.18.5:33231 (size: 7.3 KB, free: 870.9 MB)
  [INFO] [2017-12-14 14:15:57][org.apache.spark.SparkContext]Created broadcast 28 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:57][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[36] at map at KMeansModel.scala:144) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:57][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 18.0 with 1 tasks
  [INFO] [2017-12-14 14:15:57][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 18.0 (TID 18, localhost, executor driver, partition 0, ANY, 5397 bytes)
  [INFO] [2017-12-14 14:15:57][org.apache.spark.executor.Executor]Running task 0.0 in stage 18.0 (TID 18)
  [INFO] [2017-12-14 14:15:57][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 47.627375 ms
  [INFO] [2017-12-14 14:15:57][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/data/part-00000-b27e5343-d95e-472b-9d50-03bfef091743-c000.snappy.parquet, range: 0-1564, partition values: [empty row]
  [INFO] [2017-12-14 14:15:57][org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport]Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  required int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
  [INFO] [2017-12-14 14:15:57][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 46.965706 ms
  [INFO] [2017-12-14 14:15:57][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 43.82781 ms
  [INFO] [2017-12-14 14:15:57][org.apache.parquet.hadoop.InternalParquetRecordReader]RecordReader initialized will read a total of 4 records.
  [INFO] [2017-12-14 14:15:57][org.apache.parquet.hadoop.InternalParquetRecordReader]at row 0. reading next block
  [INFO] [2017-12-14 14:15:57][org.apache.hadoop.io.compress.CodecPool]Got brand-new decompressor [.snappy]
  [INFO] [2017-12-14 14:15:57][org.apache.parquet.hadoop.InternalParquetRecordReader]block read in memory in 36 ms. row count = 4
  [INFO] [2017-12-14 14:15:58][org.apache.spark.executor.Executor]Finished task 0.0 in stage 18.0 (TID 18). 1799 bytes result sent to driver
  [INFO] [2017-12-14 14:15:58][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 18.0 (TID 18) in 714 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:15:58][org.apache.spark.scheduler.DAGScheduler]ResultStage 18 (collect at KMeansModel.scala:144) finished in 0.709 s
  [INFO] [2017-12-14 14:15:58][org.apache.spark.scheduler.DAGScheduler]Job 13 finished: collect at KMeansModel.scala:144, took 0.748819 s
  [INFO] [2017-12-14 14:15:58][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 18.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:15:58][org.apache.spark.SparkContext]Starting job: collect at ModelTest.java:91
  [INFO] [2017-12-14 14:15:58][org.apache.spark.scheduler.DAGScheduler]Got job 14 (collect at ModelTest.java:91) with 1 output partitions
  [INFO] [2017-12-14 14:15:58][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 19 (collect at ModelTest.java:91)
  [INFO] [2017-12-14 14:15:58][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:15:58][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:15:58][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 19 (MapPartitionsRDD[37] at map at ModelTest.java:91), which has no missing parents
  [INFO] [2017-12-14 14:15:58][org.apache.spark.storage.memory.MemoryStore]Block broadcast_29 stored as values in memory (estimated size 5.4 KB, free 870.4 MB)
  [INFO] [2017-12-14 14:15:58][org.apache.spark.storage.memory.MemoryStore]Block broadcast_29_piece0 stored as bytes in memory (estimated size 3.1 KB, free 870.4 MB)
  [INFO] [2017-12-14 14:15:58][org.apache.spark.storage.BlockManagerInfo]Added broadcast_29_piece0 in memory on 192.168.18.5:33231 (size: 3.1 KB, free: 870.9 MB)
  [INFO] [2017-12-14 14:15:58][org.apache.spark.SparkContext]Created broadcast 29 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:15:58][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[37] at map at ModelTest.java:91) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:15:58][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 19.0 with 1 tasks
  [INFO] [2017-12-14 14:15:58][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 19.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 4884 bytes)
  [INFO] [2017-12-14 14:15:58][org.apache.spark.executor.Executor]Running task 0.0 in stage 19.0 (TID 19)
  [INFO] [2017-12-14 14:15:58][org.apache.spark.storage.BlockManager]Found block rdd_2_0 locally
  [INFO] [2017-12-14 14:15:58][org.apache.spark.ContextCleaner]Cleaned accumulator 462
  [INFO] [2017-12-14 14:15:58][org.apache.spark.ContextCleaner]Cleaned accumulator 460
  [INFO] [2017-12-14 14:15:58][org.apache.spark.ContextCleaner]Cleaned accumulator 461
  [INFO] [2017-12-14 14:15:58][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_28_piece0 on 192.168.18.5:33231 in memory (size: 7.3 KB, free: 870.9 MB)
  [INFO] [2017-12-14 14:15:58][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_27_piece0 on 192.168.18.5:33231 in memory (size: 21.2 KB, free: 871.0 MB)
  [INFO] [2017-12-14 14:15:58][org.apache.spark.ContextCleaner]Cleaned accumulator 464
  [INFO] [2017-12-14 14:15:58][org.apache.spark.ContextCleaner]Cleaned accumulator 463
  [INFO] [2017-12-14 14:15:59][org.apache.spark.storage.memory.MemoryStore]Block taskresult_19 stored as bytes in memory (estimated size 10.7 MB, free 859.9 MB)
  [INFO] [2017-12-14 14:15:59][org.apache.spark.storage.BlockManagerInfo]Added taskresult_19 in memory on 192.168.18.5:33231 (size: 10.7 MB, free: 860.2 MB)
  [INFO] [2017-12-14 14:15:59][org.apache.spark.executor.Executor]Finished task 0.0 in stage 19.0 (TID 19). 11257671 bytes result sent via BlockManager)
  [INFO] [2017-12-14 14:15:59][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.18.5:33231 after 34 ms (0 ms spent in bootstraps)
  [INFO] [2017-12-14 14:16:00][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 19.0 (TID 19) in 1973 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:16:00][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 19.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:16:00][org.apache.spark.scheduler.DAGScheduler]ResultStage 19 (collect at ModelTest.java:91) finished in 1.973 s
  [INFO] [2017-12-14 14:16:00][org.apache.spark.scheduler.DAGScheduler]Job 14 finished: collect at ModelTest.java:91, took 2.019858 s
  [INFO] [2017-12-14 14:16:00][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_19 on 192.168.18.5:33231 in memory (size: 10.7 MB, free: 871.0 MB)
  [INFO] [2017-12-14 14:16:00][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@4da6bb31{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:16:00][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:16:00][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
  [INFO] [2017-12-14 14:16:00][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
  [INFO] [2017-12-14 14:16:00][org.apache.spark.storage.BlockManager]BlockManager stopped
  [INFO] [2017-12-14 14:16:00][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
  [INFO] [2017-12-14 14:16:00][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
  [INFO] [2017-12-14 14:16:00][org.apache.spark.SparkContext]Successfully stopped SparkContext
  [INFO] [2017-12-14 14:16:00][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
  [INFO] [2017-12-14 14:16:00][org.apache.spark.util.ShutdownHookManager]Deleting directory /tmp/spark-2ee8c385-57bc-4a93-aac6-0049b8357e3a
  [INFO] [2017-12-14 14:18:52][org.apache.spark.SparkContext]Running Spark version 2.2.0
  [WARN] [2017-12-14 14:18:53][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-14 14:18:53][org.apache.spark.util.Utils]Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.18.5 instead (on interface eth0)
  [WARN] [2017-12-14 14:18:53][org.apache.spark.util.Utils]Set SPARK_LOCAL_IP if you need to bind to another address
  [INFO] [2017-12-14 14:18:53][org.apache.spark.SparkContext]Submitted application: JavaKMeansExample
  [INFO] [2017-12-14 14:18:53][org.apache.spark.SecurityManager]Changing view acls to: hadoop
  [INFO] [2017-12-14 14:18:53][org.apache.spark.SecurityManager]Changing modify acls to: hadoop
  [INFO] [2017-12-14 14:18:53][org.apache.spark.SecurityManager]Changing view acls groups to: 
  [INFO] [2017-12-14 14:18:53][org.apache.spark.SecurityManager]Changing modify acls groups to: 
  [INFO] [2017-12-14 14:18:53][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
  [INFO] [2017-12-14 14:18:54][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 43605.
  [INFO] [2017-12-14 14:18:54][org.apache.spark.SparkEnv]Registering MapOutputTracker
  [INFO] [2017-12-14 14:18:54][org.apache.spark.SparkEnv]Registering BlockManagerMaster
  [INFO] [2017-12-14 14:18:54][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
  [INFO] [2017-12-14 14:18:54][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
  [INFO] [2017-12-14 14:18:54][org.apache.spark.storage.DiskBlockManager]Created local directory at /tmp/blockmgr-1938b013-df05-4469-9e1c-85b1c50e051a
  [INFO] [2017-12-14 14:18:54][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 882.6 MB
  [INFO] [2017-12-14 14:18:54][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.util.log]Logging initialized @3044ms
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.Server]Started @3233ms
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@7b8de31f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:18:54][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@b672aa8{/jobs,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@62923ee6{/jobs/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f19c9d2{/jobs/job,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4b6166aa{/jobs/job/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fd4cae3{/stages,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a1217f9{/stages/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@523424b5{/stages/stage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a7e2d9d{/stages/stage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b52c0d6{/stages/pool,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4cc76301{/stages/pool/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f19b8b3{/storage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a486d78{/storage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7ef2d7a6{/storage/rdd,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4c36250e{/storage/rdd/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@49f5c307{/environment,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5471388b{/environment/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1601e47{/executors,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66971f6b{/executors/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@517bd097{/executors/threadDump,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:54][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9cc6cb{/executors/threadDump/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:55][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@56e07a08{/static,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:55][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72b16078{/,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:55][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70fab835{/api,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:55][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@759fad4{/jobs/job/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:55][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53499d85{/stages/stage/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:55][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:18:55][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
  [INFO] [2017-12-14 14:18:55][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37561.
  [INFO] [2017-12-14 14:18:55][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.18.5:37561
  [INFO] [2017-12-14 14:18:55][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
  [INFO] [2017-12-14 14:18:55][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.18.5, 37561, None)
  [INFO] [2017-12-14 14:18:55][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.18.5:37561 with 882.6 MB RAM, BlockManagerId(driver, 192.168.18.5, 37561, None)
  [INFO] [2017-12-14 14:18:55][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.18.5, 37561, None)
  [INFO] [2017-12-14 14:18:55][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.18.5, 37561, None)
  [INFO] [2017-12-14 14:18:55][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@46044faa{/metrics/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:56][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 209.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:18:56][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:18:56][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.18.5:37561 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:18:56][org.apache.spark.SparkContext]Created broadcast 0 from textFile at ModelTest.java:44
  [INFO] [2017-12-14 14:18:56][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 209.1 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:18:56][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:18:56][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.18.5:37561 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:18:56][org.apache.spark.SparkContext]Created broadcast 1 from textFile at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:18:57][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:18:57][org.apache.spark.SparkContext]Starting job: first at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:18:57][org.apache.spark.scheduler.DAGScheduler]Got job 0 (first at modelSaveLoad.scala:129) with 1 output partitions
  [INFO] [2017-12-14 14:18:57][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (first at modelSaveLoad.scala:129)
  [INFO] [2017-12-14 14:18:57][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:18:57][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:18:57][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[4] at textFile at modelSaveLoad.scala:129), which has no missing parents
  [INFO] [2017-12-14 14:18:57][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:18:57][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1980.0 B, free 882.1 MB)
  [INFO] [2017-12-14 14:18:57][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.18.5:37561 (size: 1980.0 B, free: 882.6 MB)
  [INFO] [2017-12-14 14:18:57][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:18:57][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[4] at textFile at modelSaveLoad.scala:129) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:18:57][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
  [INFO] [2017-12-14 14:18:58][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4926 bytes)
  [INFO] [2017-12-14 14:18:58][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
  [INFO] [2017-12-14 14:18:58][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata/part-00000:0+80
  [INFO] [2017-12-14 14:18:58][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 876 bytes result sent to driver
  [INFO] [2017-12-14 14:18:58][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 362 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:18:58][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (first at modelSaveLoad.scala:129) finished in 0.407 s
  [INFO] [2017-12-14 14:18:58][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:18:58][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: first at modelSaveLoad.scala:129, took 0.644223 s
  [INFO] [2017-12-14 14:18:58][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/').
  [INFO] [2017-12-14 14:18:58][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/'.
  [INFO] [2017-12-14 14:18:58][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f10f633{/SQL,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:58][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3440e9cd{/SQL/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:58][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@502a4156{/SQL/execution,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:58][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64e1377c{/SQL/execution/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:58][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1319bc2a{/static/sql,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:18:59][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
  [INFO] [2017-12-14 14:18:59][org.apache.spark.SparkContext]Starting job: parquet at KMeansModel.scala:142
  [INFO] [2017-12-14 14:18:59][org.apache.spark.scheduler.DAGScheduler]Got job 1 (parquet at KMeansModel.scala:142) with 1 output partitions
  [INFO] [2017-12-14 14:18:59][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (parquet at KMeansModel.scala:142)
  [INFO] [2017-12-14 14:18:59][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:18:59][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:18:59][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[6] at parquet at KMeansModel.scala:142), which has no missing parents
  [INFO] [2017-12-14 14:18:59][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 62.1 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:18:59][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.8 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:18:59][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.18.5:37561 (size: 21.8 KB, free: 882.5 MB)
  [INFO] [2017-12-14 14:18:59][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:18:59][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at parquet at KMeansModel.scala:142) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:18:59][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
  [INFO] [2017-12-14 14:18:59][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5086 bytes)
  [INFO] [2017-12-14 14:18:59][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
  [INFO] [2017-12-14 14:19:00][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.18.5:37561 in memory (size: 1980.0 B, free: 882.5 MB)
  [INFO] [2017-12-14 14:19:00][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 1687 bytes result sent to driver
  [INFO] [2017-12-14 14:19:00][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 840 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:19:00][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:19:00][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (parquet at KMeansModel.scala:142) finished in 0.843 s
  [INFO] [2017-12-14 14:19:00][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: parquet at KMeansModel.scala:142, took 0.966928 s
  [INFO] [2017-12-14 14:19:00][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.18.5:37561 in memory (size: 21.8 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:19:00][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.18.5:37561 in memory (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:19:03][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
  [INFO] [2017-12-14 14:19:03][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: 
  [INFO] [2017-12-14 14:19:03][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<id: int, point: vector>
  [INFO] [2017-12-14 14:19:03][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
  [INFO] [2017-12-14 14:19:03][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 358.572225 ms
  [INFO] [2017-12-14 14:19:03][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 222.5 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:19:03][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.2 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:19:03][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.18.5:37561 (size: 21.2 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:19:03][org.apache.spark.SparkContext]Created broadcast 4 from rdd at KMeansModel.scala:144
  [INFO] [2017-12-14 14:19:03][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 4195868 bytes, open cost is considered as scanning 4194304 bytes.
  [INFO] [2017-12-14 14:19:04][org.apache.spark.SparkContext]Starting job: collect at KMeansModel.scala:144
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.DAGScheduler]Got job 2 (collect at KMeansModel.scala:144) with 1 output partitions
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (collect at KMeansModel.scala:144)
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[11] at map at KMeansModel.scala:144), which has no missing parents
  [INFO] [2017-12-14 14:19:04][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 15.9 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:19:04][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:19:04][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.18.5:37561 (size: 7.3 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:19:04][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at map at KMeansModel.scala:144) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 5397 bytes)
  [INFO] [2017-12-14 14:19:04][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
  [INFO] [2017-12-14 14:19:04][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 35.308514 ms
  [INFO] [2017-12-14 14:19:04][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/data/part-00000-b27e5343-d95e-472b-9d50-03bfef091743-c000.snappy.parquet, range: 0-1564, partition values: [empty row]
  [INFO] [2017-12-14 14:19:04][org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport]Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  required int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
  [INFO] [2017-12-14 14:19:04][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 50.419761 ms
  [INFO] [2017-12-14 14:19:04][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 70.485713 ms
  [INFO] [2017-12-14 14:19:04][org.apache.parquet.hadoop.InternalParquetRecordReader]RecordReader initialized will read a total of 4 records.
  [INFO] [2017-12-14 14:19:04][org.apache.parquet.hadoop.InternalParquetRecordReader]at row 0. reading next block
  [INFO] [2017-12-14 14:19:04][org.apache.hadoop.io.compress.CodecPool]Got brand-new decompressor [.snappy]
  [INFO] [2017-12-14 14:19:04][org.apache.parquet.hadoop.InternalParquetRecordReader]block read in memory in 41 ms. row count = 4
  [INFO] [2017-12-14 14:19:04][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 1885 bytes result sent to driver
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 719 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (collect at KMeansModel.scala:144) finished in 0.716 s
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: collect at KMeansModel.scala:144, took 0.734865 s
  [INFO] [2017-12-14 14:19:04][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:19:04][org.apache.spark.SparkContext]Starting job: collect at ModelTest.java:91
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.DAGScheduler]Got job 3 (collect at ModelTest.java:91) with 1 output partitions
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (collect at ModelTest.java:91)
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[12] at map at ModelTest.java:91), which has no missing parents
  [INFO] [2017-12-14 14:19:04][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 5.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:19:04][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.0 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:19:04][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.18.5:37561 (size: 3.0 KB, free: 882.5 MB)
  [INFO] [2017-12-14 14:19:04][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[12] at map at ModelTest.java:91) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
  [INFO] [2017-12-14 14:19:04][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4884 bytes)
  [INFO] [2017-12-14 14:19:04][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
  [INFO] [2017-12-14 14:19:04][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/output/clean/JuLei/part-r-00000:0+4881501
  [INFO] [2017-12-14 14:19:05][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.18.5:37561 in memory (size: 7.3 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:19:05][org.apache.spark.storage.memory.MemoryStore]Block rdd_2_0 stored as values in memory (estimated size 11.6 MB, free 870.5 MB)
  [INFO] [2017-12-14 14:19:05][org.apache.spark.storage.BlockManagerInfo]Added rdd_2_0 in memory on 192.168.18.5:37561 (size: 11.6 MB, free: 871.0 MB)
  [WARN] [2017-12-14 14:19:05][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
  [WARN] [2017-12-14 14:19:05][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
  [INFO] [2017-12-14 14:19:06][org.apache.spark.storage.memory.MemoryStore]Block taskresult_3 stored as bytes in memory (estimated size 10.7 MB, free 859.8 MB)
  [INFO] [2017-12-14 14:19:06][org.apache.spark.storage.BlockManagerInfo]Added taskresult_3 in memory on 192.168.18.5:37561 (size: 10.7 MB, free: 860.2 MB)
  [INFO] [2017-12-14 14:19:06][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 11258368 bytes result sent via BlockManager)
  [INFO] [2017-12-14 14:19:06][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.18.5:37561 after 34 ms (0 ms spent in bootstraps)
  [INFO] [2017-12-14 14:19:07][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.18.5:37561 in memory (size: 21.2 KB, free: 860.2 MB)
  [INFO] [2017-12-14 14:19:07][org.apache.spark.ContextCleaner]Cleaned accumulator 74
  [INFO] [2017-12-14 14:19:07][org.apache.spark.ContextCleaner]Cleaned accumulator 76
  [INFO] [2017-12-14 14:19:07][org.apache.spark.ContextCleaner]Cleaned accumulator 75
  [INFO] [2017-12-14 14:19:07][org.apache.spark.ContextCleaner]Cleaned accumulator 72
  [INFO] [2017-12-14 14:19:07][org.apache.spark.ContextCleaner]Cleaned accumulator 73
  [INFO] [2017-12-14 14:19:07][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 2393 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:19:07][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_3 on 192.168.18.5:37561 in memory (size: 10.7 MB, free: 871.0 MB)
  [INFO] [2017-12-14 14:19:07][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:19:07][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (collect at ModelTest.java:91) finished in 2.395 s
  [INFO] [2017-12-14 14:19:07][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: collect at ModelTest.java:91, took 2.412273 s
  [INFO] [2017-12-14 14:19:07][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@7b8de31f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:19:07][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:19:07][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
  [INFO] [2017-12-14 14:19:07][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
  [INFO] [2017-12-14 14:19:07][org.apache.spark.storage.BlockManager]BlockManager stopped
  [INFO] [2017-12-14 14:19:07][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
  [INFO] [2017-12-14 14:19:07][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
  [INFO] [2017-12-14 14:19:07][org.apache.spark.SparkContext]Successfully stopped SparkContext
  [INFO] [2017-12-14 14:19:07][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
  [INFO] [2017-12-14 14:19:07][org.apache.spark.util.ShutdownHookManager]Deleting directory /tmp/spark-8956e2ec-b5e3-4487-b50f-1ecf67244ae8
  [INFO] [2017-12-14 14:22:31][org.apache.spark.SparkContext]Running Spark version 2.2.0
  [WARN] [2017-12-14 14:22:31][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-14 14:22:32][org.apache.spark.util.Utils]Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.18.5 instead (on interface eth0)
  [WARN] [2017-12-14 14:22:32][org.apache.spark.util.Utils]Set SPARK_LOCAL_IP if you need to bind to another address
  [INFO] [2017-12-14 14:22:32][org.apache.spark.SparkContext]Submitted application: JavaKMeansExample
  [INFO] [2017-12-14 14:22:32][org.apache.spark.SecurityManager]Changing view acls to: hadoop
  [INFO] [2017-12-14 14:22:32][org.apache.spark.SecurityManager]Changing modify acls to: hadoop
  [INFO] [2017-12-14 14:22:32][org.apache.spark.SecurityManager]Changing view acls groups to: 
  [INFO] [2017-12-14 14:22:32][org.apache.spark.SecurityManager]Changing modify acls groups to: 
  [INFO] [2017-12-14 14:22:32][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
  [INFO] [2017-12-14 14:22:32][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 45587.
  [INFO] [2017-12-14 14:22:32][org.apache.spark.SparkEnv]Registering MapOutputTracker
  [INFO] [2017-12-14 14:22:32][org.apache.spark.SparkEnv]Registering BlockManagerMaster
  [INFO] [2017-12-14 14:22:32][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
  [INFO] [2017-12-14 14:22:32][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
  [INFO] [2017-12-14 14:22:32][org.apache.spark.storage.DiskBlockManager]Created local directory at /tmp/blockmgr-08594159-7a8c-4ea7-a7f2-c6fe9345143d
  [INFO] [2017-12-14 14:22:32][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 882.6 MB
  [INFO] [2017-12-14 14:22:32][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.util.log]Logging initialized @2916ms
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.Server]Started @3029ms
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@370f532c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:22:33][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b5176f2{/jobs,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f811d00{/jobs/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4089713{/jobs/job,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@b91d8c4{/jobs/job/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a77614d{/stages,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a067c25{/stages/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bde62ff{/stages/stage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@791cbf87{/stages/stage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@754777cd{/stages/pool,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@372ea2bc{/stages/pool/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f08c4b{/storage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7de0c6ae{/storage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@cdc3aae{/storage/rdd,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dcbb60{/storage/rdd/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21526f6c{/environment,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@299266e2{/environment/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66ea1466{/executors,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bffddff{/executors/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@50687efb{/executors/threadDump,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@142eef62{/executors/threadDump/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5990e6c5{/static,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21694e53{/,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@22c86919{/api,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ea1bcdc{/jobs/job/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64712be{/stages/stage/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:33][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:22:33][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
  [INFO] [2017-12-14 14:22:33][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41842.
  [INFO] [2017-12-14 14:22:33][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.18.5:41842
  [INFO] [2017-12-14 14:22:33][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
  [INFO] [2017-12-14 14:22:33][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.18.5, 41842, None)
  [INFO] [2017-12-14 14:22:33][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.18.5:41842 with 882.6 MB RAM, BlockManagerId(driver, 192.168.18.5, 41842, None)
  [INFO] [2017-12-14 14:22:33][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.18.5, 41842, None)
  [INFO] [2017-12-14 14:22:33][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.18.5, 41842, None)
  [INFO] [2017-12-14 14:22:33][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@13e9f2e2{/metrics/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:34][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 209.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:22:34][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:22:34][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.18.5:41842 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:22:34][org.apache.spark.SparkContext]Created broadcast 0 from textFile at ModelTest.java:44
  [INFO] [2017-12-14 14:22:34][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 209.1 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:22:34][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:22:34][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.18.5:41842 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:22:34][org.apache.spark.SparkContext]Created broadcast 1 from textFile at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:22:35][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:22:35][org.apache.spark.SparkContext]Starting job: first at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:22:35][org.apache.spark.scheduler.DAGScheduler]Got job 0 (first at modelSaveLoad.scala:129) with 1 output partitions
  [INFO] [2017-12-14 14:22:35][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (first at modelSaveLoad.scala:129)
  [INFO] [2017-12-14 14:22:35][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:22:35][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:22:35][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[4] at textFile at modelSaveLoad.scala:129), which has no missing parents
  [INFO] [2017-12-14 14:22:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:22:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1980.0 B, free 882.1 MB)
  [INFO] [2017-12-14 14:22:35][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.18.5:41842 (size: 1980.0 B, free: 882.6 MB)
  [INFO] [2017-12-14 14:22:35][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:22:36][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[4] at textFile at modelSaveLoad.scala:129) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:22:36][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
  [INFO] [2017-12-14 14:22:36][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4926 bytes)
  [INFO] [2017-12-14 14:22:36][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
  [INFO] [2017-12-14 14:22:36][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata/part-00000:0+80
  [INFO] [2017-12-14 14:22:36][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 876 bytes result sent to driver
  [INFO] [2017-12-14 14:22:36][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 352 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:22:36][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (first at modelSaveLoad.scala:129) finished in 0.381 s
  [INFO] [2017-12-14 14:22:36][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:22:36][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: first at modelSaveLoad.scala:129, took 0.606707 s
  [INFO] [2017-12-14 14:22:36][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/').
  [INFO] [2017-12-14 14:22:36][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/'.
  [INFO] [2017-12-14 14:22:36][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@28a9494b{/SQL,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:36][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1163a27{/SQL/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:36][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@b0fd744{/SQL/execution,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:36][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a1ef65c{/SQL/execution/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:36][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@798dad3d{/static/sql,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:22:37][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.18.5:41842 in memory (size: 1980.0 B, free: 882.6 MB)
  [INFO] [2017-12-14 14:22:37][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
  [INFO] [2017-12-14 14:22:37][org.apache.spark.SparkContext]Starting job: parquet at KMeansModel.scala:142
  [INFO] [2017-12-14 14:22:37][org.apache.spark.scheduler.DAGScheduler]Got job 1 (parquet at KMeansModel.scala:142) with 1 output partitions
  [INFO] [2017-12-14 14:22:37][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (parquet at KMeansModel.scala:142)
  [INFO] [2017-12-14 14:22:37][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:22:37][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:22:37][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[6] at parquet at KMeansModel.scala:142), which has no missing parents
  [INFO] [2017-12-14 14:22:37][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 62.1 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:22:37][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.8 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:22:37][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.18.5:41842 (size: 21.8 KB, free: 882.5 MB)
  [INFO] [2017-12-14 14:22:37][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:22:37][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at parquet at KMeansModel.scala:142) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:22:37][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
  [INFO] [2017-12-14 14:22:37][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5086 bytes)
  [INFO] [2017-12-14 14:22:37][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
  [INFO] [2017-12-14 14:22:38][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 1644 bytes result sent to driver
  [INFO] [2017-12-14 14:22:38][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 435 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:22:38][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (parquet at KMeansModel.scala:142) finished in 0.428 s
  [INFO] [2017-12-14 14:22:38][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: parquet at KMeansModel.scala:142, took 0.495809 s
  [INFO] [2017-12-14 14:22:38][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:22:38][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.18.5:41842 in memory (size: 21.8 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:22:38][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.18.5:41842 in memory (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:22:40][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
  [INFO] [2017-12-14 14:22:40][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: 
  [INFO] [2017-12-14 14:22:40][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<id: int, point: vector>
  [INFO] [2017-12-14 14:22:40][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
  [INFO] [2017-12-14 14:22:41][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 315.763328 ms
  [INFO] [2017-12-14 14:22:41][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 222.5 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:22:41][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.2 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:22:41][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.18.5:41842 (size: 21.2 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:22:41][org.apache.spark.SparkContext]Created broadcast 4 from rdd at KMeansModel.scala:144
  [INFO] [2017-12-14 14:22:41][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 4195868 bytes, open cost is considered as scanning 4194304 bytes.
  [INFO] [2017-12-14 14:22:41][org.apache.spark.SparkContext]Starting job: collect at KMeansModel.scala:144
  [INFO] [2017-12-14 14:22:41][org.apache.spark.scheduler.DAGScheduler]Got job 2 (collect at KMeansModel.scala:144) with 1 output partitions
  [INFO] [2017-12-14 14:22:41][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (collect at KMeansModel.scala:144)
  [INFO] [2017-12-14 14:22:41][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:22:41][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:22:41][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[11] at map at KMeansModel.scala:144), which has no missing parents
  [INFO] [2017-12-14 14:22:41][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 15.9 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:22:41][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:22:41][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.18.5:41842 (size: 7.3 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:22:41][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:22:41][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at map at KMeansModel.scala:144) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:22:41][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
  [INFO] [2017-12-14 14:22:41][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 5397 bytes)
  [INFO] [2017-12-14 14:22:41][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
  [INFO] [2017-12-14 14:22:41][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 49.069264 ms
  [INFO] [2017-12-14 14:22:41][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/data/part-00000-b27e5343-d95e-472b-9d50-03bfef091743-c000.snappy.parquet, range: 0-1564, partition values: [empty row]
  [INFO] [2017-12-14 14:22:41][org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport]Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  required int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
  [INFO] [2017-12-14 14:22:41][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 66.997442 ms
  [INFO] [2017-12-14 14:22:41][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 51.666776 ms
  [INFO] [2017-12-14 14:22:41][org.apache.parquet.hadoop.InternalParquetRecordReader]RecordReader initialized will read a total of 4 records.
  [INFO] [2017-12-14 14:22:41][org.apache.parquet.hadoop.InternalParquetRecordReader]at row 0. reading next block
  [INFO] [2017-12-14 14:22:41][org.apache.hadoop.io.compress.CodecPool]Got brand-new decompressor [.snappy]
  [INFO] [2017-12-14 14:22:41][org.apache.parquet.hadoop.InternalParquetRecordReader]block read in memory in 45 ms. row count = 4
  [INFO] [2017-12-14 14:22:42][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 1842 bytes result sent to driver
  [INFO] [2017-12-14 14:22:42][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 670 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:22:42][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:22:42][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (collect at KMeansModel.scala:144) finished in 0.665 s
  [INFO] [2017-12-14 14:22:42][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: collect at KMeansModel.scala:144, took 0.691046 s
  [INFO] [2017-12-14 14:22:42][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:22:42][org.apache.spark.SparkContext]Starting job: collect at ModelTest.java:91
  [INFO] [2017-12-14 14:22:42][org.apache.spark.scheduler.DAGScheduler]Got job 3 (collect at ModelTest.java:91) with 1 output partitions
  [INFO] [2017-12-14 14:22:42][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (collect at ModelTest.java:91)
  [INFO] [2017-12-14 14:22:42][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:22:42][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:22:42][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[12] at map at ModelTest.java:91), which has no missing parents
  [INFO] [2017-12-14 14:22:42][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 5.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:22:42][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.0 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:22:42][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.18.5:41842 (size: 3.0 KB, free: 882.5 MB)
  [INFO] [2017-12-14 14:22:42][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:22:42][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[12] at map at ModelTest.java:91) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:22:42][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
  [INFO] [2017-12-14 14:22:42][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4884 bytes)
  [INFO] [2017-12-14 14:22:42][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
  [INFO] [2017-12-14 14:22:42][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/output/clean/JuLei/part-r-00000:0+4881501
  [INFO] [2017-12-14 14:22:42][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.18.5:41842 in memory (size: 7.3 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:22:43][org.apache.spark.storage.memory.MemoryStore]Block rdd_2_0 stored as values in memory (estimated size 11.6 MB, free 870.5 MB)
  [INFO] [2017-12-14 14:22:43][org.apache.spark.storage.BlockManagerInfo]Added rdd_2_0 in memory on 192.168.18.5:41842 (size: 11.6 MB, free: 871.0 MB)
  [WARN] [2017-12-14 14:22:43][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
  [WARN] [2017-12-14 14:22:43][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
  [INFO] [2017-12-14 14:22:44][org.apache.spark.storage.memory.MemoryStore]Block taskresult_3 stored as bytes in memory (estimated size 10.7 MB, free 859.8 MB)
  [INFO] [2017-12-14 14:22:44][org.apache.spark.storage.BlockManagerInfo]Added taskresult_3 in memory on 192.168.18.5:41842 (size: 10.7 MB, free: 860.2 MB)
  [INFO] [2017-12-14 14:22:44][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 11258368 bytes result sent via BlockManager)
  [INFO] [2017-12-14 14:22:44][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.18.5:41842 after 49 ms (0 ms spent in bootstraps)
  [INFO] [2017-12-14 14:22:44][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.18.5:41842 in memory (size: 21.2 KB, free: 860.2 MB)
  [INFO] [2017-12-14 14:22:44][org.apache.spark.ContextCleaner]Cleaned accumulator 76
  [INFO] [2017-12-14 14:22:44][org.apache.spark.ContextCleaner]Cleaned accumulator 74
  [INFO] [2017-12-14 14:22:44][org.apache.spark.ContextCleaner]Cleaned accumulator 72
  [INFO] [2017-12-14 14:22:44][org.apache.spark.ContextCleaner]Cleaned accumulator 75
  [INFO] [2017-12-14 14:22:44][org.apache.spark.ContextCleaner]Cleaned accumulator 73
  [INFO] [2017-12-14 14:22:45][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 2804 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:22:45][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_3 on 192.168.18.5:41842 in memory (size: 10.7 MB, free: 871.0 MB)
  [INFO] [2017-12-14 14:22:45][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:22:45][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (collect at ModelTest.java:91) finished in 2.806 s
  [INFO] [2017-12-14 14:22:45][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: collect at ModelTest.java:91, took 2.856091 s
  [INFO] [2017-12-14 14:22:45][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
  [INFO] [2017-12-14 14:22:45][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@370f532c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:22:45][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:22:45][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
  [INFO] [2017-12-14 14:22:45][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
  [INFO] [2017-12-14 14:22:45][org.apache.spark.storage.BlockManager]BlockManager stopped
  [INFO] [2017-12-14 14:22:45][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
  [INFO] [2017-12-14 14:22:45][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
  [INFO] [2017-12-14 14:22:45][org.apache.spark.SparkContext]Successfully stopped SparkContext
  [INFO] [2017-12-14 14:22:45][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
  [INFO] [2017-12-14 14:22:45][org.apache.spark.util.ShutdownHookManager]Deleting directory /tmp/spark-da799c52-991e-4b22-a26e-ebabc4d4ca49
  [INFO] [2017-12-14 14:24:45][org.apache.spark.SparkContext]Running Spark version 2.2.0
  [WARN] [2017-12-14 14:24:46][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-14 14:24:46][org.apache.spark.util.Utils]Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.18.5 instead (on interface eth0)
  [WARN] [2017-12-14 14:24:46][org.apache.spark.util.Utils]Set SPARK_LOCAL_IP if you need to bind to another address
  [INFO] [2017-12-14 14:24:46][org.apache.spark.SparkContext]Submitted application: JavaKMeansExample
  [INFO] [2017-12-14 14:24:46][org.apache.spark.SecurityManager]Changing view acls to: hadoop
  [INFO] [2017-12-14 14:24:46][org.apache.spark.SecurityManager]Changing modify acls to: hadoop
  [INFO] [2017-12-14 14:24:46][org.apache.spark.SecurityManager]Changing view acls groups to: 
  [INFO] [2017-12-14 14:24:46][org.apache.spark.SecurityManager]Changing modify acls groups to: 
  [INFO] [2017-12-14 14:24:46][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
  [INFO] [2017-12-14 14:24:46][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 39447.
  [INFO] [2017-12-14 14:24:46][org.apache.spark.SparkEnv]Registering MapOutputTracker
  [INFO] [2017-12-14 14:24:46][org.apache.spark.SparkEnv]Registering BlockManagerMaster
  [INFO] [2017-12-14 14:24:46][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
  [INFO] [2017-12-14 14:24:46][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
  [INFO] [2017-12-14 14:24:46][org.apache.spark.storage.DiskBlockManager]Created local directory at /tmp/blockmgr-f2accd3a-9235-457b-a9c1-450116a69d07
  [INFO] [2017-12-14 14:24:46][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 882.6 MB
  [INFO] [2017-12-14 14:24:47][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.util.log]Logging initialized @2542ms
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.Server]Started @2662ms
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@359afa51{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:24:47][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b5176f2{/jobs,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f811d00{/jobs/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4089713{/jobs/job,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@b91d8c4{/jobs/job/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a77614d{/stages,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a067c25{/stages/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bde62ff{/stages/stage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@791cbf87{/stages/stage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@754777cd{/stages/pool,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@372ea2bc{/stages/pool/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f08c4b{/storage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7de0c6ae{/storage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@cdc3aae{/storage/rdd,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dcbb60{/storage/rdd/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21526f6c{/environment,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@299266e2{/environment/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66ea1466{/executors,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bffddff{/executors/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@50687efb{/executors/threadDump,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@142eef62{/executors/threadDump/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5990e6c5{/static,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21694e53{/,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@22c86919{/api,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ea1bcdc{/jobs/job/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64712be{/stages/stage/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:47][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:24:47][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
  [INFO] [2017-12-14 14:24:47][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34849.
  [INFO] [2017-12-14 14:24:47][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.18.5:34849
  [INFO] [2017-12-14 14:24:47][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
  [INFO] [2017-12-14 14:24:47][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.18.5, 34849, None)
  [INFO] [2017-12-14 14:24:47][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.18.5:34849 with 882.6 MB RAM, BlockManagerId(driver, 192.168.18.5, 34849, None)
  [INFO] [2017-12-14 14:24:47][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.18.5, 34849, None)
  [INFO] [2017-12-14 14:24:47][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.18.5, 34849, None)
  [INFO] [2017-12-14 14:24:47][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@13e9f2e2{/metrics/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 209.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:24:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:24:48][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.18.5:34849 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:24:48][org.apache.spark.SparkContext]Created broadcast 0 from textFile at ModelTest.java:44
  [INFO] [2017-12-14 14:24:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 209.1 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:24:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:24:48][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.18.5:34849 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:24:48][org.apache.spark.SparkContext]Created broadcast 1 from textFile at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:24:49][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:24:49][org.apache.spark.SparkContext]Starting job: first at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:24:49][org.apache.spark.scheduler.DAGScheduler]Got job 0 (first at modelSaveLoad.scala:129) with 1 output partitions
  [INFO] [2017-12-14 14:24:49][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (first at modelSaveLoad.scala:129)
  [INFO] [2017-12-14 14:24:49][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:24:49][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:24:49][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[4] at textFile at modelSaveLoad.scala:129), which has no missing parents
  [INFO] [2017-12-14 14:24:49][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:24:49][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1980.0 B, free 882.1 MB)
  [INFO] [2017-12-14 14:24:49][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.18.5:34849 (size: 1980.0 B, free: 882.6 MB)
  [INFO] [2017-12-14 14:24:49][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:24:49][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[4] at textFile at modelSaveLoad.scala:129) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:24:49][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
  [INFO] [2017-12-14 14:24:50][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4926 bytes)
  [INFO] [2017-12-14 14:24:50][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
  [INFO] [2017-12-14 14:24:50][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata/part-00000:0+80
  [INFO] [2017-12-14 14:24:50][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 876 bytes result sent to driver
  [INFO] [2017-12-14 14:24:50][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 441 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:24:50][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (first at modelSaveLoad.scala:129) finished in 0.473 s
  [INFO] [2017-12-14 14:24:50][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:24:50][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: first at modelSaveLoad.scala:129, took 0.737748 s
  [INFO] [2017-12-14 14:24:50][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/').
  [INFO] [2017-12-14 14:24:50][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/'.
  [INFO] [2017-12-14 14:24:50][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@250a9031{/SQL,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:50][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6d67f5eb{/SQL/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:50][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@30ec7d21{/SQL/execution,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:50][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7efa3f63{/SQL/execution/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:50][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f3560d4{/static/sql,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:24:51][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
  [INFO] [2017-12-14 14:24:51][org.apache.spark.SparkContext]Starting job: parquet at KMeansModel.scala:142
  [INFO] [2017-12-14 14:24:51][org.apache.spark.scheduler.DAGScheduler]Got job 1 (parquet at KMeansModel.scala:142) with 1 output partitions
  [INFO] [2017-12-14 14:24:51][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (parquet at KMeansModel.scala:142)
  [INFO] [2017-12-14 14:24:51][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:24:51][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:24:51][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[6] at parquet at KMeansModel.scala:142), which has no missing parents
  [INFO] [2017-12-14 14:24:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 62.1 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:24:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.8 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:24:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.18.5:34849 (size: 21.8 KB, free: 882.5 MB)
  [INFO] [2017-12-14 14:24:51][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:24:51][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at parquet at KMeansModel.scala:142) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:24:51][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
  [INFO] [2017-12-14 14:24:51][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5086 bytes)
  [INFO] [2017-12-14 14:24:51][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
  [INFO] [2017-12-14 14:24:51][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.18.5:34849 in memory (size: 1980.0 B, free: 882.5 MB)
  [INFO] [2017-12-14 14:24:52][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 1687 bytes result sent to driver
  [INFO] [2017-12-14 14:24:52][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 525 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:24:52][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:24:52][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (parquet at KMeansModel.scala:142) finished in 0.526 s
  [INFO] [2017-12-14 14:24:52][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: parquet at KMeansModel.scala:142, took 0.598084 s
  [INFO] [2017-12-14 14:24:52][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.18.5:34849 in memory (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:24:52][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.18.5:34849 in memory (size: 21.8 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:24:54][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
  [INFO] [2017-12-14 14:24:54][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: 
  [INFO] [2017-12-14 14:24:54][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<id: int, point: vector>
  [INFO] [2017-12-14 14:24:54][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
  [INFO] [2017-12-14 14:24:54][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 300.357056 ms
  [INFO] [2017-12-14 14:24:54][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 222.5 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:24:54][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.2 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:24:55][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.18.5:34849 (size: 21.2 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:24:55][org.apache.spark.SparkContext]Created broadcast 4 from rdd at KMeansModel.scala:144
  [INFO] [2017-12-14 14:24:55][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 4195868 bytes, open cost is considered as scanning 4194304 bytes.
  [INFO] [2017-12-14 14:24:55][org.apache.spark.SparkContext]Starting job: collect at KMeansModel.scala:144
  [INFO] [2017-12-14 14:24:55][org.apache.spark.scheduler.DAGScheduler]Got job 2 (collect at KMeansModel.scala:144) with 1 output partitions
  [INFO] [2017-12-14 14:24:55][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (collect at KMeansModel.scala:144)
  [INFO] [2017-12-14 14:24:55][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:24:55][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:24:55][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[11] at map at KMeansModel.scala:144), which has no missing parents
  [INFO] [2017-12-14 14:24:55][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 15.9 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:24:55][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:24:55][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.18.5:34849 (size: 7.3 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:24:55][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:24:55][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at map at KMeansModel.scala:144) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:24:55][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
  [INFO] [2017-12-14 14:24:55][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 5397 bytes)
  [INFO] [2017-12-14 14:24:55][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
  [INFO] [2017-12-14 14:24:55][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 20.510823 ms
  [INFO] [2017-12-14 14:24:55][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/data/part-00000-b27e5343-d95e-472b-9d50-03bfef091743-c000.snappy.parquet, range: 0-1564, partition values: [empty row]
  [INFO] [2017-12-14 14:24:55][org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport]Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  required int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
  [INFO] [2017-12-14 14:24:55][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 92.283978 ms
  [INFO] [2017-12-14 14:24:55][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 24.749258 ms
  [INFO] [2017-12-14 14:24:55][org.apache.parquet.hadoop.InternalParquetRecordReader]RecordReader initialized will read a total of 4 records.
  [INFO] [2017-12-14 14:24:55][org.apache.parquet.hadoop.InternalParquetRecordReader]at row 0. reading next block
  [INFO] [2017-12-14 14:24:55][org.apache.hadoop.io.compress.CodecPool]Got brand-new decompressor [.snappy]
  [INFO] [2017-12-14 14:24:55][org.apache.parquet.hadoop.InternalParquetRecordReader]block read in memory in 44 ms. row count = 4
  [INFO] [2017-12-14 14:24:55][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 1885 bytes result sent to driver
  [INFO] [2017-12-14 14:24:55][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 776 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:24:55][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:24:55][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (collect at KMeansModel.scala:144) finished in 0.777 s
  [INFO] [2017-12-14 14:24:55][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: collect at KMeansModel.scala:144, took 0.816882 s
  [INFO] [2017-12-14 14:24:55][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:24:55][org.apache.spark.SparkContext]Starting job: collect at ModelTest.java:91
  [INFO] [2017-12-14 14:24:55][org.apache.spark.scheduler.DAGScheduler]Got job 3 (collect at ModelTest.java:91) with 1 output partitions
  [INFO] [2017-12-14 14:24:55][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (collect at ModelTest.java:91)
  [INFO] [2017-12-14 14:24:55][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:24:55][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:24:55][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[12] at map at ModelTest.java:91), which has no missing parents
  [INFO] [2017-12-14 14:24:56][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 5.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:24:56][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.0 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:24:56][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.18.5:34849 (size: 3.0 KB, free: 882.5 MB)
  [INFO] [2017-12-14 14:24:56][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:24:56][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[12] at map at ModelTest.java:91) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:24:56][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
  [INFO] [2017-12-14 14:24:56][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4884 bytes)
  [INFO] [2017-12-14 14:24:56][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
  [INFO] [2017-12-14 14:24:56][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/output/clean/JuLei/part-r-00000:0+4881501
  [INFO] [2017-12-14 14:24:56][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.18.5:34849 in memory (size: 7.3 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:24:56][org.apache.spark.storage.memory.MemoryStore]Block rdd_2_0 stored as values in memory (estimated size 11.6 MB, free 870.5 MB)
  [INFO] [2017-12-14 14:24:56][org.apache.spark.storage.BlockManagerInfo]Added rdd_2_0 in memory on 192.168.18.5:34849 (size: 11.6 MB, free: 871.0 MB)
  [WARN] [2017-12-14 14:24:56][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
  [WARN] [2017-12-14 14:24:56][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
  [INFO] [2017-12-14 14:24:57][org.apache.spark.storage.memory.MemoryStore]Block taskresult_3 stored as bytes in memory (estimated size 10.7 MB, free 859.8 MB)
  [INFO] [2017-12-14 14:24:57][org.apache.spark.storage.BlockManagerInfo]Added taskresult_3 in memory on 192.168.18.5:34849 (size: 10.7 MB, free: 860.2 MB)
  [INFO] [2017-12-14 14:24:57][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 11258368 bytes result sent via BlockManager)
  [INFO] [2017-12-14 14:24:57][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.18.5:34849 after 32 ms (0 ms spent in bootstraps)
  [INFO] [2017-12-14 14:24:57][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 1983 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:24:57][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_3 on 192.168.18.5:34849 in memory (size: 10.7 MB, free: 871.0 MB)
  [INFO] [2017-12-14 14:24:57][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (collect at ModelTest.java:91) finished in 1.978 s
  [INFO] [2017-12-14 14:24:57][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: collect at ModelTest.java:91, took 1.999566 s
  [INFO] [2017-12-14 14:24:58][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:24:58][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
  [INFO] [2017-12-14 14:24:58][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@359afa51{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:24:58][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:24:58][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
  [INFO] [2017-12-14 14:24:58][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
  [INFO] [2017-12-14 14:24:58][org.apache.spark.storage.BlockManager]BlockManager stopped
  [INFO] [2017-12-14 14:24:58][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
  [INFO] [2017-12-14 14:24:58][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
  [INFO] [2017-12-14 14:24:58][org.apache.spark.SparkContext]Successfully stopped SparkContext
  [INFO] [2017-12-14 14:24:58][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
  [INFO] [2017-12-14 14:24:58][org.apache.spark.util.ShutdownHookManager]Deleting directory /tmp/spark-f734e1f7-78b0-41c5-b2e6-c422634684fa
  [INFO] [2017-12-14 14:26:26][org.apache.spark.SparkContext]Running Spark version 2.2.0
  [WARN] [2017-12-14 14:26:26][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-14 14:26:26][org.apache.spark.util.Utils]Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.18.5 instead (on interface eth0)
  [WARN] [2017-12-14 14:26:26][org.apache.spark.util.Utils]Set SPARK_LOCAL_IP if you need to bind to another address
  [INFO] [2017-12-14 14:26:26][org.apache.spark.SparkContext]Submitted application: JavaKMeansExample
  [INFO] [2017-12-14 14:26:26][org.apache.spark.SecurityManager]Changing view acls to: hadoop
  [INFO] [2017-12-14 14:26:26][org.apache.spark.SecurityManager]Changing modify acls to: hadoop
  [INFO] [2017-12-14 14:26:26][org.apache.spark.SecurityManager]Changing view acls groups to: 
  [INFO] [2017-12-14 14:26:26][org.apache.spark.SecurityManager]Changing modify acls groups to: 
  [INFO] [2017-12-14 14:26:26][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
  [INFO] [2017-12-14 14:26:27][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 42240.
  [INFO] [2017-12-14 14:26:27][org.apache.spark.SparkEnv]Registering MapOutputTracker
  [INFO] [2017-12-14 14:26:27][org.apache.spark.SparkEnv]Registering BlockManagerMaster
  [INFO] [2017-12-14 14:26:27][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
  [INFO] [2017-12-14 14:26:27][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
  [INFO] [2017-12-14 14:26:27][org.apache.spark.storage.DiskBlockManager]Created local directory at /tmp/blockmgr-60d985b6-650e-43c0-84b4-412d87c46e39
  [INFO] [2017-12-14 14:26:27][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 882.6 MB
  [INFO] [2017-12-14 14:26:27][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.util.log]Logging initialized @2553ms
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.Server]Started @2665ms
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@361c294e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:26:27][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@b672aa8{/jobs,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@62923ee6{/jobs/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@f19c9d2{/jobs/job,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4b6166aa{/jobs/job/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4fd4cae3{/stages,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a1217f9{/stages/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@523424b5{/stages/stage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a7e2d9d{/stages/stage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2b52c0d6{/stages/pool,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4cc76301{/stages/pool/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f19b8b3{/storage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a486d78{/storage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7ef2d7a6{/storage/rdd,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4c36250e{/storage/rdd/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@49f5c307{/environment,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5471388b{/environment/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1601e47{/executors,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66971f6b{/executors/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@517bd097{/executors/threadDump,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a9cc6cb{/executors/threadDump/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@56e07a08{/static,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72b16078{/,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@70fab835{/api,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@759fad4{/jobs/job/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@53499d85{/stages/stage/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:27][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:26:28][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
  [INFO] [2017-12-14 14:26:28][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39337.
  [INFO] [2017-12-14 14:26:28][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.18.5:39337
  [INFO] [2017-12-14 14:26:28][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
  [INFO] [2017-12-14 14:26:28][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.18.5, 39337, None)
  [INFO] [2017-12-14 14:26:28][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.18.5:39337 with 882.6 MB RAM, BlockManagerId(driver, 192.168.18.5, 39337, None)
  [INFO] [2017-12-14 14:26:28][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.18.5, 39337, None)
  [INFO] [2017-12-14 14:26:28][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.18.5, 39337, None)
  [INFO] [2017-12-14 14:26:28][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@46044faa{/metrics/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:28][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 209.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:26:29][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:26:29][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.18.5:39337 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:26:29][org.apache.spark.SparkContext]Created broadcast 0 from textFile at ModelTest.java:44
  [INFO] [2017-12-14 14:26:29][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 209.1 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:26:29][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:26:29][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.18.5:39337 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:26:29][org.apache.spark.SparkContext]Created broadcast 1 from textFile at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:26:30][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:26:30][org.apache.spark.SparkContext]Starting job: first at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:26:30][org.apache.spark.scheduler.DAGScheduler]Got job 0 (first at modelSaveLoad.scala:129) with 1 output partitions
  [INFO] [2017-12-14 14:26:30][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (first at modelSaveLoad.scala:129)
  [INFO] [2017-12-14 14:26:30][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:26:30][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:26:30][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[4] at textFile at modelSaveLoad.scala:129), which has no missing parents
  [INFO] [2017-12-14 14:26:30][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:26:30][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1980.0 B, free 882.1 MB)
  [INFO] [2017-12-14 14:26:30][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.18.5:39337 (size: 1980.0 B, free: 882.6 MB)
  [INFO] [2017-12-14 14:26:30][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:26:30][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[4] at textFile at modelSaveLoad.scala:129) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:26:30][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
  [INFO] [2017-12-14 14:26:30][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4926 bytes)
  [INFO] [2017-12-14 14:26:30][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
  [INFO] [2017-12-14 14:26:31][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata/part-00000:0+80
  [INFO] [2017-12-14 14:26:31][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 919 bytes result sent to driver
  [INFO] [2017-12-14 14:26:31][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 441 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:26:31][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (first at modelSaveLoad.scala:129) finished in 0.502 s
  [INFO] [2017-12-14 14:26:31][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: first at modelSaveLoad.scala:129, took 0.766977 s
  [INFO] [2017-12-14 14:26:31][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:26:31][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/').
  [INFO] [2017-12-14 14:26:31][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/'.
  [INFO] [2017-12-14 14:26:31][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a09303{/SQL,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:31][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@322ba549{/SQL/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:31][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@92d1782{/SQL/execution,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:31][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72976b4{/SQL/execution/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:31][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@27abb6ca{/static/sql,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:26:32][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
  [INFO] [2017-12-14 14:26:32][org.apache.spark.SparkContext]Starting job: parquet at KMeansModel.scala:142
  [INFO] [2017-12-14 14:26:32][org.apache.spark.scheduler.DAGScheduler]Got job 1 (parquet at KMeansModel.scala:142) with 1 output partitions
  [INFO] [2017-12-14 14:26:32][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (parquet at KMeansModel.scala:142)
  [INFO] [2017-12-14 14:26:32][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:26:32][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:26:32][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[6] at parquet at KMeansModel.scala:142), which has no missing parents
  [INFO] [2017-12-14 14:26:32][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 62.1 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:26:32][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.8 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:26:32][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.18.5:39337 (size: 21.8 KB, free: 882.5 MB)
  [INFO] [2017-12-14 14:26:32][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:26:32][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at parquet at KMeansModel.scala:142) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:26:32][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
  [INFO] [2017-12-14 14:26:32][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5086 bytes)
  [INFO] [2017-12-14 14:26:32][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
  [INFO] [2017-12-14 14:26:33][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.18.5:39337 in memory (size: 1980.0 B, free: 882.5 MB)
  [INFO] [2017-12-14 14:26:33][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 1687 bytes result sent to driver
  [INFO] [2017-12-14 14:26:33][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 460 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:26:33][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (parquet at KMeansModel.scala:142) finished in 0.460 s
  [INFO] [2017-12-14 14:26:33][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:26:33][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: parquet at KMeansModel.scala:142, took 0.537271 s
  [INFO] [2017-12-14 14:26:33][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.18.5:39337 in memory (size: 21.8 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:26:33][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.18.5:39337 in memory (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:26:35][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
  [INFO] [2017-12-14 14:26:35][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: 
  [INFO] [2017-12-14 14:26:35][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<id: int, point: vector>
  [INFO] [2017-12-14 14:26:35][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
  [INFO] [2017-12-14 14:26:36][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 268.051614 ms
  [INFO] [2017-12-14 14:26:36][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 222.5 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:26:36][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.2 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:26:36][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.18.5:39337 (size: 21.2 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:26:36][org.apache.spark.SparkContext]Created broadcast 4 from rdd at KMeansModel.scala:144
  [INFO] [2017-12-14 14:26:36][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 4195868 bytes, open cost is considered as scanning 4194304 bytes.
  [INFO] [2017-12-14 14:26:36][org.apache.spark.SparkContext]Starting job: collect at KMeansModel.scala:144
  [INFO] [2017-12-14 14:26:36][org.apache.spark.scheduler.DAGScheduler]Got job 2 (collect at KMeansModel.scala:144) with 1 output partitions
  [INFO] [2017-12-14 14:26:36][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (collect at KMeansModel.scala:144)
  [INFO] [2017-12-14 14:26:36][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:26:36][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:26:36][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[11] at map at KMeansModel.scala:144), which has no missing parents
  [INFO] [2017-12-14 14:26:36][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 15.9 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:26:36][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:26:36][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.18.5:39337 (size: 7.3 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:26:36][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:26:36][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at map at KMeansModel.scala:144) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:26:36][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
  [INFO] [2017-12-14 14:26:36][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 5397 bytes)
  [INFO] [2017-12-14 14:26:36][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
  [INFO] [2017-12-14 14:26:36][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 31.784469 ms
  [INFO] [2017-12-14 14:26:36][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/data/part-00000-b27e5343-d95e-472b-9d50-03bfef091743-c000.snappy.parquet, range: 0-1564, partition values: [empty row]
  [INFO] [2017-12-14 14:26:36][org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport]Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  required int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
  [INFO] [2017-12-14 14:26:36][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 100.439489 ms
  [INFO] [2017-12-14 14:26:36][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 29.119537 ms
  [INFO] [2017-12-14 14:26:36][org.apache.parquet.hadoop.InternalParquetRecordReader]RecordReader initialized will read a total of 4 records.
  [INFO] [2017-12-14 14:26:36][org.apache.parquet.hadoop.InternalParquetRecordReader]at row 0. reading next block
  [INFO] [2017-12-14 14:26:36][org.apache.hadoop.io.compress.CodecPool]Got brand-new decompressor [.snappy]
  [INFO] [2017-12-14 14:26:36][org.apache.parquet.hadoop.InternalParquetRecordReader]block read in memory in 56 ms. row count = 4
  [INFO] [2017-12-14 14:26:37][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 1885 bytes result sent to driver
  [INFO] [2017-12-14 14:26:37][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 894 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:26:37][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (collect at KMeansModel.scala:144) finished in 0.886 s
  [INFO] [2017-12-14 14:26:37][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:26:37][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: collect at KMeansModel.scala:144, took 0.917063 s
  [INFO] [2017-12-14 14:26:37][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:26:37][org.apache.spark.SparkContext]Starting job: collect at ModelTest.java:91
  [INFO] [2017-12-14 14:26:37][org.apache.spark.scheduler.DAGScheduler]Got job 3 (collect at ModelTest.java:91) with 1 output partitions
  [INFO] [2017-12-14 14:26:37][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (collect at ModelTest.java:91)
  [INFO] [2017-12-14 14:26:37][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:26:37][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:26:37][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[12] at map at ModelTest.java:91), which has no missing parents
  [INFO] [2017-12-14 14:26:37][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 5.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:26:37][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.0 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:26:37][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.18.5:39337 (size: 3.0 KB, free: 882.5 MB)
  [INFO] [2017-12-14 14:26:37][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:26:37][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[12] at map at ModelTest.java:91) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:26:37][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
  [INFO] [2017-12-14 14:26:37][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4884 bytes)
  [INFO] [2017-12-14 14:26:37][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
  [INFO] [2017-12-14 14:26:37][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/output/clean/JuLei/part-r-00000:0+4881501
  [INFO] [2017-12-14 14:26:38][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.18.5:39337 in memory (size: 7.3 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:26:38][org.apache.spark.storage.memory.MemoryStore]Block rdd_2_0 stored as values in memory (estimated size 11.6 MB, free 870.5 MB)
  [INFO] [2017-12-14 14:26:38][org.apache.spark.storage.BlockManagerInfo]Added rdd_2_0 in memory on 192.168.18.5:39337 (size: 11.6 MB, free: 871.0 MB)
  [WARN] [2017-12-14 14:26:38][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
  [WARN] [2017-12-14 14:26:38][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
  [INFO] [2017-12-14 14:26:39][org.apache.spark.storage.memory.MemoryStore]Block taskresult_3 stored as bytes in memory (estimated size 10.7 MB, free 859.8 MB)
  [INFO] [2017-12-14 14:26:39][org.apache.spark.storage.BlockManagerInfo]Added taskresult_3 in memory on 192.168.18.5:39337 (size: 10.7 MB, free: 860.2 MB)
  [INFO] [2017-12-14 14:26:39][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 11258368 bytes result sent via BlockManager)
  [INFO] [2017-12-14 14:26:39][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.18.5:39337 after 33 ms (0 ms spent in bootstraps)
  [INFO] [2017-12-14 14:26:39][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_3 on 192.168.18.5:39337 in memory (size: 10.7 MB, free: 871.0 MB)
  [INFO] [2017-12-14 14:26:39][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 2311 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:26:39][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (collect at ModelTest.java:91) finished in 2.318 s
  [INFO] [2017-12-14 14:26:39][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: collect at ModelTest.java:91, took 2.335861 s
  [INFO] [2017-12-14 14:26:39][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:26:39][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
  [INFO] [2017-12-14 14:26:39][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@361c294e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:26:39][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:26:39][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
  [INFO] [2017-12-14 14:26:39][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
  [INFO] [2017-12-14 14:26:39][org.apache.spark.storage.BlockManager]BlockManager stopped
  [INFO] [2017-12-14 14:26:39][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
  [INFO] [2017-12-14 14:26:39][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
  [INFO] [2017-12-14 14:26:39][org.apache.spark.SparkContext]Successfully stopped SparkContext
  [INFO] [2017-12-14 14:26:39][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
  [INFO] [2017-12-14 14:26:39][org.apache.spark.util.ShutdownHookManager]Deleting directory /tmp/spark-52f689f8-093f-4a79-a0a8-d0c7d498c250
  [INFO] [2017-12-14 14:28:41][org.apache.spark.SparkContext]Running Spark version 2.2.0
  [WARN] [2017-12-14 14:28:41][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-14 14:28:41][org.apache.spark.util.Utils]Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.18.5 instead (on interface eth0)
  [WARN] [2017-12-14 14:28:41][org.apache.spark.util.Utils]Set SPARK_LOCAL_IP if you need to bind to another address
  [INFO] [2017-12-14 14:28:41][org.apache.spark.SparkContext]Submitted application: JavaKMeansExample
  [INFO] [2017-12-14 14:28:41][org.apache.spark.SecurityManager]Changing view acls to: hadoop
  [INFO] [2017-12-14 14:28:41][org.apache.spark.SecurityManager]Changing modify acls to: hadoop
  [INFO] [2017-12-14 14:28:41][org.apache.spark.SecurityManager]Changing view acls groups to: 
  [INFO] [2017-12-14 14:28:41][org.apache.spark.SecurityManager]Changing modify acls groups to: 
  [INFO] [2017-12-14 14:28:41][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
  [INFO] [2017-12-14 14:28:42][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 40584.
  [INFO] [2017-12-14 14:28:42][org.apache.spark.SparkEnv]Registering MapOutputTracker
  [INFO] [2017-12-14 14:28:42][org.apache.spark.SparkEnv]Registering BlockManagerMaster
  [INFO] [2017-12-14 14:28:42][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
  [INFO] [2017-12-14 14:28:42][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
  [INFO] [2017-12-14 14:28:42][org.apache.spark.storage.DiskBlockManager]Created local directory at /tmp/blockmgr-ae823d55-87f3-41fe-af8d-74c2de809000
  [INFO] [2017-12-14 14:28:42][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 882.6 MB
  [INFO] [2017-12-14 14:28:42][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.util.log]Logging initialized @2876ms
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.Server]Started @3003ms
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@370f532c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:28:42][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b5176f2{/jobs,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f811d00{/jobs/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4089713{/jobs/job,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@b91d8c4{/jobs/job/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a77614d{/stages,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a067c25{/stages/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bde62ff{/stages/stage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@791cbf87{/stages/stage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@754777cd{/stages/pool,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@372ea2bc{/stages/pool/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f08c4b{/storage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7de0c6ae{/storage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@cdc3aae{/storage/rdd,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dcbb60{/storage/rdd/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21526f6c{/environment,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@299266e2{/environment/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66ea1466{/executors,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bffddff{/executors/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@50687efb{/executors/threadDump,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@142eef62{/executors/threadDump/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5990e6c5{/static,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21694e53{/,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@22c86919{/api,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ea1bcdc{/jobs/job/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64712be{/stages/stage/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:42][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:28:43][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
  [INFO] [2017-12-14 14:28:43][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46607.
  [INFO] [2017-12-14 14:28:43][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.18.5:46607
  [INFO] [2017-12-14 14:28:43][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
  [INFO] [2017-12-14 14:28:43][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.18.5, 46607, None)
  [INFO] [2017-12-14 14:28:43][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.18.5:46607 with 882.6 MB RAM, BlockManagerId(driver, 192.168.18.5, 46607, None)
  [INFO] [2017-12-14 14:28:43][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.18.5, 46607, None)
  [INFO] [2017-12-14 14:28:43][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.18.5, 46607, None)
  [INFO] [2017-12-14 14:28:43][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@13e9f2e2{/metrics/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:44][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 209.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:28:44][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:28:44][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.18.5:46607 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:28:44][org.apache.spark.SparkContext]Created broadcast 0 from textFile at ModelTest.java:44
  [INFO] [2017-12-14 14:28:44][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 209.1 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:28:44][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:28:44][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.18.5:46607 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:28:44][org.apache.spark.SparkContext]Created broadcast 1 from textFile at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:28:45][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:28:46][org.apache.spark.SparkContext]Starting job: first at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:28:46][org.apache.spark.scheduler.DAGScheduler]Got job 0 (first at modelSaveLoad.scala:129) with 1 output partitions
  [INFO] [2017-12-14 14:28:46][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (first at modelSaveLoad.scala:129)
  [INFO] [2017-12-14 14:28:46][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:28:46][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:28:46][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[4] at textFile at modelSaveLoad.scala:129), which has no missing parents
  [INFO] [2017-12-14 14:28:46][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:28:46][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1980.0 B, free 882.1 MB)
  [INFO] [2017-12-14 14:28:46][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.18.5:46607 (size: 1980.0 B, free: 882.6 MB)
  [INFO] [2017-12-14 14:28:46][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:28:46][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[4] at textFile at modelSaveLoad.scala:129) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:28:46][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
  [INFO] [2017-12-14 14:28:46][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4926 bytes)
  [INFO] [2017-12-14 14:28:46][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
  [INFO] [2017-12-14 14:28:46][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata/part-00000:0+80
  [INFO] [2017-12-14 14:28:46][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 919 bytes result sent to driver
  [INFO] [2017-12-14 14:28:46][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 358 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:28:46][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (first at modelSaveLoad.scala:129) finished in 0.405 s
  [INFO] [2017-12-14 14:28:46][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:28:46][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: first at modelSaveLoad.scala:129, took 0.710027 s
  [INFO] [2017-12-14 14:28:46][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/').
  [INFO] [2017-12-14 14:28:46][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/'.
  [INFO] [2017-12-14 14:28:46][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@45ab3bdd{/SQL,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:46][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1642eeae{/SQL/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:46][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@322ba549{/SQL/execution,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:46][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5416f8db{/SQL/execution/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:46][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@726934e2{/static/sql,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:28:47][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
  [INFO] [2017-12-14 14:28:47][org.apache.spark.SparkContext]Starting job: parquet at KMeansModel.scala:142
  [INFO] [2017-12-14 14:28:47][org.apache.spark.scheduler.DAGScheduler]Got job 1 (parquet at KMeansModel.scala:142) with 1 output partitions
  [INFO] [2017-12-14 14:28:47][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (parquet at KMeansModel.scala:142)
  [INFO] [2017-12-14 14:28:47][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:28:47][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:28:47][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[6] at parquet at KMeansModel.scala:142), which has no missing parents
  [INFO] [2017-12-14 14:28:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 62.1 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:28:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.8 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:28:48][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.18.5:46607 (size: 21.8 KB, free: 882.5 MB)
  [INFO] [2017-12-14 14:28:48][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:28:48][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at parquet at KMeansModel.scala:142) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:28:48][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
  [INFO] [2017-12-14 14:28:48][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5086 bytes)
  [INFO] [2017-12-14 14:28:48][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
  [INFO] [2017-12-14 14:28:48][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.18.5:46607 in memory (size: 1980.0 B, free: 882.5 MB)
  [INFO] [2017-12-14 14:28:48][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 1687 bytes result sent to driver
  [INFO] [2017-12-14 14:28:48][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 509 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:28:48][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:28:48][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (parquet at KMeansModel.scala:142) finished in 0.512 s
  [INFO] [2017-12-14 14:28:48][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: parquet at KMeansModel.scala:142, took 0.592532 s
  [INFO] [2017-12-14 14:28:49][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.18.5:46607 in memory (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:28:49][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.18.5:46607 in memory (size: 21.8 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:28:50][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
  [INFO] [2017-12-14 14:28:50][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: 
  [INFO] [2017-12-14 14:28:50][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<id: int, point: vector>
  [INFO] [2017-12-14 14:28:50][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
  [INFO] [2017-12-14 14:28:51][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 313.50928 ms
  [INFO] [2017-12-14 14:28:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 222.5 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:28:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.2 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:28:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.18.5:46607 (size: 21.2 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:28:51][org.apache.spark.SparkContext]Created broadcast 4 from rdd at KMeansModel.scala:144
  [INFO] [2017-12-14 14:28:51][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 4195868 bytes, open cost is considered as scanning 4194304 bytes.
  [INFO] [2017-12-14 14:28:51][org.apache.spark.SparkContext]Starting job: collect at KMeansModel.scala:144
  [INFO] [2017-12-14 14:28:51][org.apache.spark.scheduler.DAGScheduler]Got job 2 (collect at KMeansModel.scala:144) with 1 output partitions
  [INFO] [2017-12-14 14:28:51][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (collect at KMeansModel.scala:144)
  [INFO] [2017-12-14 14:28:51][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:28:51][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:28:51][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[11] at map at KMeansModel.scala:144), which has no missing parents
  [INFO] [2017-12-14 14:28:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 15.9 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:28:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:28:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.18.5:46607 (size: 7.3 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:28:51][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:28:51][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at map at KMeansModel.scala:144) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:28:51][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
  [INFO] [2017-12-14 14:28:51][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 5397 bytes)
  [INFO] [2017-12-14 14:28:51][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
  [INFO] [2017-12-14 14:28:51][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 45.418329 ms
  [INFO] [2017-12-14 14:28:51][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/data/part-00000-b27e5343-d95e-472b-9d50-03bfef091743-c000.snappy.parquet, range: 0-1564, partition values: [empty row]
  [INFO] [2017-12-14 14:28:51][org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport]Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  required int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
  [INFO] [2017-12-14 14:28:52][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 59.597316 ms
  [INFO] [2017-12-14 14:28:52][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 21.372336 ms
  [INFO] [2017-12-14 14:28:52][org.apache.parquet.hadoop.InternalParquetRecordReader]RecordReader initialized will read a total of 4 records.
  [INFO] [2017-12-14 14:28:52][org.apache.parquet.hadoop.InternalParquetRecordReader]at row 0. reading next block
  [INFO] [2017-12-14 14:28:52][org.apache.hadoop.io.compress.CodecPool]Got brand-new decompressor [.snappy]
  [INFO] [2017-12-14 14:28:52][org.apache.parquet.hadoop.InternalParquetRecordReader]block read in memory in 34 ms. row count = 4
  [INFO] [2017-12-14 14:28:52][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 1885 bytes result sent to driver
  [INFO] [2017-12-14 14:28:52][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 552 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:28:52][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:28:52][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (collect at KMeansModel.scala:144) finished in 0.556 s
  [INFO] [2017-12-14 14:28:52][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: collect at KMeansModel.scala:144, took 0.582499 s
  [INFO] [2017-12-14 14:28:52][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:28:52][org.apache.spark.SparkContext]Starting job: collect at ModelTest.java:91
  [INFO] [2017-12-14 14:28:52][org.apache.spark.scheduler.DAGScheduler]Got job 3 (collect at ModelTest.java:91) with 1 output partitions
  [INFO] [2017-12-14 14:28:52][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (collect at ModelTest.java:91)
  [INFO] [2017-12-14 14:28:52][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:28:52][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:28:52][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[12] at map at ModelTest.java:91), which has no missing parents
  [INFO] [2017-12-14 14:28:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 5.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:28:52][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.0 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:28:52][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.18.5:46607 (size: 3.0 KB, free: 882.5 MB)
  [INFO] [2017-12-14 14:28:52][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:28:52][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[12] at map at ModelTest.java:91) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:28:52][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
  [INFO] [2017-12-14 14:28:52][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4884 bytes)
  [INFO] [2017-12-14 14:28:52][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
  [INFO] [2017-12-14 14:28:52][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/output/clean/JuLei/part-r-00000:0+4881501
  [INFO] [2017-12-14 14:28:53][org.apache.spark.storage.memory.MemoryStore]Block rdd_2_0 stored as values in memory (estimated size 11.6 MB, free 870.5 MB)
  [INFO] [2017-12-14 14:28:53][org.apache.spark.storage.BlockManagerInfo]Added rdd_2_0 in memory on 192.168.18.5:46607 (size: 11.6 MB, free: 870.9 MB)
  [WARN] [2017-12-14 14:28:53][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
  [WARN] [2017-12-14 14:28:53][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
  [INFO] [2017-12-14 14:28:53][org.apache.spark.storage.memory.MemoryStore]Block taskresult_3 stored as bytes in memory (estimated size 10.7 MB, free 859.8 MB)
  [INFO] [2017-12-14 14:28:53][org.apache.spark.storage.BlockManagerInfo]Added taskresult_3 in memory on 192.168.18.5:46607 (size: 10.7 MB, free: 860.2 MB)
  [INFO] [2017-12-14 14:28:53][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 11258368 bytes result sent via BlockManager)
  [INFO] [2017-12-14 14:28:54][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.18.5:46607 after 32 ms (0 ms spent in bootstraps)
  [INFO] [2017-12-14 14:28:54][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_3 on 192.168.18.5:46607 in memory (size: 10.7 MB, free: 870.9 MB)
  [INFO] [2017-12-14 14:28:54][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 1998 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:28:54][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:28:54][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (collect at ModelTest.java:91) finished in 1.999 s
  [INFO] [2017-12-14 14:28:54][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: collect at ModelTest.java:91, took 2.022864 s
  [INFO] [2017-12-14 14:28:54][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
  [INFO] [2017-12-14 14:28:54][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@370f532c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:28:54][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:28:54][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
  [INFO] [2017-12-14 14:28:54][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
  [INFO] [2017-12-14 14:28:54][org.apache.spark.storage.BlockManager]BlockManager stopped
  [INFO] [2017-12-14 14:28:54][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
  [INFO] [2017-12-14 14:28:54][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
  [INFO] [2017-12-14 14:28:54][org.apache.spark.SparkContext]Successfully stopped SparkContext
  [INFO] [2017-12-14 14:28:54][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
  [INFO] [2017-12-14 14:28:54][org.apache.spark.util.ShutdownHookManager]Deleting directory /tmp/spark-a272034c-908d-449c-99d1-5df82c090858
  [INFO] [2017-12-14 14:29:24][org.apache.spark.SparkContext]Running Spark version 2.2.0
  [WARN] [2017-12-14 14:29:25][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-14 14:29:25][org.apache.spark.util.Utils]Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.18.5 instead (on interface eth0)
  [WARN] [2017-12-14 14:29:25][org.apache.spark.util.Utils]Set SPARK_LOCAL_IP if you need to bind to another address
  [INFO] [2017-12-14 14:29:25][org.apache.spark.SparkContext]Submitted application: JavaKMeansExample
  [INFO] [2017-12-14 14:29:25][org.apache.spark.SecurityManager]Changing view acls to: hadoop
  [INFO] [2017-12-14 14:29:25][org.apache.spark.SecurityManager]Changing modify acls to: hadoop
  [INFO] [2017-12-14 14:29:25][org.apache.spark.SecurityManager]Changing view acls groups to: 
  [INFO] [2017-12-14 14:29:25][org.apache.spark.SecurityManager]Changing modify acls groups to: 
  [INFO] [2017-12-14 14:29:25][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
  [INFO] [2017-12-14 14:29:26][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 42945.
  [INFO] [2017-12-14 14:29:26][org.apache.spark.SparkEnv]Registering MapOutputTracker
  [INFO] [2017-12-14 14:29:26][org.apache.spark.SparkEnv]Registering BlockManagerMaster
  [INFO] [2017-12-14 14:29:26][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
  [INFO] [2017-12-14 14:29:26][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
  [INFO] [2017-12-14 14:29:26][org.apache.spark.storage.DiskBlockManager]Created local directory at /tmp/blockmgr-5000e738-66f1-4876-bf5c-619778354a15
  [INFO] [2017-12-14 14:29:26][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 882.6 MB
  [INFO] [2017-12-14 14:29:26][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
  [INFO] [2017-12-14 14:29:26][org.spark_project.jetty.util.log]Logging initialized @4300ms
  [INFO] [2017-12-14 14:29:26][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.Server]Started @4695ms
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@1ee60a06{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:29:27][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b5176f2{/jobs,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f811d00{/jobs/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4089713{/jobs/job,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@b91d8c4{/jobs/job/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a77614d{/stages,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a067c25{/stages/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bde62ff{/stages/stage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@791cbf87{/stages/stage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@754777cd{/stages/pool,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@372ea2bc{/stages/pool/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f08c4b{/storage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7de0c6ae{/storage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@cdc3aae{/storage/rdd,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dcbb60{/storage/rdd/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21526f6c{/environment,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@299266e2{/environment/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66ea1466{/executors,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bffddff{/executors/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@50687efb{/executors/threadDump,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@142eef62{/executors/threadDump/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5990e6c5{/static,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21694e53{/,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@22c86919{/api,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ea1bcdc{/jobs/job/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64712be{/stages/stage/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:27][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:29:27][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
  [INFO] [2017-12-14 14:29:28][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38625.
  [INFO] [2017-12-14 14:29:28][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.18.5:38625
  [INFO] [2017-12-14 14:29:28][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
  [INFO] [2017-12-14 14:29:28][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.18.5, 38625, None)
  [INFO] [2017-12-14 14:29:28][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.18.5:38625 with 882.6 MB RAM, BlockManagerId(driver, 192.168.18.5, 38625, None)
  [INFO] [2017-12-14 14:29:28][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.18.5, 38625, None)
  [INFO] [2017-12-14 14:29:28][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.18.5, 38625, None)
  [INFO] [2017-12-14 14:29:28][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1532c619{/metrics/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:29][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 209.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:29:29][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:29:29][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.18.5:38625 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:29:29][org.apache.spark.SparkContext]Created broadcast 0 from textFile at ModelTest.java:44
  [INFO] [2017-12-14 14:29:30][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 209.1 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:29:30][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:29:30][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.18.5:38625 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:29:30][org.apache.spark.SparkContext]Created broadcast 1 from textFile at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:29:32][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:29:32][org.apache.spark.SparkContext]Starting job: first at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:29:32][org.apache.spark.scheduler.DAGScheduler]Got job 0 (first at modelSaveLoad.scala:129) with 1 output partitions
  [INFO] [2017-12-14 14:29:32][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (first at modelSaveLoad.scala:129)
  [INFO] [2017-12-14 14:29:32][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:29:32][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:29:32][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[4] at textFile at modelSaveLoad.scala:129), which has no missing parents
  [INFO] [2017-12-14 14:29:32][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:29:32][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1980.0 B, free 882.1 MB)
  [INFO] [2017-12-14 14:29:32][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.18.5:38625 (size: 1980.0 B, free: 882.6 MB)
  [INFO] [2017-12-14 14:29:32][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:29:32][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[4] at textFile at modelSaveLoad.scala:129) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:29:32][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
  [INFO] [2017-12-14 14:29:32][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4926 bytes)
  [INFO] [2017-12-14 14:29:32][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
  [INFO] [2017-12-14 14:29:33][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata/part-00000:0+80
  [INFO] [2017-12-14 14:29:33][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 876 bytes result sent to driver
  [INFO] [2017-12-14 14:29:33][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 1080 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:29:33][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (first at modelSaveLoad.scala:129) finished in 1.234 s
  [INFO] [2017-12-14 14:29:33][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:29:33][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: first at modelSaveLoad.scala:129, took 1.476997 s
  [INFO] [2017-12-14 14:29:34][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/').
  [INFO] [2017-12-14 14:29:34][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/'.
  [INFO] [2017-12-14 14:29:34][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1163a27{/SQL,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:34][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@502a4156{/SQL/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:34][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6a1ef65c{/SQL/execution,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:34][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@726934e2{/SQL/execution/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:34][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@696db620{/static/sql,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:29:34][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
  [INFO] [2017-12-14 14:29:35][org.apache.spark.SparkContext]Starting job: parquet at KMeansModel.scala:142
  [INFO] [2017-12-14 14:29:35][org.apache.spark.scheduler.DAGScheduler]Got job 1 (parquet at KMeansModel.scala:142) with 1 output partitions
  [INFO] [2017-12-14 14:29:35][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (parquet at KMeansModel.scala:142)
  [INFO] [2017-12-14 14:29:35][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:29:35][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:29:35][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[6] at parquet at KMeansModel.scala:142), which has no missing parents
  [INFO] [2017-12-14 14:29:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 62.1 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:29:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.8 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:29:35][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.18.5:38625 (size: 21.8 KB, free: 882.5 MB)
  [INFO] [2017-12-14 14:29:35][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:29:35][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at parquet at KMeansModel.scala:142) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:29:35][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
  [INFO] [2017-12-14 14:29:35][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5086 bytes)
  [INFO] [2017-12-14 14:29:35][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
  [INFO] [2017-12-14 14:29:36][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.18.5:38625 in memory (size: 1980.0 B, free: 882.5 MB)
  [INFO] [2017-12-14 14:29:36][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 1687 bytes result sent to driver
  [INFO] [2017-12-14 14:29:36][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 991 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:29:36][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:29:36][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (parquet at KMeansModel.scala:142) finished in 0.984 s
  [INFO] [2017-12-14 14:29:36][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: parquet at KMeansModel.scala:142, took 1.094740 s
  [INFO] [2017-12-14 14:29:37][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.18.5:38625 in memory (size: 21.8 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:29:37][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.18.5:38625 in memory (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:29:40][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
  [INFO] [2017-12-14 14:29:40][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: 
  [INFO] [2017-12-14 14:29:40][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<id: int, point: vector>
  [INFO] [2017-12-14 14:29:40][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
  [INFO] [2017-12-14 14:29:41][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 304.572704 ms
  [INFO] [2017-12-14 14:29:41][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 222.5 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:29:41][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.2 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:29:41][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.18.5:38625 (size: 21.2 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:29:41][org.apache.spark.SparkContext]Created broadcast 4 from rdd at KMeansModel.scala:144
  [INFO] [2017-12-14 14:29:41][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 4195868 bytes, open cost is considered as scanning 4194304 bytes.
  [INFO] [2017-12-14 14:29:42][org.apache.spark.SparkContext]Starting job: collect at KMeansModel.scala:144
  [INFO] [2017-12-14 14:29:42][org.apache.spark.scheduler.DAGScheduler]Got job 2 (collect at KMeansModel.scala:144) with 1 output partitions
  [INFO] [2017-12-14 14:29:42][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (collect at KMeansModel.scala:144)
  [INFO] [2017-12-14 14:29:42][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:29:42][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:29:42][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[11] at map at KMeansModel.scala:144), which has no missing parents
  [INFO] [2017-12-14 14:29:42][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 15.9 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:29:42][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:29:42][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.18.5:38625 (size: 7.3 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:29:42][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:29:42][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at map at KMeansModel.scala:144) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:29:42][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
  [INFO] [2017-12-14 14:29:42][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 5397 bytes)
  [INFO] [2017-12-14 14:29:42][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
  [INFO] [2017-12-14 14:29:42][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 172.18736 ms
  [INFO] [2017-12-14 14:29:42][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/data/part-00000-b27e5343-d95e-472b-9d50-03bfef091743-c000.snappy.parquet, range: 0-1564, partition values: [empty row]
  [INFO] [2017-12-14 14:29:42][org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport]Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  required int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
  [INFO] [2017-12-14 14:29:43][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 126.581456 ms
  [INFO] [2017-12-14 14:29:43][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 44.196529 ms
  [INFO] [2017-12-14 14:29:43][org.apache.parquet.hadoop.InternalParquetRecordReader]RecordReader initialized will read a total of 4 records.
  [INFO] [2017-12-14 14:29:43][org.apache.parquet.hadoop.InternalParquetRecordReader]at row 0. reading next block
  [INFO] [2017-12-14 14:29:43][org.apache.hadoop.io.compress.CodecPool]Got brand-new decompressor [.snappy]
  [INFO] [2017-12-14 14:29:43][org.apache.parquet.hadoop.InternalParquetRecordReader]block read in memory in 46 ms. row count = 4
  [INFO] [2017-12-14 14:29:43][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 1885 bytes result sent to driver
  [INFO] [2017-12-14 14:29:43][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 1268 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:29:43][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:29:43][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (collect at KMeansModel.scala:144) finished in 1.256 s
  [INFO] [2017-12-14 14:29:43][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: collect at KMeansModel.scala:144, took 1.307241 s
  [INFO] [2017-12-14 14:29:43][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:29:43][org.apache.spark.SparkContext]Starting job: collect at ModelTest.java:91
  [INFO] [2017-12-14 14:29:43][org.apache.spark.scheduler.DAGScheduler]Got job 3 (collect at ModelTest.java:91) with 1 output partitions
  [INFO] [2017-12-14 14:29:43][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (collect at ModelTest.java:91)
  [INFO] [2017-12-14 14:29:43][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:29:43][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:29:43][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[12] at map at ModelTest.java:91), which has no missing parents
  [INFO] [2017-12-14 14:29:43][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 5.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:29:43][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.0 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:29:43][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.18.5:38625 (size: 3.0 KB, free: 882.5 MB)
  [INFO] [2017-12-14 14:29:43][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:29:43][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[12] at map at ModelTest.java:91) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:29:43][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
  [INFO] [2017-12-14 14:29:43][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4884 bytes)
  [INFO] [2017-12-14 14:29:43][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
  [INFO] [2017-12-14 14:29:43][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/output/clean/JuLei/part-r-00000:0+4881501
  [INFO] [2017-12-14 14:29:44][org.apache.spark.storage.memory.MemoryStore]Block rdd_2_0 stored as values in memory (estimated size 11.6 MB, free 870.5 MB)
  [INFO] [2017-12-14 14:29:44][org.apache.spark.storage.BlockManagerInfo]Added rdd_2_0 in memory on 192.168.18.5:38625 (size: 11.6 MB, free: 870.9 MB)
  [WARN] [2017-12-14 14:29:44][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
  [WARN] [2017-12-14 14:29:44][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
  [INFO] [2017-12-14 14:29:45][org.apache.spark.storage.memory.MemoryStore]Block taskresult_3 stored as bytes in memory (estimated size 10.7 MB, free 859.8 MB)
  [INFO] [2017-12-14 14:29:45][org.apache.spark.storage.BlockManagerInfo]Added taskresult_3 in memory on 192.168.18.5:38625 (size: 10.7 MB, free: 860.2 MB)
  [INFO] [2017-12-14 14:29:45][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 11258368 bytes result sent via BlockManager)
  [INFO] [2017-12-14 14:29:46][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.18.5:38625 after 106 ms (0 ms spent in bootstraps)
  [INFO] [2017-12-14 14:29:47][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.18.5:38625 in memory (size: 7.3 KB, free: 860.2 MB)
  [INFO] [2017-12-14 14:29:47][org.apache.spark.ContextCleaner]Cleaned accumulator 75
  [INFO] [2017-12-14 14:29:47][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.18.5:38625 in memory (size: 21.2 KB, free: 860.2 MB)
  [INFO] [2017-12-14 14:29:47][org.apache.spark.ContextCleaner]Cleaned accumulator 72
  [INFO] [2017-12-14 14:29:47][org.apache.spark.ContextCleaner]Cleaned accumulator 73
  [INFO] [2017-12-14 14:29:47][org.apache.spark.ContextCleaner]Cleaned accumulator 74
  [INFO] [2017-12-14 14:29:47][org.apache.spark.ContextCleaner]Cleaned accumulator 76
  [INFO] [2017-12-14 14:29:47][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_3 on 192.168.18.5:38625 in memory (size: 10.7 MB, free: 871.0 MB)
  [INFO] [2017-12-14 14:29:47][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 3900 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:29:47][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:29:47][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (collect at ModelTest.java:91) finished in 3.901 s
  [INFO] [2017-12-14 14:29:47][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: collect at ModelTest.java:91, took 3.917832 s
  [INFO] [2017-12-14 14:29:47][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
  [INFO] [2017-12-14 14:29:47][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@1ee60a06{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:29:47][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:29:47][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
  [INFO] [2017-12-14 14:29:47][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
  [INFO] [2017-12-14 14:29:47][org.apache.spark.storage.BlockManager]BlockManager stopped
  [INFO] [2017-12-14 14:29:47][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
  [INFO] [2017-12-14 14:29:47][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
  [INFO] [2017-12-14 14:29:47][org.apache.spark.SparkContext]Successfully stopped SparkContext
  [INFO] [2017-12-14 14:29:47][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
  [INFO] [2017-12-14 14:29:47][org.apache.spark.util.ShutdownHookManager]Deleting directory /tmp/spark-c8b8dfa4-7326-4949-932c-a170a9109ea0
  [INFO] [2017-12-14 14:31:54][org.apache.spark.SparkContext]Running Spark version 2.2.0
  [WARN] [2017-12-14 14:31:55][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-14 14:31:55][org.apache.spark.util.Utils]Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.18.5 instead (on interface eth0)
  [WARN] [2017-12-14 14:31:55][org.apache.spark.util.Utils]Set SPARK_LOCAL_IP if you need to bind to another address
  [INFO] [2017-12-14 14:31:55][org.apache.spark.SparkContext]Submitted application: JavaKMeansExample
  [INFO] [2017-12-14 14:31:55][org.apache.spark.SecurityManager]Changing view acls to: hadoop
  [INFO] [2017-12-14 14:31:55][org.apache.spark.SecurityManager]Changing modify acls to: hadoop
  [INFO] [2017-12-14 14:31:55][org.apache.spark.SecurityManager]Changing view acls groups to: 
  [INFO] [2017-12-14 14:31:55][org.apache.spark.SecurityManager]Changing modify acls groups to: 
  [INFO] [2017-12-14 14:31:55][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
  [INFO] [2017-12-14 14:31:55][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 38271.
  [INFO] [2017-12-14 14:31:55][org.apache.spark.SparkEnv]Registering MapOutputTracker
  [INFO] [2017-12-14 14:31:55][org.apache.spark.SparkEnv]Registering BlockManagerMaster
  [INFO] [2017-12-14 14:31:55][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
  [INFO] [2017-12-14 14:31:55][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
  [INFO] [2017-12-14 14:31:55][org.apache.spark.storage.DiskBlockManager]Created local directory at /tmp/blockmgr-4ac50a68-b412-4cf4-9873-cc0edecad5fc
  [INFO] [2017-12-14 14:31:56][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 882.6 MB
  [INFO] [2017-12-14 14:31:56][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.util.log]Logging initialized @2550ms
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.Server]Started @2685ms
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@4175a2a2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:31:56][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b5176f2{/jobs,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f811d00{/jobs/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4089713{/jobs/job,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@b91d8c4{/jobs/job/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a77614d{/stages,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a067c25{/stages/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bde62ff{/stages/stage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@791cbf87{/stages/stage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@754777cd{/stages/pool,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@372ea2bc{/stages/pool/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f08c4b{/storage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7de0c6ae{/storage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@cdc3aae{/storage/rdd,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dcbb60{/storage/rdd/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21526f6c{/environment,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@299266e2{/environment/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66ea1466{/executors,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bffddff{/executors/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@50687efb{/executors/threadDump,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@142eef62{/executors/threadDump/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5990e6c5{/static,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21694e53{/,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@22c86919{/api,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ea1bcdc{/jobs/job/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64712be{/stages/stage/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:56][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:31:56][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
  [INFO] [2017-12-14 14:31:56][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39795.
  [INFO] [2017-12-14 14:31:56][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.18.5:39795
  [INFO] [2017-12-14 14:31:56][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
  [INFO] [2017-12-14 14:31:56][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.18.5, 39795, None)
  [INFO] [2017-12-14 14:31:56][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.18.5:39795 with 882.6 MB RAM, BlockManagerId(driver, 192.168.18.5, 39795, None)
  [INFO] [2017-12-14 14:31:56][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.18.5, 39795, None)
  [INFO] [2017-12-14 14:31:56][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.18.5, 39795, None)
  [INFO] [2017-12-14 14:31:56][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@13e9f2e2{/metrics/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:57][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 209.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:31:57][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:31:57][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.18.5:39795 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:31:57][org.apache.spark.SparkContext]Created broadcast 0 from textFile at ModelTest.java:44
  [INFO] [2017-12-14 14:31:57][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 209.1 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:31:57][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:31:57][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.18.5:39795 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:31:57][org.apache.spark.SparkContext]Created broadcast 1 from textFile at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:31:58][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:31:58][org.apache.spark.SparkContext]Starting job: first at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:31:58][org.apache.spark.scheduler.DAGScheduler]Got job 0 (first at modelSaveLoad.scala:129) with 1 output partitions
  [INFO] [2017-12-14 14:31:58][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (first at modelSaveLoad.scala:129)
  [INFO] [2017-12-14 14:31:58][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:31:58][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:31:58][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[4] at textFile at modelSaveLoad.scala:129), which has no missing parents
  [INFO] [2017-12-14 14:31:58][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:31:58][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1980.0 B, free 882.1 MB)
  [INFO] [2017-12-14 14:31:58][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.18.5:39795 (size: 1980.0 B, free: 882.6 MB)
  [INFO] [2017-12-14 14:31:58][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:31:58][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[4] at textFile at modelSaveLoad.scala:129) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:31:58][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
  [INFO] [2017-12-14 14:31:59][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4926 bytes)
  [INFO] [2017-12-14 14:31:59][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
  [INFO] [2017-12-14 14:31:59][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata/part-00000:0+80
  [INFO] [2017-12-14 14:31:59][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 876 bytes result sent to driver
  [INFO] [2017-12-14 14:31:59][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 404 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:31:59][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:31:59][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (first at modelSaveLoad.scala:129) finished in 0.458 s
  [INFO] [2017-12-14 14:31:59][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: first at modelSaveLoad.scala:129, took 0.671981 s
  [INFO] [2017-12-14 14:31:59][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/').
  [INFO] [2017-12-14 14:31:59][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/'.
  [INFO] [2017-12-14 14:31:59][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e1fd62b{/SQL,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:59][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@558b4942{/SQL/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:59][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2262d6d5{/SQL/execution,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:59][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ff0b1cc{/SQL/execution/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:31:59][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6088451e{/static/sql,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:32:00][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
  [INFO] [2017-12-14 14:32:00][org.apache.spark.SparkContext]Starting job: parquet at KMeansModel.scala:142
  [INFO] [2017-12-14 14:32:00][org.apache.spark.scheduler.DAGScheduler]Got job 1 (parquet at KMeansModel.scala:142) with 1 output partitions
  [INFO] [2017-12-14 14:32:00][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (parquet at KMeansModel.scala:142)
  [INFO] [2017-12-14 14:32:00][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:32:00][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:32:00][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[6] at parquet at KMeansModel.scala:142), which has no missing parents
  [INFO] [2017-12-14 14:32:00][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 62.1 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:32:00][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.8 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:32:00][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.18.5:39795 (size: 21.8 KB, free: 882.5 MB)
  [INFO] [2017-12-14 14:32:00][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:32:00][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at parquet at KMeansModel.scala:142) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:32:00][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
  [INFO] [2017-12-14 14:32:00][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5086 bytes)
  [INFO] [2017-12-14 14:32:00][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
  [INFO] [2017-12-14 14:32:01][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.18.5:39795 in memory (size: 1980.0 B, free: 882.5 MB)
  [INFO] [2017-12-14 14:32:01][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 1687 bytes result sent to driver
  [INFO] [2017-12-14 14:32:01][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 578 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:32:01][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:32:01][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (parquet at KMeansModel.scala:142) finished in 0.575 s
  [INFO] [2017-12-14 14:32:01][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: parquet at KMeansModel.scala:142, took 0.669329 s
  [INFO] [2017-12-14 14:32:01][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.18.5:39795 in memory (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:32:01][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.18.5:39795 in memory (size: 21.8 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:32:03][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
  [INFO] [2017-12-14 14:32:03][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: 
  [INFO] [2017-12-14 14:32:03][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<id: int, point: vector>
  [INFO] [2017-12-14 14:32:03][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
  [INFO] [2017-12-14 14:32:04][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 308.712467 ms
  [INFO] [2017-12-14 14:32:04][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 222.5 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:32:04][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.2 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:32:04][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.18.5:39795 (size: 21.2 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:32:04][org.apache.spark.SparkContext]Created broadcast 4 from rdd at KMeansModel.scala:144
  [INFO] [2017-12-14 14:32:04][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 4195868 bytes, open cost is considered as scanning 4194304 bytes.
  [INFO] [2017-12-14 14:32:04][org.apache.spark.SparkContext]Starting job: collect at KMeansModel.scala:144
  [INFO] [2017-12-14 14:32:04][org.apache.spark.scheduler.DAGScheduler]Got job 2 (collect at KMeansModel.scala:144) with 1 output partitions
  [INFO] [2017-12-14 14:32:04][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (collect at KMeansModel.scala:144)
  [INFO] [2017-12-14 14:32:04][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:32:04][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:32:04][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[11] at map at KMeansModel.scala:144), which has no missing parents
  [INFO] [2017-12-14 14:32:04][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 15.9 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:32:04][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:32:04][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.18.5:39795 (size: 7.3 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:32:04][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:32:04][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at map at KMeansModel.scala:144) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:32:04][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
  [INFO] [2017-12-14 14:32:04][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 5397 bytes)
  [INFO] [2017-12-14 14:32:04][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
  [INFO] [2017-12-14 14:32:04][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 43.201844 ms
  [INFO] [2017-12-14 14:32:04][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/data/part-00000-b27e5343-d95e-472b-9d50-03bfef091743-c000.snappy.parquet, range: 0-1564, partition values: [empty row]
  [INFO] [2017-12-14 14:32:04][org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport]Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  required int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
  [INFO] [2017-12-14 14:32:05][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 60.865049 ms
  [INFO] [2017-12-14 14:32:05][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 25.005179 ms
  [INFO] [2017-12-14 14:32:05][org.apache.parquet.hadoop.InternalParquetRecordReader]RecordReader initialized will read a total of 4 records.
  [INFO] [2017-12-14 14:32:05][org.apache.parquet.hadoop.InternalParquetRecordReader]at row 0. reading next block
  [INFO] [2017-12-14 14:32:05][org.apache.hadoop.io.compress.CodecPool]Got brand-new decompressor [.snappy]
  [INFO] [2017-12-14 14:32:05][org.apache.parquet.hadoop.InternalParquetRecordReader]block read in memory in 45 ms. row count = 4
  [INFO] [2017-12-14 14:32:05][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 1885 bytes result sent to driver
  [INFO] [2017-12-14 14:32:05][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 666 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:32:05][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (collect at KMeansModel.scala:144) finished in 0.668 s
  [INFO] [2017-12-14 14:32:05][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: collect at KMeansModel.scala:144, took 0.698219 s
  [INFO] [2017-12-14 14:32:05][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:32:05][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:32:05][org.apache.spark.SparkContext]Starting job: collect at ModelTest.java:91
  [INFO] [2017-12-14 14:32:05][org.apache.spark.scheduler.DAGScheduler]Got job 3 (collect at ModelTest.java:91) with 1 output partitions
  [INFO] [2017-12-14 14:32:05][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (collect at ModelTest.java:91)
  [INFO] [2017-12-14 14:32:05][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:32:05][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:32:05][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[12] at map at ModelTest.java:91), which has no missing parents
  [INFO] [2017-12-14 14:32:05][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 5.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:32:05][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.0 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:32:05][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.18.5:39795 (size: 3.0 KB, free: 882.5 MB)
  [INFO] [2017-12-14 14:32:05][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:32:05][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[12] at map at ModelTest.java:91) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:32:05][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
  [INFO] [2017-12-14 14:32:05][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4884 bytes)
  [INFO] [2017-12-14 14:32:05][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
  [INFO] [2017-12-14 14:32:05][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/output/clean/JuLei/part-r-00000:0+4881501
  [INFO] [2017-12-14 14:32:06][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.18.5:39795 in memory (size: 7.3 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:32:06][org.apache.spark.storage.memory.MemoryStore]Block rdd_2_0 stored as values in memory (estimated size 11.6 MB, free 870.5 MB)
  [INFO] [2017-12-14 14:32:06][org.apache.spark.storage.BlockManagerInfo]Added rdd_2_0 in memory on 192.168.18.5:39795 (size: 11.6 MB, free: 871.0 MB)
  [WARN] [2017-12-14 14:32:06][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
  [WARN] [2017-12-14 14:32:06][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
  [INFO] [2017-12-14 14:32:07][org.apache.spark.storage.memory.MemoryStore]Block taskresult_3 stored as bytes in memory (estimated size 10.7 MB, free 859.8 MB)
  [INFO] [2017-12-14 14:32:07][org.apache.spark.storage.BlockManagerInfo]Added taskresult_3 in memory on 192.168.18.5:39795 (size: 10.7 MB, free: 860.2 MB)
  [INFO] [2017-12-14 14:32:07][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 11258368 bytes result sent via BlockManager)
  [INFO] [2017-12-14 14:32:07][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.18.5:39795 after 36 ms (0 ms spent in bootstraps)
  [INFO] [2017-12-14 14:32:07][org.apache.spark.ContextCleaner]Cleaned accumulator 73
  [INFO] [2017-12-14 14:32:07][org.apache.spark.ContextCleaner]Cleaned accumulator 72
  [INFO] [2017-12-14 14:32:07][org.apache.spark.ContextCleaner]Cleaned accumulator 74
  [INFO] [2017-12-14 14:32:07][org.apache.spark.ContextCleaner]Cleaned accumulator 76
  [INFO] [2017-12-14 14:32:07][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.18.5:39795 in memory (size: 21.2 KB, free: 860.2 MB)
  [INFO] [2017-12-14 14:32:07][org.apache.spark.ContextCleaner]Cleaned accumulator 75
  [INFO] [2017-12-14 14:32:07][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 2294 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:32:07][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_3 on 192.168.18.5:39795 in memory (size: 10.7 MB, free: 871.0 MB)
  [INFO] [2017-12-14 14:32:07][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (collect at ModelTest.java:91) finished in 2.297 s
  [INFO] [2017-12-14 14:32:07][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: collect at ModelTest.java:91, took 2.320984 s
  [INFO] [2017-12-14 14:32:07][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:32:10][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@4175a2a2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:32:10][org.apache.spark.storage.DiskBlockManager]Shutdown hook called
  [INFO] [2017-12-14 14:32:10][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:32:10][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
  [INFO] [2017-12-14 14:32:10][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
  [INFO] [2017-12-14 14:32:10][org.apache.spark.util.ShutdownHookManager]Deleting directory /tmp/spark-65ea22a1-620f-4cdb-9c65-3a4addcf32c4/userFiles-9078b765-3a08-4ea2-aa5d-fd3f890338bb
  [INFO] [2017-12-14 14:32:10][org.apache.spark.util.ShutdownHookManager]Deleting directory /tmp/spark-65ea22a1-620f-4cdb-9c65-3a4addcf32c4
  [INFO] [2017-12-14 14:32:10][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
  [INFO] [2017-12-14 14:32:10][org.apache.spark.storage.BlockManager]BlockManager stopped
  [INFO] [2017-12-14 14:32:10][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
  [INFO] [2017-12-14 14:32:10][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
  [INFO] [2017-12-14 14:32:10][org.apache.spark.SparkContext]Successfully stopped SparkContext
  [INFO] [2017-12-14 14:33:02][org.apache.spark.SparkContext]Running Spark version 2.2.0
  [WARN] [2017-12-14 14:33:02][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN] [2017-12-14 14:33:02][org.apache.spark.util.Utils]Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.18.5 instead (on interface eth0)
  [WARN] [2017-12-14 14:33:02][org.apache.spark.util.Utils]Set SPARK_LOCAL_IP if you need to bind to another address
  [INFO] [2017-12-14 14:33:02][org.apache.spark.SparkContext]Submitted application: JavaKMeansExample
  [INFO] [2017-12-14 14:33:02][org.apache.spark.SecurityManager]Changing view acls to: hadoop
  [INFO] [2017-12-14 14:33:02][org.apache.spark.SecurityManager]Changing modify acls to: hadoop
  [INFO] [2017-12-14 14:33:02][org.apache.spark.SecurityManager]Changing view acls groups to: 
  [INFO] [2017-12-14 14:33:02][org.apache.spark.SecurityManager]Changing modify acls groups to: 
  [INFO] [2017-12-14 14:33:02][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
  [INFO] [2017-12-14 14:33:03][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 36108.
  [INFO] [2017-12-14 14:33:03][org.apache.spark.SparkEnv]Registering MapOutputTracker
  [INFO] [2017-12-14 14:33:03][org.apache.spark.SparkEnv]Registering BlockManagerMaster
  [INFO] [2017-12-14 14:33:03][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
  [INFO] [2017-12-14 14:33:03][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
  [INFO] [2017-12-14 14:33:03][org.apache.spark.storage.DiskBlockManager]Created local directory at /tmp/blockmgr-81604940-770f-4cd4-97ca-064dc9afb0e2
  [INFO] [2017-12-14 14:33:03][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 882.6 MB
  [INFO] [2017-12-14 14:33:03][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.util.log]Logging initialized @2838ms
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.Server]jetty-9.3.z-SNAPSHOT
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.Server]Started @2945ms
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.AbstractConnector]Started ServerConnector@1ee60a06{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:33:03][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6b5176f2{/jobs,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7f811d00{/jobs/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4089713{/jobs/job,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@b91d8c4{/jobs/job/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@a77614d{/stages,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4a067c25{/stages/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bde62ff{/stages/stage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@791cbf87{/stages/stage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@754777cd{/stages/pool,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@372ea2bc{/stages/pool/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f08c4b{/storage,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7de0c6ae{/storage/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@cdc3aae{/storage/rdd,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5dcbb60{/storage/rdd/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21526f6c{/environment,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@299266e2{/environment/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@66ea1466{/executors,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3bffddff{/executors/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@50687efb{/executors/threadDump,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@142eef62{/executors/threadDump/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5990e6c5{/static,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@21694e53{/,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@22c86919{/api,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ea1bcdc{/jobs/job/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@64712be{/stages/stage/kill,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:03][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:33:04][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
  [INFO] [2017-12-14 14:33:04][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36584.
  [INFO] [2017-12-14 14:33:04][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.18.5:36584
  [INFO] [2017-12-14 14:33:04][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
  [INFO] [2017-12-14 14:33:04][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.18.5, 36584, None)
  [INFO] [2017-12-14 14:33:04][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.18.5:36584 with 882.6 MB RAM, BlockManagerId(driver, 192.168.18.5, 36584, None)
  [INFO] [2017-12-14 14:33:04][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.18.5, 36584, None)
  [INFO] [2017-12-14 14:33:04][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.18.5, 36584, None)
  [INFO] [2017-12-14 14:33:04][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1532c619{/metrics/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:05][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 209.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:33:05][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.4 MB)
  [INFO] [2017-12-14 14:33:05][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.18.5:36584 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:33:05][org.apache.spark.SparkContext]Created broadcast 0 from textFile at ModelTest.java:44
  [INFO] [2017-12-14 14:33:05][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 209.1 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:33:05][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.1 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:33:05][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.18.5:36584 (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:33:05][org.apache.spark.SparkContext]Created broadcast 1 from textFile at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:33:06][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:33:06][org.apache.spark.SparkContext]Starting job: first at modelSaveLoad.scala:129
  [INFO] [2017-12-14 14:33:06][org.apache.spark.scheduler.DAGScheduler]Got job 0 (first at modelSaveLoad.scala:129) with 1 output partitions
  [INFO] [2017-12-14 14:33:06][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (first at modelSaveLoad.scala:129)
  [INFO] [2017-12-14 14:33:06][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:33:06][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:33:06][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[4] at textFile at modelSaveLoad.scala:129), which has no missing parents
  [INFO] [2017-12-14 14:33:06][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:33:06][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 1980.0 B, free 882.1 MB)
  [INFO] [2017-12-14 14:33:06][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.18.5:36584 (size: 1980.0 B, free: 882.6 MB)
  [INFO] [2017-12-14 14:33:06][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:33:06][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata MapPartitionsRDD[4] at textFile at modelSaveLoad.scala:129) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:33:06][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
  [INFO] [2017-12-14 14:33:06][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4926 bytes)
  [INFO] [2017-12-14 14:33:06][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
  [INFO] [2017-12-14 14:33:07][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/metadata/part-00000:0+80
  [INFO] [2017-12-14 14:33:07][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 876 bytes result sent to driver
  [INFO] [2017-12-14 14:33:07][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 439 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:33:07][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:33:07][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (first at modelSaveLoad.scala:129) finished in 0.467 s
  [INFO] [2017-12-14 14:33:07][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: first at modelSaveLoad.scala:129, took 0.696697 s
  [INFO] [2017-12-14 14:33:07][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/').
  [INFO] [2017-12-14 14:33:07][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/home/hadoop/EclipseJava/workspace/storm/spark-warehouse/'.
  [INFO] [2017-12-14 14:33:07][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@45ab3bdd{/SQL,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:07][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1642eeae{/SQL/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:07][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@322ba549{/SQL/execution,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:07][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5416f8db{/SQL/execution/json,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:07][org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@726934e2{/static/sql,null,AVAILABLE,@Spark}
  [INFO] [2017-12-14 14:33:08][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.18.5:36584 in memory (size: 1980.0 B, free: 882.6 MB)
  [INFO] [2017-12-14 14:33:08][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
  [INFO] [2017-12-14 14:33:08][org.apache.spark.SparkContext]Starting job: parquet at KMeansModel.scala:142
  [INFO] [2017-12-14 14:33:08][org.apache.spark.scheduler.DAGScheduler]Got job 1 (parquet at KMeansModel.scala:142) with 1 output partitions
  [INFO] [2017-12-14 14:33:08][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (parquet at KMeansModel.scala:142)
  [INFO] [2017-12-14 14:33:08][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:33:08][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:33:08][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[6] at parquet at KMeansModel.scala:142), which has no missing parents
  [INFO] [2017-12-14 14:33:08][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 62.1 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:33:08][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.8 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:33:08][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.18.5:36584 (size: 21.8 KB, free: 882.5 MB)
  [INFO] [2017-12-14 14:33:08][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:33:08][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at parquet at KMeansModel.scala:142) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:33:08][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
  [INFO] [2017-12-14 14:33:08][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5086 bytes)
  [INFO] [2017-12-14 14:33:08][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 1)
  [INFO] [2017-12-14 14:33:09][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 1). 1644 bytes result sent to driver
  [INFO] [2017-12-14 14:33:09][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 466 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:33:09][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:33:09][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (parquet at KMeansModel.scala:142) finished in 0.467 s
  [INFO] [2017-12-14 14:33:09][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: parquet at KMeansModel.scala:142, took 0.555930 s
  [INFO] [2017-12-14 14:33:09][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.18.5:36584 in memory (size: 21.8 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:33:09][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.18.5:36584 in memory (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:33:11][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Pruning directories with: 
  [INFO] [2017-12-14 14:33:11][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Post-Scan Filters: 
  [INFO] [2017-12-14 14:33:11][org.apache.spark.sql.execution.datasources.FileSourceStrategy]Output Data Schema: struct<id: int, point: vector>
  [INFO] [2017-12-14 14:33:11][org.apache.spark.sql.execution.FileSourceScanExec]Pushed Filters: 
  [INFO] [2017-12-14 14:33:12][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 288.137815 ms
  [INFO] [2017-12-14 14:33:12][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 222.5 KB, free 882.2 MB)
  [INFO] [2017-12-14 14:33:12][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.2 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:33:12][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.18.5:36584 (size: 21.2 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:33:12][org.apache.spark.SparkContext]Created broadcast 4 from rdd at KMeansModel.scala:144
  [INFO] [2017-12-14 14:33:12][org.apache.spark.sql.execution.FileSourceScanExec]Planning scan with bin packing, max size: 4195868 bytes, open cost is considered as scanning 4194304 bytes.
  [INFO] [2017-12-14 14:33:12][org.apache.spark.SparkContext]Starting job: collect at KMeansModel.scala:144
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.DAGScheduler]Got job 2 (collect at KMeansModel.scala:144) with 1 output partitions
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (collect at KMeansModel.scala:144)
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (MapPartitionsRDD[11] at map at KMeansModel.scala:144), which has no missing parents
  [INFO] [2017-12-14 14:33:12][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 15.9 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:33:12][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:33:12][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.18.5:36584 (size: 7.3 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:33:12][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at map at KMeansModel.scala:144) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 5397 bytes)
  [INFO] [2017-12-14 14:33:12][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 2)
  [INFO] [2017-12-14 14:33:12][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 58.430297 ms
  [INFO] [2017-12-14 14:33:12][org.apache.spark.sql.execution.datasources.FileScanRDD]Reading File path: hdfs://localhost:9000/user/hadoop/target/org/apache/spark/JavaKMeansExample/KMeansModel/data/part-00000-b27e5343-d95e-472b-9d50-03bfef091743-c000.snappy.parquet, range: 0-1564, partition values: [empty row]
  [INFO] [2017-12-14 14:33:12][org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport]Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  required int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
  [INFO] [2017-12-14 14:33:12][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 60.267033 ms
  [INFO] [2017-12-14 14:33:12][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 57.898415 ms
  [INFO] [2017-12-14 14:33:12][org.apache.parquet.hadoop.InternalParquetRecordReader]RecordReader initialized will read a total of 4 records.
  [INFO] [2017-12-14 14:33:12][org.apache.parquet.hadoop.InternalParquetRecordReader]at row 0. reading next block
  [INFO] [2017-12-14 14:33:12][org.apache.hadoop.io.compress.CodecPool]Got brand-new decompressor [.snappy]
  [INFO] [2017-12-14 14:33:12][org.apache.parquet.hadoop.InternalParquetRecordReader]block read in memory in 44 ms. row count = 4
  [INFO] [2017-12-14 14:33:12][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 2). 1842 bytes result sent to driver
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 2) in 689 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (collect at KMeansModel.scala:144) finished in 0.688 s
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: collect at KMeansModel.scala:144, took 0.725338 s
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:33:12][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
  [INFO] [2017-12-14 14:33:12][org.apache.spark.SparkContext]Starting job: collect at ModelTest.java:91
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.DAGScheduler]Got job 3 (collect at ModelTest.java:91) with 1 output partitions
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (collect at ModelTest.java:91)
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[12] at map at ModelTest.java:91), which has no missing parents
  [INFO] [2017-12-14 14:33:12][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 5.3 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:33:12][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.0 KB, free 882.1 MB)
  [INFO] [2017-12-14 14:33:12][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.18.5:36584 (size: 3.0 KB, free: 882.5 MB)
  [INFO] [2017-12-14 14:33:12][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1006
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[12] at map at ModelTest.java:91) (first 15 tasks are for partitions Vector(0))
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
  [INFO] [2017-12-14 14:33:12][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 4884 bytes)
  [INFO] [2017-12-14 14:33:12][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 3)
  [INFO] [2017-12-14 14:33:13][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://localhost:9000/user/hadoop/output/clean/JuLei/part-r-00000:0+4881501
  [INFO] [2017-12-14 14:33:13][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.18.5:36584 in memory (size: 7.3 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:33:14][org.apache.spark.storage.memory.MemoryStore]Block rdd_2_0 stored as values in memory (estimated size 11.6 MB, free 870.5 MB)
  [INFO] [2017-12-14 14:33:14][org.apache.spark.storage.BlockManagerInfo]Added rdd_2_0 in memory on 192.168.18.5:36584 (size: 11.6 MB, free: 871.0 MB)
  [WARN] [2017-12-14 14:33:14][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
  [WARN] [2017-12-14 14:33:14][com.github.fommil.netlib.BLAS]Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
  [INFO] [2017-12-14 14:33:14][org.apache.spark.storage.memory.MemoryStore]Block taskresult_3 stored as bytes in memory (estimated size 10.7 MB, free 859.8 MB)
  [INFO] [2017-12-14 14:33:14][org.apache.spark.storage.BlockManagerInfo]Added taskresult_3 in memory on 192.168.18.5:36584 (size: 10.7 MB, free: 860.2 MB)
  [INFO] [2017-12-14 14:33:14][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 3). 11258368 bytes result sent via BlockManager)
  [INFO] [2017-12-14 14:33:14][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to /192.168.18.5:36584 after 33 ms (0 ms spent in bootstraps)
  [INFO] [2017-12-14 14:33:15][org.apache.spark.storage.BlockManagerInfo]Removed taskresult_3 on 192.168.18.5:36584 in memory (size: 10.7 MB, free: 871.0 MB)
  [INFO] [2017-12-14 14:33:15][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 3) in 2142 ms on localhost (executor driver) (1/1)
  [INFO] [2017-12-14 14:33:15][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (collect at ModelTest.java:91) finished in 2.143 s
  [INFO] [2017-12-14 14:33:15][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: collect at ModelTest.java:91, took 2.160283 s
  [INFO] [2017-12-14 14:33:15][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
  [INFO] [2017-12-14 14:33:15][org.apache.spark.ContextCleaner]Cleaned accumulator 72
  [INFO] [2017-12-14 14:33:15][org.apache.spark.ContextCleaner]Cleaned accumulator 75
  [INFO] [2017-12-14 14:33:15][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.18.5:36584 in memory (size: 21.2 KB, free: 871.0 MB)
  [INFO] [2017-12-14 14:33:15][org.apache.spark.ContextCleaner]Cleaned accumulator 76
  [INFO] [2017-12-14 14:33:15][org.apache.spark.storage.BlockManager]Removing RDD 2
  [INFO] [2017-12-14 14:33:15][org.apache.spark.ContextCleaner]Cleaned RDD 2
  [INFO] [2017-12-14 14:33:15][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 192.168.18.5:36584 in memory (size: 3.0 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:33:15][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 192.168.18.5:36584 in memory (size: 20.1 KB, free: 882.6 MB)
  [INFO] [2017-12-14 14:33:15][org.apache.spark.ContextCleaner]Cleaned accumulator 73
  [INFO] [2017-12-14 14:33:15][org.apache.spark.ContextCleaner]Cleaned accumulator 74
  [INFO] [2017-12-14 14:33:15][org.spark_project.jetty.server.AbstractConnector]Stopped Spark@1ee60a06{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
  [INFO] [2017-12-14 14:33:15][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.18.5:4040
  [INFO] [2017-12-14 14:33:15][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
  [INFO] [2017-12-14 14:33:15][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
  [INFO] [2017-12-14 14:33:15][org.apache.spark.storage.BlockManager]BlockManager stopped
  [INFO] [2017-12-14 14:33:15][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
  [INFO] [2017-12-14 14:33:15][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
  [INFO] [2017-12-14 14:33:15][org.apache.spark.SparkContext]Successfully stopped SparkContext
  [INFO] [2017-12-14 14:33:15][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
  [INFO] [2017-12-14 14:33:15][org.apache.spark.util.ShutdownHookManager]Deleting directory /tmp/spark-49a136bc-772d-442b-92fb-ac875ccd3c41
  [WARN] [2017-12-18 10:40:30][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [INFO] [2017-12-18 11:37:02][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-18 11:37:02][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 11:37:02 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 11:37:03][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-18 11:37:03][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-18 11:37:04][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [org/springframework/jdbc/support/sql-error-codes.xml]
  [INFO] [2017-12-18 11:37:04][org.springframework.jdbc.support.SQLErrorCodesFactory]SQLErrorCodes loaded: [DB2, Derby, H2, HSQL, Informix, MS-SQL, MySQL, Oracle, PostgreSQL, Sybase]
  [INFO] [2017-12-18 11:37:04][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 11:37:02 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 11:38:28][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-18 11:38:29][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 11:38:29 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 11:38:29][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-18 11:38:29][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-18 11:38:30][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [org/springframework/jdbc/support/sql-error-codes.xml]
  [INFO] [2017-12-18 11:38:30][org.springframework.jdbc.support.SQLErrorCodesFactory]SQLErrorCodes loaded: [DB2, Derby, H2, HSQL, Informix, MS-SQL, MySQL, Oracle, PostgreSQL, Sybase]
  [INFO] [2017-12-18 11:38:30][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 11:38:29 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 11:42:26][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-18 11:42:27][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 11:42:27 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 11:42:27][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-18 11:42:27][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-18 11:42:28][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [org/springframework/jdbc/support/sql-error-codes.xml]
  [INFO] [2017-12-18 11:42:28][org.springframework.jdbc.support.SQLErrorCodesFactory]SQLErrorCodes loaded: [DB2, Derby, H2, HSQL, Informix, MS-SQL, MySQL, Oracle, PostgreSQL, Sybase]
  [INFO] [2017-12-18 11:42:28][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 11:42:27 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 11:52:43][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-18 11:52:44][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 11:52:44 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 11:52:44][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-18 11:52:44][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-18 11:52:46][org.zyp.testmybatis.TestLearnStatistics][{"account_no":"a1015088819","book_id":"45ff80ee9e84495a8a01880e125cc0b3","booknum":0,"runtime":0.2972222222222222},{"account_no":"a1015088819","book_id":"da1df87e65f84bcb8634d16cb808994e","booknum":0,"runtime":7.619444444444444}]
  [INFO] [2017-12-18 11:52:46][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 11:52:44 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 11:58:21][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-18 11:58:22][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 11:58:22 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 11:58:22][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-18 11:58:22][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-18 11:58:23][org.zyp.testmybatis.TestLearnStatistics][{"account_no":"a1015088819","book_id":"45ff80ee9e84495a8a01880e125cc0b3","runtime":0.2972222222222222},{"account_no":"a1015088819","book_id":"da1df87e65f84bcb8634d16cb808994e","runtime":7.619444444444444}]
  [INFO] [2017-12-18 11:58:23][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 11:58:22 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 13:46:51][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-18 13:46:52][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 13:46:52 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 13:46:53][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-18 13:46:53][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-18 13:46:54][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 13:46:52 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 13:57:18][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-18 13:57:19][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 13:57:19 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 13:57:20][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-18 13:57:20][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-18 13:57:21][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 13:57:19 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 13:58:28][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-18 13:58:31][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 13:58:31 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 13:58:32][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-18 13:58:32][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-18 13:58:32][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-18 13:58:35][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 13:58:35 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 13:58:36][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-18 13:58:37][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-18 13:58:37][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [ERROR] [2017-12-18 13:58:37][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@7a362b6b] to prepare test instance [org.zyp.testmybatis.TestLearnStatistics@60df60da]
  java.lang.IllegalStateException: Failed to load ApplicationContext
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:99)
	at org.springframework.test.context.DefaultTestContext.getApplicationContext(DefaultTestContext.java:101)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookController': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'UserService': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.userMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/preBookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'clickRateMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/clickRateMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/userMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/topLessonMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/LearnStatisticsMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'bookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/bookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/preBookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'clickRateMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/clickRateMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/userMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/topLessonMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/LearnStatisticsMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'bookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/bookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:700)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:121)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:60)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.delegateLoading(AbstractDelegatingSmartContextLoader.java:100)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.loadContext(AbstractDelegatingSmartContextLoader.java:250)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContextInternal(CacheAwareContextLoaderDelegate.java:64)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)
	... 25 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'UserService': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.userMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1014)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:957)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 41 more
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.userMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.raiseNoSuchBeanDefinitionException(DefaultListableBeanFactory.java:1100)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:960)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 57 more
[INFO] [2017-12-18 13:58:38][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 13:58:38 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 13:58:39][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-18 13:58:39][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [ERROR] [2017-12-18 13:58:39][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@7a362b6b] to prepare test instance [org.zyp.testmybatis.TestLearnStatistics@60df60da]
  java.lang.IllegalStateException: Failed to load ApplicationContext
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:99)
	at org.springframework.test.context.DefaultTestContext.getApplicationContext(DefaultTestContext.java:101)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookController': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'UserService': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.userMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/preBookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'clickRateMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/clickRateMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/userMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/topLessonMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/LearnStatisticsMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'bookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/bookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/preBookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'clickRateMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/clickRateMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/userMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/topLessonMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/LearnStatisticsMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'bookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/bookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:700)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:121)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:60)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.delegateLoading(AbstractDelegatingSmartContextLoader.java:100)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.loadContext(AbstractDelegatingSmartContextLoader.java:250)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContextInternal(CacheAwareContextLoaderDelegate.java:64)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)
	... 25 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'UserService': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.userMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1014)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:957)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 41 more
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.userMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.raiseNoSuchBeanDefinitionException(DefaultListableBeanFactory.java:1100)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:960)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 57 more
[ERROR] [2017-12-18 13:58:40][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@7a362b6b] to prepare test instance [org.zyp.testmybatis.TestLearnStatistics@60df60da]
  java.lang.IllegalStateException: Failed to load ApplicationContext
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:99)
	at org.springframework.test.context.DefaultTestContext.getApplicationContext(DefaultTestContext.java:101)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookController': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'UserService': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.userMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/preBookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'clickRateMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/clickRateMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/userMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/topLessonMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/LearnStatisticsMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'bookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/bookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/preBookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'clickRateMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/clickRateMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/userMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/topLessonMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/LearnStatisticsMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'bookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/bookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:700)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:121)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:60)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.delegateLoading(AbstractDelegatingSmartContextLoader.java:100)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.loadContext(AbstractDelegatingSmartContextLoader.java:250)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContextInternal(CacheAwareContextLoaderDelegate.java:64)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)
	... 25 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'UserService': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.userMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1014)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:957)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 41 more
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.userMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.raiseNoSuchBeanDefinitionException(DefaultListableBeanFactory.java:1100)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:960)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 57 more
[INFO] [2017-12-18 13:59:39][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-18 13:59:39][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 13:59:39 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 13:59:40][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-18 13:59:40][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [ERROR] [2017-12-18 13:59:41][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@7a362b6b] to prepare test instance [org.zyp.testmybatis.TestLearnStatistics@60df60da]
  java.lang.IllegalStateException: Failed to load ApplicationContext
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:99)
	at org.springframework.test.context.DefaultTestContext.getApplicationContext(DefaultTestContext.java:101)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookController': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'UserService': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.userMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/preBookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'clickRateMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/clickRateMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/userMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/topLessonMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/LearnStatisticsMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'bookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/bookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/preBookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'clickRateMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/clickRateMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/userMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/topLessonMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/LearnStatisticsMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'bookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/bookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/LearnStatisticsMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: org.apache.ibatis.builder.BuilderException: Error resolving class. Cause: org.apache.ibatis.type.TypeException: Could not resolve type alias 'java.lang.string'.  Cause: java.lang.ClassNotFoundException: Cannot find class: java.lang.string
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:700)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:121)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:60)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.delegateLoading(AbstractDelegatingSmartContextLoader.java:100)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.loadContext(AbstractDelegatingSmartContextLoader.java:250)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContextInternal(CacheAwareContextLoaderDelegate.java:64)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)
	... 25 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'UserService': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.userMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1014)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:957)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 41 more
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.userMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.raiseNoSuchBeanDefinitionException(DefaultListableBeanFactory.java:1100)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:960)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 57 more
[INFO] [2017-12-18 14:01:01][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-18 14:01:02][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 14:01:02 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 14:01:02][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-18 14:01:02][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-18 14:01:04][org.zyp.testmybatis.TestLearnStatistics][{"account_no":"a1015088819","book_id":"45ff80ee9e84495a8a01880e125cc0b3","booknum":0,"runtime":0.2972222222222222},{"account_no":"a1015088819","book_id":"da1df87e65f84bcb8634d16cb808994e","booknum":0,"runtime":7.619444444444444}]
  [INFO] [2017-12-18 14:01:04][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 14:01:02 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 14:01:53][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-18 14:01:54][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 14:01:54 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 14:01:54][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-18 14:01:54][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-18 14:01:56][org.zyp.testmybatis.TestLearnStatistics][{"account_no":"a1015088819","book_id":"45ff80ee9e84495a8a01880e125cc0b3","booknum":0,"runtime":0.2972222222222222},{"account_no":"a1015088819","book_id":"da1df87e65f84bcb8634d16cb808994e","booknum":0,"runtime":7.619444444444444}]
  [INFO] [2017-12-18 14:01:56][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 14:01:54 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 14:18:41][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-18 14:18:42][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 14:18:42 CST 2017]; root of context hierarchy
  [INFO] [2017-12-18 14:18:43][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-18 14:18:43][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-18 14:18:44][org.zyp.testmybatis.TestLearnStatistics][{"account_no":"a1015088819","allRuntime":"7.916666666666666","book_id":"45ff80ee9e84495a8a01880e125cc0b3","booknum":2,"runtime":0.2972222222222222},{"account_no":"a1015088819","book_id":"da1df87e65f84bcb8634d16cb808994e","booknum":0,"runtime":7.619444444444444}]
  [INFO] [2017-12-18 14:18:44][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Mon Dec 18 14:18:42 CST 2017]; root of context hierarchy
  [INFO] [2017-12-19 10:25:42][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-19 10:25:43][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@7a36aefa: startup date [Tue Dec 19 10:25:43 CST 2017]; root of context hierarchy
  [INFO] [2017-12-19 10:25:44][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-19 10:25:44][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [ERROR] [2017-12-19 10:25:46][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@466276d8] to prepare test instance [org.zyp.testmybatis.TestPreBook@5ce8d869]
  java.lang.IllegalStateException: Failed to load ApplicationContext
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:99)
	at org.springframework.test.context.DefaultTestContext.getApplicationContext(DefaultTestContext.java:101)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookController': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'UserService': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.userMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/preBookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'clickRateMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/clickRateMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/userMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/topLessonMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/LearnStatisticsMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'bookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/bookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/preBookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'clickRateMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/clickRateMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/userMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/topLessonMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/LearnStatisticsMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'bookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/bookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:700)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:121)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:60)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.delegateLoading(AbstractDelegatingSmartContextLoader.java:100)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.loadContext(AbstractDelegatingSmartContextLoader.java:250)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContextInternal(CacheAwareContextLoaderDelegate.java:64)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)
	... 25 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'UserService': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.userMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1014)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:957)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 41 more
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.userMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.raiseNoSuchBeanDefinitionException(DefaultListableBeanFactory.java:1100)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:960)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 57 more
[INFO] [2017-12-19 10:27:16][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-19 10:27:17][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@7a36aefa: startup date [Tue Dec 19 10:27:17 CST 2017]; root of context hierarchy
  [INFO] [2017-12-19 10:27:17][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-19 10:27:17][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [ERROR] [2017-12-19 10:27:19][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@d62fe5b] to prepare test instance [org.zyp.testmybatis.TestPreBook@49964d75]
  java.lang.IllegalStateException: Failed to load ApplicationContext
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:99)
	at org.springframework.test.context.DefaultTestContext.getApplicationContext(DefaultTestContext.java:101)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsController': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'LearnStatistics': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.LearnStatisticsMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/preBookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'clickRateMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/clickRateMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/userMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/topLessonMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/LearnStatisticsMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'bookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/bookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/preBookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'clickRateMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/clickRateMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/userMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/topLessonMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/LearnStatisticsMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'bookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/bookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:700)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:121)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:60)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.delegateLoading(AbstractDelegatingSmartContextLoader.java:100)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.loadContext(AbstractDelegatingSmartContextLoader.java:250)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContextInternal(CacheAwareContextLoaderDelegate.java:64)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)
	... 25 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'LearnStatistics': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.LearnStatisticsMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1014)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:957)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 41 more
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.LearnStatisticsMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.raiseNoSuchBeanDefinitionException(DefaultListableBeanFactory.java:1100)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:960)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 57 more
[INFO] [2017-12-19 10:30:07][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-19 10:30:08][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@7a36aefa: startup date [Tue Dec 19 10:30:08 CST 2017]; root of context hierarchy
  [INFO] [2017-12-19 10:30:09][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-19 10:30:09][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [ERROR] [2017-12-19 10:30:10][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@d62fe5b] to prepare test instance [org.zyp.testmybatis.TestPreBook@49964d75]
  java.lang.IllegalStateException: Failed to load ApplicationContext
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:99)
	at org.springframework.test.context.DefaultTestContext.getApplicationContext(DefaultTestContext.java:101)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsController': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'LearnStatistics': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.LearnStatisticsMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/preBookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'clickRateMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/clickRateMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/userMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/topLessonMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/LearnStatisticsMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'bookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/bookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/preBookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'clickRateMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/clickRateMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/userMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/topLessonMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/LearnStatisticsMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'bookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/bookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:700)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:121)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:60)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.delegateLoading(AbstractDelegatingSmartContextLoader.java:100)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.loadContext(AbstractDelegatingSmartContextLoader.java:250)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContextInternal(CacheAwareContextLoaderDelegate.java:64)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)
	... 25 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'LearnStatistics': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.LearnStatisticsMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1014)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:957)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 41 more
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.LearnStatisticsMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.raiseNoSuchBeanDefinitionException(DefaultListableBeanFactory.java:1100)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:960)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 57 more
[INFO] [2017-12-19 10:30:33][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-19 10:30:34][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@7a36aefa: startup date [Tue Dec 19 10:30:34 CST 2017]; root of context hierarchy
  [INFO] [2017-12-19 10:30:34][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-19 10:30:34][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [ERROR] [2017-12-19 10:30:36][org.springframework.test.context.TestContextManager]Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@d62fe5b] to prepare test instance [org.zyp.testmybatis.TestPreBook@49964d75]
  java.lang.IllegalStateException: Failed to load ApplicationContext
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:99)
	at org.springframework.test.context.DefaultTestContext.getApplicationContext(DefaultTestContext.java:101)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109)
	at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75)
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:319)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:212)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner$1.runReflectiveCall(SpringJUnit4ClassRunner.java:289)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.methodBlock(SpringJUnit4ClassRunner.java:291)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:232)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:89)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:175)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsController': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'LearnStatistics': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.LearnStatisticsMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/preBookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'clickRateMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/clickRateMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/userMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/topLessonMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/LearnStatisticsMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'bookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/bookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'preBookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/preBookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'clickRateMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/clickRateMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/userMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'topLessonMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/topLessonMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'learnStatisticsMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/LearnStatisticsMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'bookMapper' defined in file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/dao/bookMapper.class]: Cannot resolve reference to bean 'sqlSessionFactory' while setting bean property 'sqlSessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sqlSessionFactory' defined in class path resource [spring-mybatis.xml]: Invocation of init method failed; nested exception is org.springframework.core.NestedIOException: Failed to parse mapping resource: 'file [/home/hadoop/EclipseJava/workspace/storm/target/classes/com/log/map/bookMapper.xml]'; nested exception is org.apache.ibatis.builder.BuilderException: Error parsing Mapper XML. Cause: java.lang.IllegalArgumentException: Result Maps collection already contains value for com.log.dao.bookMapper.BaseResultMap
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:700)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:121)
	at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:60)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.delegateLoading(AbstractDelegatingSmartContextLoader.java:100)
	at org.springframework.test.context.support.AbstractDelegatingSmartContextLoader.loadContext(AbstractDelegatingSmartContextLoader.java:250)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContextInternal(CacheAwareContextLoaderDelegate.java:64)
	at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)
	... 25 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'LearnStatistics': Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.LearnStatisticsMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:307)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1014)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:957)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 41 more
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.log.dao.LearnStatisticsMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@javax.annotation.Resource(shareable=true, lookup=, name=, description=, authenticationType=CONTAINER, type=class java.lang.Object, mappedName=)}
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.raiseNoSuchBeanDefinitionException(DefaultListableBeanFactory.java:1100)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:960)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:855)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:441)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:419)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:544)
	at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:155)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:87)
	at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessPropertyValues(CommonAnnotationBeanPostProcessor.java:304)
	... 57 more
[INFO] [2017-12-19 10:46:06][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-19 10:46:07][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@7a36aefa: startup date [Tue Dec 19 10:46:07 CST 2017]; root of context hierarchy
  [INFO] [2017-12-19 10:46:07][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-19 10:46:07][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-19 10:46:09][org.zyp.testmybatis.TestMyBatis]["97-3.9992483","43-3.8377337","366-3.17177","264-3.0743296","256-2.9858916","54-2.7529395","55-2.4843833","211-2.450248","152-2.4386883","78-2.3679228"]
  [INFO] [2017-12-19 10:46:09][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@7a36aefa: startup date [Tue Dec 19 10:46:07 CST 2017]; root of context hierarchy
  [WARN] [2017-12-19 11:35:39][org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [INFO] [2017-12-19 14:32:04][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-19 14:32:05][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Tue Dec 19 14:32:05 CST 2017]; root of context hierarchy
  [INFO] [2017-12-19 14:32:06][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-19 14:32:06][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-19 14:32:08][org.zyp.testmybatis.TestMonthly][]
  [INFO] [2017-12-19 14:32:08][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Tue Dec 19 14:32:05 CST 2017]; root of context hierarchy
  [INFO] [2017-12-19 14:33:10][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-19 14:33:12][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Tue Dec 19 14:33:12 CST 2017]; root of context hierarchy
  [INFO] [2017-12-19 14:33:12][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-19 14:33:12][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-19 14:33:13][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Tue Dec 19 14:33:12 CST 2017]; root of context hierarchy
  [INFO] [2017-12-19 14:34:49][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-19 14:34:50][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Tue Dec 19 14:34:50 CST 2017]; root of context hierarchy
  [INFO] [2017-12-19 14:34:50][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-19 14:34:50][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-19 14:34:52][org.zyp.testmybatis.TestMonthly][{"account_no":"lllijie","date":"2013-09","runtime":"1.4558333333333335"},{"account_no":"lllijie","date":"2013-10","runtime":"0.7886111111111112"}]
  [INFO] [2017-12-19 14:34:52][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Tue Dec 19 14:34:50 CST 2017]; root of context hierarchy
  [INFO] [2017-12-19 16:19:10][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-19 16:19:11][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Tue Dec 19 16:19:11 CST 2017]; root of context hierarchy
  [INFO] [2017-12-19 16:19:11][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-19 16:19:11][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-19 16:19:12][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Tue Dec 19 16:19:11 CST 2017]; root of context hierarchy
  [INFO] [2017-12-19 16:23:54][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-19 16:23:55][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Tue Dec 19 16:23:55 CST 2017]; root of context hierarchy
  [INFO] [2017-12-19 16:23:55][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-19 16:23:55][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-19 16:23:56][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Tue Dec 19 16:23:55 CST 2017]; root of context hierarchy
  [INFO] [2017-12-19 16:25:04][org.springframework.beans.factory.xml.XmlBeanDefinitionReader]Loading XML bean definitions from class path resource [spring-mybatis.xml]
  [INFO] [2017-12-19 16:25:05][org.springframework.context.support.GenericApplicationContext]Refreshing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Tue Dec 19 16:25:05 CST 2017]; root of context hierarchy
  [INFO] [2017-12-19 16:25:06][org.springframework.beans.factory.config.PropertyPlaceholderConfigurer]Loading properties file from class path resource [jdbc.properties]
  [INFO] [2017-12-19 16:25:06][org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor]JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
  [INFO] [2017-12-19 16:25:07][org.zyp.testmybatis.TestMonthly][{"account_no":"lllijie","date":"2013-09","runtime":"1.4558333333333335"},{"account_no":"lllijie","date":"2013-10","runtime":"0.7886111111111112"}]
  [INFO] [2017-12-19 16:25:07][org.springframework.context.support.GenericApplicationContext]Closing org.springframework.context.support.GenericApplicationContext@2fd66ad3: startup date [Tue Dec 19 16:25:05 CST 2017]; root of context hierarchy
  